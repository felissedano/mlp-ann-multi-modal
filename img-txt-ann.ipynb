{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/felis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/felis/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load the necessary libraries\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "# import numpy.random as random\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle, gzip, urllib.request, json\n",
    "from datetime import datetime\n",
    "from IPython.core.debugger import Pdb # I encourage you to use the degubber, rather than print statements!\n",
    "\n",
    "import csv\n",
    "import nltk as nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('tokenizers/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('wordnet')\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer      \n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "if cp.is_available() and cp.cuda.is_available():\n",
    "    import cupy as cnp\n",
    "else:\n",
    "    import numpy as cnp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# check if the device running this code will have cuda and cupy available for GPU acceleration.\n",
    "print(cp.is_available())\n",
    "print(cp.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function text preprocessing\n",
    "\n",
    "# The function for lemmatizing, used inside tfidfvectorizer\n",
    "def tokenize_lemma_removepunc(text:str) -> list[str]:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens_no_punc = [word for word in tokens if word not in string.punctuation]\n",
    "    lemmas = [lemmatizer.lemmatize(token) for token in tokens_no_punc]\n",
    "    return lemmas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "# Some helper functions\n",
    "\n",
    "# The sigmoid function\n",
    "def sigmoid(Xs):\n",
    "    \"\"\"\n",
    "    Returns the sigmoid function, i.e. 1/(1+exp(-X))\n",
    "    \"\"\"\n",
    "    \n",
    "    # to avoid runtime warnings, if abs(X) is more than 500, let's just cap it there\n",
    "    toobig   = Xs > 500\n",
    "    toosmall = Xs < -500\n",
    "    Xs[toobig]   = 500\n",
    "    Xs[toosmall] = -500\n",
    "        \n",
    "    return 1.0/(1.0 + cnp.exp(-Xs))\n",
    " \n",
    "# A helper function to add an \"always on\" bias units to the inputs\n",
    "def add_bias(inputs):\n",
    "    \"\"\"\n",
    "    Append an \"always on\" bias to some inputs\n",
    "    \"\"\"\n",
    "    \n",
    "    return cnp.append(inputs,cnp.ones((1,inputs.shape[1])),axis=0)\n",
    "\n",
    "# Creates a random set of batches, returns an array of indices, one for each batch\n",
    "def create_batches(batch_size, num_samples):\n",
    "    \"\"\"\n",
    "    For a given number of samples, returns an array of indices of random batches of the specified size.\n",
    "    \n",
    "    If the size of the data is not divisible by the batch size some samples will not be included.\n",
    "    \"\"\"\n",
    "    \n",
    "    # determine the total number of batches\n",
    "    num_batches = int(cnp.floor(num_samples/batch_size))\n",
    "    \n",
    "    # get the batches (without replacement)\n",
    "    return cnp.random.choice(cnp.arange(num_samples),size=(num_batches,batch_size),replace=False)\n",
    "\n",
    "# Calculate the accuracy of the network on some data\n",
    "def calculate_accuracy(outputs,targets, output_size):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy in categorization of some outputs given some targets.\n",
    "    \"\"\"\n",
    "    \n",
    "    # binarize the outputs for an easy calculation\n",
    "    categories = (outputs == cnp.tile(outputs.max(axis=0),(output_size,1))).astype('float')\n",
    "    \n",
    "    # get the accuracy\n",
    "    accuracy = cnp.sum(categories*targets)/targets.shape[1]\n",
    "    \n",
    "    return accuracy*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "# The main network class\n",
    "class MLP(object):\n",
    "    \"\"\"\n",
    "    The class for creating and training a two-layer perceptron.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The initialization function\n",
    "    def __init__(self,I, O, N,B,eta,alpha=1.0,sigma=1.0,algorithm='backprop'):\n",
    "        \"\"\"\n",
    "        The initialization function for the MLP.\n",
    "         - I is the size of input array\n",
    "         - O is the size of output array\n",
    "         - N is the number of hidden units\n",
    "         - B is the batch size\n",
    "         - eta is the learning rate\n",
    "         - alpha is the SD for initializing the weights\n",
    "         - sigma is the SD for weight perturbation functions\n",
    "         - algorithm is a string indicating which learning algorithm to use, \n",
    "           options are ('backprop','perturb','feedback'), will default to backprop\n",
    "        \"\"\" \n",
    "                      \n",
    "        # store the variables for easy access\n",
    "        self.I     = I\n",
    "        self.O     = O\n",
    "        self.N     = N\n",
    "        self.B     = B\n",
    "        self.eta   = eta\n",
    "        self.alpha = alpha\n",
    "        self.sigma = sigma\n",
    "        self.algorithm = algorithm\n",
    "        \n",
    "        # initialize the weights\n",
    "        self.W_h = cnp.random.normal(scale=self.alpha,size=(self.N, self.I+1)) # input-to-hidden weights & bias\n",
    "        self.W_y = cnp.random.normal(scale=self.alpha,size=(self.O,self.N+1))  # hidden-to-output weights & bias\n",
    "        self.V   = cnp.random.normal(scale=self.alpha,size=(self.N,self.O))    # feedback weights\n",
    "        \n",
    "    # The function for performing a forward pass up through the network during inference\n",
    "    def inference(self,inputs,W_h=None,W_y=None):\n",
    "        \"\"\"\n",
    "        Recognize inputs, i.e. do a forward pass up through the network. If desired, alternative weights\n",
    "        can be provided\n",
    "        \"\"\"\n",
    "        \n",
    "        # load the current weights if no weights given\n",
    "        if W_h is None:\n",
    "            W_h = self.W_h\n",
    "        if W_y is None:\n",
    "            W_y = self.W_y\n",
    "\n",
    "        hidden = sigmoid(cnp.dot(W_h,add_bias(inputs))) \n",
    "        \n",
    "        # calculate the output activities\n",
    "        output = sigmoid(cnp.dot(W_y,add_bias(hidden)))\n",
    "\n",
    "        return (hidden,output)\n",
    "                           \n",
    "    # the function for calculating the loss\n",
    "    def loss(self,inputs,targets,W_h=None,W_y=None):\n",
    "        \"\"\"\n",
    "        Calculate the mean-squared error loss on the given targets (average over the batch)\n",
    "        \"\"\"\n",
    "                      \n",
    "        # do a forward sweep through the network\n",
    "        (hidden,output) = self.inference(inputs,W_h,W_y)\n",
    "                      \n",
    "        return cnp.mean(cnp.sum((targets - output)**2,axis=0))\n",
    "    \n",
    "    # function for calculating perturbation updates\n",
    "    def perturb(self,inputs,targets):\n",
    "        \"\"\"\n",
    "        Calculates the weight updates for perturbation learning\n",
    "        \"\"\"\n",
    "        \n",
    "        # do a forward pass\n",
    "        (hidden,output) = self.inference(inputs)\n",
    "        \n",
    "        # get the random perturbations\n",
    "        delta_W_h = cnp.random.normal(0,self.sigma**2,self.W_h.shape)\n",
    "        delta_W_y = cnp.random.normal(0,self.sigma**2,self.W_y.shape)\n",
    "\n",
    "\n",
    "        prime_W_h = self.W_h + self.eta*delta_W_h\n",
    "        prime_W_y = self.W_y + self.eta*delta_W_y\n",
    "\n",
    "        delta_loss = self.loss(inputs=inputs,targets=targets) - self.loss(inputs=inputs,targets=targets,W_h=prime_W_h,W_y=prime_W_y)\n",
    "\n",
    "        tmp1 = delta_loss * delta_W_h\n",
    "        tmp2 = delta_loss * delta_W_y      \n",
    "\n",
    "        self.W_h += self.eta*tmp1\n",
    "        self.W_y += self.eta*tmp2\n",
    "\n",
    "        return\n",
    "                         \n",
    "    # function for calculating gradient updates\n",
    "    def gradient(self,inputs,targets):\n",
    "        \"\"\"\n",
    "        Calculates the weight updates for gradient descent learning\n",
    "        \"\"\"\n",
    "\n",
    "        # do a forward pass\n",
    "        (hidden,output) = self.inference(inputs)\n",
    "        \n",
    "        # calculate the gradients\n",
    "        delta_W_h = cnp.zeros(self.W_h.shape)\n",
    "        delta_W_y = cnp.zeros(self.W_y.shape)\n",
    "\n",
    "        delta_W_y_prime = - self.eta * cnp.dot((output - targets) * output * (1 - output), cnp.transpose(hidden))\n",
    "        self.W_y += cnp.pad(delta_W_y_prime, ((0, 0), (0, 1)), mode='constant')\n",
    "\n",
    "        delta_W_h_prime = -self.eta * cnp.dot(cnp.transpose(cnp.dot(cnp.transpose((output - targets) * output * (1 - output)), self.W_y[:, :-1])) *  hidden  * (1 - hidden), cnp.transpose(inputs))\n",
    "\n",
    "        self.W_h += cnp.pad(delta_W_h_prime, ((0, 0), (0, 1)), mode='constant')\n",
    "\n",
    "        \n",
    "        return\n",
    "    \n",
    "    # function for calculating feedback alignment updates\n",
    "    def feedback(self,inputs,targets):\n",
    "        \"\"\"\n",
    "        Calculates the weight updates for feedback alignment learning\n",
    "        \"\"\"\n",
    "        \n",
    "        # do a forward pass\n",
    "        (hidden,output) = self.inference(inputs)\n",
    "        \n",
    "        # calculate the updates\n",
    "        delta_W_h = cnp.zeros(self.W_h.shape)\n",
    "        delta_W_y = cnp.zeros(self.W_y.shape)\n",
    "\n",
    "        delta_W_y_prime = - self.eta * cnp.dot((output - targets) * output * (1 - output), cnp.transpose(hidden))\n",
    "        self.W_y += cnp.pad(delta_W_y_prime, ((0, 0), (0, 1)), mode='constant')\n",
    "\n",
    "        delta_W_h_prime = -self.eta * cnp.dot(np.transpose(cnp.dot(cnp.transpose((output - targets) * output * (1 - output)), self.V.T)) *  hidden  * (1 - hidden), cnp.transpose(inputs))\n",
    "\n",
    "        self.W_h += cnp.pad(delta_W_h_prime, ((0, 0), (0, 1)), mode='constant')\n",
    "                           \n",
    "        return\n",
    "    \n",
    "\n",
    "    # function for updating the network\n",
    "    def update(self,inputs,targets):\n",
    "                         \n",
    "        # calculate the updates for the weights with the appropriate algorithm\n",
    "        if self.algorithm == 'backprop':\n",
    "            self.gradient(inputs,targets)\n",
    "        elif self.algorithm == 'perturb':\n",
    "            self.perturb(inputs,targets)\n",
    "        elif self.algorithm == 'feedback':\n",
    "            self.feedback(inputs,targets)\n",
    "        else:\n",
    "            self.gradient(inputs,targets)\n",
    "            \n",
    "        # # do the updates (delta_W_h,delta_W_y) = \n",
    "        # self.W_h += self.eta*delta_W_h\n",
    "        # self.W_y += self.eta*delta_W_y\n",
    "    \n",
    "    # train the network using the update functions\n",
    "    def train(self,X,X_labels,num_epochs,Y,Y_labels,report=False,report_rate=10):\n",
    "        \"\"\"\n",
    "        Trains the network in batches for the given number of epochs on the data provided.\n",
    "        \n",
    "        Categorization accuracy on a test set is also calculated.\n",
    "        \n",
    "        Prints a message every report_rate epochs if requested.\n",
    "        \n",
    "        Returns an array of the losses achieved at each epoch (and accuracies if test data given).\n",
    "        \"\"\"\n",
    "        \n",
    "        # provide an output message\n",
    "        if report:\n",
    "            print(\"Training starting...\")\n",
    "        \n",
    "        # make batches from the data\n",
    "        batches = create_batches(self.B,X.shape[1])\n",
    "\n",
    "        batch_num = batches.shape[0]\n",
    "        \n",
    "        # create arrays to store loss and accuracy values\n",
    "        losses   = cnp.zeros((num_epochs*batch_num,))\n",
    "        accuracy = cnp.zeros((num_epochs,))\n",
    "        \n",
    "        # run the training for the given number of epochs\n",
    "        update_counter = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            # step through each batch\n",
    "            print(\"epoch \", epoch, \" total number of batches is \", batch_num)\n",
    "            for b in range(batch_num):\n",
    "                # print(\"batch\", b)\n",
    "\n",
    "                inputs  = X[:, batches[b,:]]\n",
    "\n",
    "                targets = X_labels[:,batches[b,:]]\n",
    "\n",
    "                losses[update_counter] = self.loss(inputs,targets)\n",
    "                \n",
    "                # update the weights\n",
    "                # print(\"doing inference for update\")\n",
    "                self.update(inputs,targets)\n",
    "                update_counter += 1\n",
    "                \n",
    "            # calculate the current test accuracy\n",
    "            (testhid,testout) = self.inference(Y)\n",
    "            accuracy[epoch]   = calculate_accuracy(testout,Y_labels, self.O)\n",
    "                \n",
    "            # print an output message every 10 epochs\n",
    "            # if report and np.mod(epoch+1,report_rate) == 0:\n",
    "            if report and cnp.mod(epoch+1,report_rate) == 0:\n",
    "                print(\"...completed \", epoch+1, \n",
    "                    #   \" epochs of training. Current loss: \",(losses[update_counter-1],2), \".\")\n",
    "                    \" epochs of training. Current loss: \",losses[update_counter-1], \".\")\n",
    "                \n",
    "        # provide an output message\n",
    "        if report:\n",
    "            print(\"Training complete.\")\n",
    "        \n",
    "        return (losses,accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " # The part for perturbation method is commented out since this method is not very relevant for the research objective. It works, just need to uncomment\n",
    "def show_results(losses_perturb, accuracy_perturb, losses_backprop, accuracy_backprop,losses_feedback, accuracy_feedback, numupdates, numepochs, type_data,alpha,num_hidden):\n",
    "    # calculate the means and standard deviations\n",
    "    # avg_loss_perturb = cnp.mean(losses_perturb,axis=1)\n",
    "    # std_loss_perturb = cnp.std(losses_perturb,axis=1) \n",
    "\n",
    "    avg_loss_backprop = cnp.mean(losses_backprop,axis=1)\n",
    "    std_loss_backprop = cnp.std(losses_backprop,axis=1)\n",
    "\n",
    "    avg_loss_feedback = cnp.mean(losses_feedback,axis=1)\n",
    "    std_loss_feedback = cnp.std(losses_feedback,axis=1)\n",
    "\n",
    "\n",
    "    # avg_acc_perturb = cnp.mean(accuracy_perturb,axis=1)\n",
    "    # std_acc_perturb = cnp.std(accuracy_perturb,axis=1)\n",
    "\n",
    "    avg_acc_backprop = cnp.mean(accuracy_backprop,axis=1)\n",
    "    std_acc_backprop = cnp.std(accuracy_backprop,axis=1)\n",
    "\n",
    "    avg_acc_feedback = cnp.mean(accuracy_feedback,axis=1)\n",
    "    std_acc_feedback = cnp.std(accuracy_feedback,axis=1)\n",
    "\n",
    "    # Plot the losses over training with shaded error regions\n",
    "    if cp.is_available():\n",
    "        # plt.plot(avg_loss_perturb.get(), label=\"Perturbation\",color='b')\n",
    "        # plt.fill_between(range(numupdates), avg_loss_perturb.get() - std_loss_perturb.get(), avg_loss_perturb.get() + std_loss_perturb.get(), color='b',alpha=0.5)\n",
    "\n",
    "        plt.plot(avg_loss_backprop.get(), label=\"Gradient Descent\",color='r')\n",
    "        plt.fill_between(range(numupdates),avg_loss_backprop.get() - std_loss_backprop.get(), avg_loss_backprop.get() + std_loss_backprop.get(),color='r',alpha=0.5)\n",
    "\n",
    "        plt.plot(avg_loss_feedback.get(), label=\"Feedback Alignment\",color='g')\n",
    "        plt.fill_between(range(numupdates), avg_loss_feedback.get() - std_loss_feedback.get() ,avg_loss_feedback.get() + std_loss_feedback.get(), color='g',alpha=0.5)\n",
    "\n",
    "    else:\n",
    "        # plt.plot(avg_loss_perturb,label=\"Perturbation\",color='b')\n",
    "        # plt.fill_between(range(numupdates),avg_loss_perturb.get()-std_loss_perturb.get(),avg_loss_perturb.get()+std_loss_perturb.get(),color='b',alpha=0.5)\n",
    "\n",
    "        plt.plot(avg_loss_backprop,label=\"Gradient Descent\",color='r')\n",
    "        plt.fill_between(range(numupdates),avg_loss_backprop-std_loss_backprop,avg_loss_backprop+std_loss_backprop,color='r',alpha=0.5)\n",
    "        \n",
    "        plt.plot(avg_loss_feedback,label=\"Feedback Alignment\",color='g')\n",
    "        plt.fill_between(range(numupdates),avg_loss_feedback-std_loss_feedback,avg_loss_feedback+std_loss_feedback,color='g',alpha=0.5)\n",
    "\n",
    "    # ax = plt.gca()\n",
    "    # # ax.set_xlim([xmin, xmax])\n",
    "    # ax.set_ylim([0, 100])\n",
    "    plt.xlabel(\"Updates\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Training loss on {type_data} data (a={alpha}, hidden unit = {num_hidden})\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the accuracies over training with shaded error regions\n",
    "    if cp.is_available():\n",
    "        # plt.plot(avg_acc_perturb.get(), label=\"Perturbation\",color='b')\n",
    "        # plt.fill_between(range(numepochs), avg_acc_perturb.get() - std_acc_perturb.get(), avg_acc_perturb.get() + std_acc_perturb.get(), color='b',alpha=0.5)\n",
    "        plt.plot(avg_acc_backprop.get(), label=\"Gradient Descent\",color='r')\n",
    "        plt.fill_between(range(numepochs), avg_acc_backprop.get() - std_acc_backprop.get(), avg_acc_backprop.get() + std_acc_backprop.get(), color='r',alpha=0.5)\n",
    "        plt.plot(avg_acc_feedback.get(), label=\"Feedback Alignment\",color='g')\n",
    "        plt.fill_between(range(numepochs), avg_acc_feedback .get()- std_acc_feedback.get(), avg_acc_feedback.get() + std_acc_feedback.get(),color='g',alpha=0.5)\n",
    "\n",
    "    else:\n",
    "        # Plot the accuracies over training with shaded error regions\n",
    "        # plt.plot(avg_acc_perturb,label=\"Perturbation\",color='b')\n",
    "        # plt.fill_between(range(numepochs), avg_acc_perturb - std_acc_perturb, avg_acc_perturb + std_acc_perturb, color='b',alpha=0.5)\n",
    "        plt.plot(avg_acc_backprop,label=\"Gradient Descent\",color='r')\n",
    "        plt.fill_between(range(numepochs), avg_acc_backprop - std_acc_backprop, avg_acc_backprop + std_acc_backprop,color='r',alpha=0.5)\n",
    "        plt.plot(avg_acc_feedback,label=\"Feedback Alignment\",color='g')\n",
    "        plt.fill_between(range(numepochs), avg_acc_feedback - std_acc_feedback, avg_acc_feedback + std_acc_feedback, color='g',alpha=0.5)\n",
    "    ax = plt.gca()\n",
    "    # ax.set_xlim([xmin, xmax])\n",
    "    ax.set_ylim([0, 100])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Test accuracy on {type_data} data (a={alpha}, hidden unit = {num_hidden})\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The cell below is to tune the hyper parameters for training. Changing it affects both the image and the text sections.\n",
    "# You might need a CUDA enabled good GPU to run anything more than 30 epochs effectively for the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters. change these to tune \n",
    "numhidden  = 100 #original is 100\n",
    "batchsize  = 500 #orignal is 100\n",
    "initweight = 0.1\n",
    "learnrate  = 0.001\n",
    "numepochs  = 500 #original is 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The below sections are for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fears for T N pension after talks\n"
     ]
    }
   ],
   "source": [
    "# Load the AG news Corpus\n",
    "\n",
    "# Initialize empty arrays for the first and second columns\n",
    "train_labels_txt = []\n",
    "train_corpus_txt = []\n",
    "\n",
    "test_labels_txt = []\n",
    "test_corpus_txt = []\n",
    "\n",
    "tmp_train_l = []\n",
    "tmp_test_l = []\n",
    "\n",
    "\n",
    "with open('agnews/train.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        output_network = [0,0,0,0]\n",
    "        output_network[int(row[0]) - 1] = 1 # label is 1-4, so minus 1 to get index\n",
    "        train_labels_txt.append(output_network)\n",
    "        train_corpus_txt.append(row[1])\n",
    "        tmp_train_l.append(int(row[0]))\n",
    "\n",
    "with open('agnews/test.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        output_network = [0,0,0,0]\n",
    "        output_network[int(row[0]) - 1] = 1 # label is 1-4, so minus 1 to get index\n",
    "        test_labels_txt.append(output_network)\n",
    "        test_corpus_txt.append(row[1])\n",
    "        tmp_test_l.append(int(row[0]))\n",
    "\n",
    "print(test_corpus_txt[0])\n",
    "\n",
    "train_corpus_txt = np.array(train_corpus_txt)[0:10000,]\n",
    "train_labels_txt = np.array(train_labels_txt)[0:10000,]\n",
    "test_corpus_txt = np.array(test_corpus_txt)[0:1000,]\n",
    "test_labels_txt = np.array(test_labels_txt)[0:1000,]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felis/Programming/work/university/comp550/.venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/felis/Programming/work/university/comp550/.venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Text preprocessing\n",
    "tf_vectorizer = TfidfVectorizer(tokenizer=tokenize_lemma_removepunc,stop_words='english',ngram_range=(1,1))\n",
    "\n",
    "x_train_tf = tf_vectorizer.fit_transform(train_corpus_txt)\n",
    "\n",
    "x_test_tf = tf_vectorizer.transform(test_corpus_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 9640)\n",
      "(10000, 4)\n",
      "(9640, 10000)\n",
      "(4, 10000)\n",
      "(9640, 1000)\n",
      "(4, 1000)\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train_corpus = x_train_tf.toarray().transpose()\n",
    "x_train_labels = train_labels_txt.transpose()\n",
    "\n",
    "y_test_corpus = x_test_tf.toarray().transpose()\n",
    "y_test_labels = test_labels_txt.transpose()\n",
    "\n",
    "print(x_train_tf.shape)\n",
    "print(train_labels_txt.shape)\n",
    "\n",
    "print(x_train_corpus.shape)\n",
    "print(x_train_labels.shape)\n",
    "print(y_test_corpus.shape)\n",
    "print(y_test_labels.shape)\n",
    "\n",
    "\n",
    "print(x_train_corpus[1])\n",
    "print(x_train_labels[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_corpus = cnp.asarray(x_train_corpus)\n",
    "x_train_labels = cnp.asarray(x_train_labels)\n",
    "y_test_corpus = cnp.asarray(y_test_corpus)\n",
    "y_test_labels = cnp.asarray(y_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total iteration to be run per training:  10000\n",
      "Training starting...\n",
      "epoch  0  total number of batches is  20\n",
      "...completed  1  epochs of training. Current loss:  0.7518524444448847 .\n",
      "epoch  1  total number of batches is  20\n",
      "...completed  2  epochs of training. Current loss:  0.7517739193388175 .\n",
      "epoch  2  total number of batches is  20\n",
      "...completed  3  epochs of training. Current loss:  0.7516951470928227 .\n",
      "epoch  3  total number of batches is  20\n",
      "...completed  4  epochs of training. Current loss:  0.7516163553180764 .\n",
      "epoch  4  total number of batches is  20\n",
      "...completed  5  epochs of training. Current loss:  0.7515375244761354 .\n",
      "epoch  5  total number of batches is  20\n",
      "...completed  6  epochs of training. Current loss:  0.7514586350018403 .\n",
      "epoch  6  total number of batches is  20\n",
      "...completed  7  epochs of training. Current loss:  0.7513796673110846 .\n",
      "epoch  7  total number of batches is  20\n",
      "...completed  8  epochs of training. Current loss:  0.7513006017959311 .\n",
      "epoch  8  total number of batches is  20\n",
      "...completed  9  epochs of training. Current loss:  0.7512214188197124 .\n",
      "epoch  9  total number of batches is  20\n",
      "...completed  10  epochs of training. Current loss:  0.7511420987121189 .\n",
      "epoch  10  total number of batches is  20\n",
      "...completed  11  epochs of training. Current loss:  0.7510626217642681 .\n",
      "epoch  11  total number of batches is  20\n",
      "...completed  12  epochs of training. Current loss:  0.7509829682237602 .\n",
      "epoch  12  total number of batches is  20\n",
      "...completed  13  epochs of training. Current loss:  0.7509031182897135 .\n",
      "epoch  13  total number of batches is  20\n",
      "...completed  14  epochs of training. Current loss:  0.7508230521077814 .\n",
      "epoch  14  total number of batches is  20\n",
      "...completed  15  epochs of training. Current loss:  0.750742749765151 .\n",
      "epoch  15  total number of batches is  20\n",
      "...completed  16  epochs of training. Current loss:  0.7506621912855177 .\n",
      "epoch  16  total number of batches is  20\n",
      "...completed  17  epochs of training. Current loss:  0.750581356624039 .\n",
      "epoch  17  total number of batches is  20\n",
      "...completed  18  epochs of training. Current loss:  0.750500225662265 .\n",
      "epoch  18  total number of batches is  20\n",
      "...completed  19  epochs of training. Current loss:  0.750418778203044 .\n",
      "epoch  19  total number of batches is  20\n",
      "...completed  20  epochs of training. Current loss:  0.7503369939654015 .\n",
      "epoch  20  total number of batches is  20\n",
      "...completed  21  epochs of training. Current loss:  0.7502548525793932 .\n",
      "epoch  21  total number of batches is  20\n",
      "...completed  22  epochs of training. Current loss:  0.7501723335809303 .\n",
      "epoch  22  total number of batches is  20\n",
      "...completed  23  epochs of training. Current loss:  0.7500894164065739 .\n",
      "epoch  23  total number of batches is  20\n",
      "...completed  24  epochs of training. Current loss:  0.7500060803883004 .\n",
      "epoch  24  total number of batches is  20\n",
      "...completed  25  epochs of training. Current loss:  0.7499223047482345 .\n",
      "epoch  25  total number of batches is  20\n",
      "...completed  26  epochs of training. Current loss:  0.7498380685933491 .\n",
      "epoch  26  total number of batches is  20\n",
      "...completed  27  epochs of training. Current loss:  0.7497533509101323 .\n",
      "epoch  27  total number of batches is  20\n",
      "...completed  28  epochs of training. Current loss:  0.7496681305592177 .\n",
      "epoch  28  total number of batches is  20\n",
      "...completed  29  epochs of training. Current loss:  0.7495823862699784 .\n",
      "epoch  29  total number of batches is  20\n",
      "...completed  30  epochs of training. Current loss:  0.7494960966350847 .\n",
      "epoch  30  total number of batches is  20\n",
      "...completed  31  epochs of training. Current loss:  0.7494092401050205 .\n",
      "epoch  31  total number of batches is  20\n",
      "...completed  32  epochs of training. Current loss:  0.7493217949825618 .\n",
      "epoch  32  total number of batches is  20\n",
      "...completed  33  epochs of training. Current loss:  0.749233739417211 .\n",
      "epoch  33  total number of batches is  20\n",
      "...completed  34  epochs of training. Current loss:  0.7491450513995922 .\n",
      "epoch  34  total number of batches is  20\n",
      "...completed  35  epochs of training. Current loss:  0.7490557087557981 .\n",
      "epoch  35  total number of batches is  20\n",
      "...completed  36  epochs of training. Current loss:  0.7489656891416958 .\n",
      "epoch  36  total number of batches is  20\n",
      "...completed  37  epochs of training. Current loss:  0.748874970037183 .\n",
      "epoch  37  total number of batches is  20\n",
      "...completed  38  epochs of training. Current loss:  0.748783528740398 .\n",
      "epoch  38  total number of batches is  20\n",
      "...completed  39  epochs of training. Current loss:  0.7486913423618805 .\n",
      "epoch  39  total number of batches is  20\n",
      "...completed  40  epochs of training. Current loss:  0.7485983878186818 .\n",
      "epoch  40  total number of batches is  20\n",
      "...completed  41  epochs of training. Current loss:  0.7485046418284235 .\n",
      "epoch  41  total number of batches is  20\n",
      "...completed  42  epochs of training. Current loss:  0.7484100809033032 .\n",
      "epoch  42  total number of batches is  20\n",
      "...completed  43  epochs of training. Current loss:  0.7483146813440457 .\n",
      "epoch  43  total number of batches is  20\n",
      "...completed  44  epochs of training. Current loss:  0.7482184192338012 .\n",
      "epoch  44  total number of batches is  20\n",
      "...completed  45  epochs of training. Current loss:  0.7481212704319826 .\n",
      "epoch  45  total number of batches is  20\n",
      "...completed  46  epochs of training. Current loss:  0.7480232105680489 .\n",
      "epoch  46  total number of batches is  20\n",
      "...completed  47  epochs of training. Current loss:  0.7479242150352269 .\n",
      "epoch  47  total number of batches is  20\n",
      "...completed  48  epochs of training. Current loss:  0.7478242589841736 .\n",
      "epoch  48  total number of batches is  20\n",
      "...completed  49  epochs of training. Current loss:  0.7477233173165766 .\n",
      "epoch  49  total number of batches is  20\n",
      "...completed  50  epochs of training. Current loss:  0.7476213646786914 .\n",
      "epoch  50  total number of batches is  20\n",
      "...completed  51  epochs of training. Current loss:  0.7475183754548143 .\n",
      "epoch  51  total number of batches is  20\n",
      "...completed  52  epochs of training. Current loss:  0.7474143237606908 .\n",
      "epoch  52  total number of batches is  20\n",
      "...completed  53  epochs of training. Current loss:  0.7473091834368567 .\n",
      "epoch  53  total number of batches is  20\n",
      "...completed  54  epochs of training. Current loss:  0.7472029280419107 .\n",
      "epoch  54  total number of batches is  20\n",
      "...completed  55  epochs of training. Current loss:  0.7470955308457196 .\n",
      "epoch  55  total number of batches is  20\n",
      "...completed  56  epochs of training. Current loss:  0.7469869648225511 .\n",
      "epoch  56  total number of batches is  20\n",
      "...completed  57  epochs of training. Current loss:  0.7468772026441386 .\n",
      "epoch  57  total number of batches is  20\n",
      "...completed  58  epochs of training. Current loss:  0.7467662166726691 .\n",
      "epoch  58  total number of batches is  20\n",
      "...completed  59  epochs of training. Current loss:  0.746653978953701 .\n",
      "epoch  59  total number of batches is  20\n",
      "...completed  60  epochs of training. Current loss:  0.7465404612090054 .\n",
      "epoch  60  total number of batches is  20\n",
      "...completed  61  epochs of training. Current loss:  0.7464256348293331 .\n",
      "epoch  61  total number of batches is  20\n",
      "...completed  62  epochs of training. Current loss:  0.7463094708671024 .\n",
      "epoch  62  total number of batches is  20\n",
      "...completed  63  epochs of training. Current loss:  0.7461919400290108 .\n",
      "epoch  63  total number of batches is  20\n",
      "...completed  64  epochs of training. Current loss:  0.7460730126685685 .\n",
      "epoch  64  total number of batches is  20\n",
      "...completed  65  epochs of training. Current loss:  0.7459526587785497 .\n",
      "epoch  65  total number of batches is  20\n",
      "...completed  66  epochs of training. Current loss:  0.7458308479833661 .\n",
      "epoch  66  total number of batches is  20\n",
      "...completed  67  epochs of training. Current loss:  0.7457075495313558 .\n",
      "epoch  67  total number of batches is  20\n",
      "...completed  68  epochs of training. Current loss:  0.745582732286993 .\n",
      "epoch  68  total number of batches is  20\n",
      "...completed  69  epochs of training. Current loss:  0.7454563647230127 .\n",
      "epoch  69  total number of batches is  20\n",
      "...completed  70  epochs of training. Current loss:  0.745328414912451 .\n",
      "epoch  70  total number of batches is  20\n",
      "...completed  71  epochs of training. Current loss:  0.7451988505206032 .\n",
      "epoch  71  total number of batches is  20\n",
      "...completed  72  epochs of training. Current loss:  0.7450676387968947 .\n",
      "epoch  72  total number of batches is  20\n",
      "...completed  73  epochs of training. Current loss:  0.7449347465666677 .\n",
      "epoch  73  total number of batches is  20\n",
      "...completed  74  epochs of training. Current loss:  0.7448001402228818 .\n",
      "epoch  74  total number of batches is  20\n",
      "...completed  75  epochs of training. Current loss:  0.7446637857177256 .\n",
      "epoch  75  total number of batches is  20\n",
      "...completed  76  epochs of training. Current loss:  0.7445256485541469 .\n",
      "epoch  76  total number of batches is  20\n",
      "...completed  77  epochs of training. Current loss:  0.7443856937772904 .\n",
      "epoch  77  total number of batches is  20\n",
      "...completed  78  epochs of training. Current loss:  0.7442438859658517 .\n",
      "epoch  78  total number of batches is  20\n",
      "...completed  79  epochs of training. Current loss:  0.7441001892233439 .\n",
      "epoch  79  total number of batches is  20\n",
      "...completed  80  epochs of training. Current loss:  0.7439545671692757 .\n",
      "epoch  80  total number of batches is  20\n",
      "...completed  81  epochs of training. Current loss:  0.7438069829302452 .\n",
      "epoch  81  total number of batches is  20\n",
      "...completed  82  epochs of training. Current loss:  0.7436573991309449 .\n",
      "epoch  82  total number of batches is  20\n",
      "...completed  83  epochs of training. Current loss:  0.743505777885083 .\n",
      "epoch  83  total number of batches is  20\n",
      "...completed  84  epochs of training. Current loss:  0.7433520807862167 .\n",
      "epoch  84  total number of batches is  20\n",
      "...completed  85  epochs of training. Current loss:  0.7431962688985042 .\n",
      "epoch  85  total number of batches is  20\n",
      "...completed  86  epochs of training. Current loss:  0.74303830274737 .\n",
      "epoch  86  total number of batches is  20\n",
      "...completed  87  epochs of training. Current loss:  0.7428781423100896 .\n",
      "epoch  87  total number of batches is  20\n",
      "...completed  88  epochs of training. Current loss:  0.7427157470062921 .\n",
      "epoch  88  total number of batches is  20\n",
      "...completed  89  epochs of training. Current loss:  0.7425510756883822 .\n",
      "epoch  89  total number of batches is  20\n",
      "...completed  90  epochs of training. Current loss:  0.7423840866318853 .\n",
      "epoch  90  total number of batches is  20\n",
      "...completed  91  epochs of training. Current loss:  0.742214737525715 .\n",
      "epoch  91  total number of batches is  20\n",
      "...completed  92  epochs of training. Current loss:  0.7420429854623664 .\n",
      "epoch  92  total number of batches is  20\n",
      "...completed  93  epochs of training. Current loss:  0.7418687869280373 .\n",
      "epoch  93  total number of batches is  20\n",
      "...completed  94  epochs of training. Current loss:  0.7416920977926802 .\n",
      "epoch  94  total number of batches is  20\n",
      "...completed  95  epochs of training. Current loss:  0.7415128732999859 .\n",
      "epoch  95  total number of batches is  20\n",
      "...completed  96  epochs of training. Current loss:  0.7413310680573062 .\n",
      "epoch  96  total number of batches is  20\n",
      "...completed  97  epochs of training. Current loss:  0.7411466360255129 .\n",
      "epoch  97  total number of batches is  20\n",
      "...completed  98  epochs of training. Current loss:  0.7409595305088026 .\n",
      "epoch  98  total number of batches is  20\n",
      "...completed  99  epochs of training. Current loss:  0.7407697041444474 .\n",
      "epoch  99  total number of batches is  20\n",
      "...completed  100  epochs of training. Current loss:  0.7405771088924976 .\n",
      "epoch  100  total number of batches is  20\n",
      "...completed  101  epochs of training. Current loss:  0.7403816960254412 .\n",
      "epoch  101  total number of batches is  20\n",
      "...completed  102  epochs of training. Current loss:  0.7401834161178237 .\n",
      "epoch  102  total number of batches is  20\n",
      "...completed  103  epochs of training. Current loss:  0.7399822190358364 .\n",
      "epoch  103  total number of batches is  20\n",
      "...completed  104  epochs of training. Current loss:  0.7397780539268756 .\n",
      "epoch  104  total number of batches is  20\n",
      "...completed  105  epochs of training. Current loss:  0.7395708692090828 .\n",
      "epoch  105  total number of batches is  20\n",
      "...completed  106  epochs of training. Current loss:  0.7393606125608702 .\n",
      "epoch  106  total number of batches is  20\n",
      "...completed  107  epochs of training. Current loss:  0.7391472309104387 .\n",
      "epoch  107  total number of batches is  20\n",
      "...completed  108  epochs of training. Current loss:  0.7389306704252993 .\n",
      "epoch  108  total number of batches is  20\n",
      "...completed  109  epochs of training. Current loss:  0.7387108765018013 .\n",
      "epoch  109  total number of batches is  20\n",
      "...completed  110  epochs of training. Current loss:  0.7384877937546823 .\n",
      "epoch  110  total number of batches is  20\n",
      "...completed  111  epochs of training. Current loss:  0.738261366006643 .\n",
      "epoch  111  total number of batches is  20\n",
      "...completed  112  epochs of training. Current loss:  0.7380315362779644 .\n",
      "epoch  112  total number of batches is  20\n",
      "...completed  113  epochs of training. Current loss:  0.7377982467761695 .\n",
      "epoch  113  total number of batches is  20\n",
      "...completed  114  epochs of training. Current loss:  0.7375614388857507 .\n",
      "epoch  114  total number of batches is  20\n",
      "...completed  115  epochs of training. Current loss:  0.7373210531579668 .\n",
      "epoch  115  total number of batches is  20\n",
      "...completed  116  epochs of training. Current loss:  0.7370770293007293 .\n",
      "epoch  116  total number of batches is  20\n",
      "...completed  117  epochs of training. Current loss:  0.7368293061685851 .\n",
      "epoch  117  total number of batches is  20\n",
      "...completed  118  epochs of training. Current loss:  0.7365778217528184 .\n",
      "epoch  118  total number of batches is  20\n",
      "...completed  119  epochs of training. Current loss:  0.7363225131716802 .\n",
      "epoch  119  total number of batches is  20\n",
      "...completed  120  epochs of training. Current loss:  0.7360633166607649 .\n",
      "epoch  120  total number of batches is  20\n",
      "...completed  121  epochs of training. Current loss:  0.7358001675635538 .\n",
      "epoch  121  total number of batches is  20\n",
      "...completed  122  epochs of training. Current loss:  0.7355330003221386 .\n",
      "epoch  122  total number of batches is  20\n",
      "...completed  123  epochs of training. Current loss:  0.7352617484681495 .\n",
      "epoch  123  total number of batches is  20\n",
      "...completed  124  epochs of training. Current loss:  0.7349863446139051 .\n",
      "epoch  124  total number of batches is  20\n",
      "...completed  125  epochs of training. Current loss:  0.7347067204438079 .\n",
      "epoch  125  total number of batches is  20\n",
      "...completed  126  epochs of training. Current loss:  0.7344228067060069 .\n",
      "epoch  126  total number of batches is  20\n",
      "...completed  127  epochs of training. Current loss:  0.7341345332043533 .\n",
      "epoch  127  total number of batches is  20\n",
      "...completed  128  epochs of training. Current loss:  0.733841828790671 .\n",
      "epoch  128  total number of batches is  20\n",
      "...completed  129  epochs of training. Current loss:  0.7335446213573741 .\n",
      "epoch  129  total number of batches is  20\n",
      "...completed  130  epochs of training. Current loss:  0.7332428378304542 .\n",
      "epoch  130  total number of batches is  20\n",
      "...completed  131  epochs of training. Current loss:  0.7329364041628694 .\n",
      "epoch  131  total number of batches is  20\n",
      "...completed  132  epochs of training. Current loss:  0.7326252453283671 .\n",
      "epoch  132  total number of batches is  20\n",
      "...completed  133  epochs of training. Current loss:  0.7323092853157709 .\n",
      "epoch  133  total number of batches is  20\n",
      "...completed  134  epochs of training. Current loss:  0.7319884471237655 .\n",
      "epoch  134  total number of batches is  20\n",
      "...completed  135  epochs of training. Current loss:  0.7316626527562181 .\n",
      "epoch  135  total number of batches is  20\n",
      "...completed  136  epochs of training. Current loss:  0.7313318232180703 .\n",
      "epoch  136  total number of batches is  20\n",
      "...completed  137  epochs of training. Current loss:  0.7309958785118412 .\n",
      "epoch  137  total number of batches is  20\n",
      "...completed  138  epochs of training. Current loss:  0.730654737634782 .\n",
      "epoch  138  total number of batches is  20\n",
      "...completed  139  epochs of training. Current loss:  0.7303083185767254 .\n",
      "epoch  139  total number of batches is  20\n",
      "...completed  140  epochs of training. Current loss:  0.7299565383186726 .\n",
      "epoch  140  total number of batches is  20\n",
      "...completed  141  epochs of training. Current loss:  0.7295993128321647 .\n",
      "epoch  141  total number of batches is  20\n",
      "...completed  142  epochs of training. Current loss:  0.7292365570794891 .\n",
      "epoch  142  total number of batches is  20\n",
      "...completed  143  epochs of training. Current loss:  0.7288681850147657 .\n",
      "epoch  143  total number of batches is  20\n",
      "...completed  144  epochs of training. Current loss:  0.7284941095859702 .\n",
      "epoch  144  total number of batches is  20\n",
      "...completed  145  epochs of training. Current loss:  0.7281142427379453 .\n",
      "epoch  145  total number of batches is  20\n",
      "...completed  146  epochs of training. Current loss:  0.7277284954164587 .\n",
      "epoch  146  total number of batches is  20\n",
      "...completed  147  epochs of training. Current loss:  0.7273367775733606 .\n",
      "epoch  147  total number of batches is  20\n",
      "...completed  148  epochs of training. Current loss:  0.7269389981729076 .\n",
      "epoch  148  total number of batches is  20\n",
      "...completed  149  epochs of training. Current loss:  0.7265350651993101 .\n",
      "epoch  149  total number of batches is  20\n",
      "...completed  150  epochs of training. Current loss:  0.7261248856655693 .\n",
      "epoch  150  total number of batches is  20\n",
      "...completed  151  epochs of training. Current loss:  0.7257083656236691 .\n",
      "epoch  151  total number of batches is  20\n",
      "...completed  152  epochs of training. Current loss:  0.7252854101761925 .\n",
      "epoch  152  total number of batches is  20\n",
      "...completed  153  epochs of training. Current loss:  0.7248559234894288 .\n",
      "epoch  153  total number of batches is  20\n",
      "...completed  154  epochs of training. Current loss:  0.7244198088080482 .\n",
      "epoch  154  total number of batches is  20\n",
      "...completed  155  epochs of training. Current loss:  0.7239769684714136 .\n",
      "epoch  155  total number of batches is  20\n",
      "...completed  156  epochs of training. Current loss:  0.7235273039316061 .\n",
      "epoch  156  total number of batches is  20\n",
      "...completed  157  epochs of training. Current loss:  0.7230707157732422 .\n",
      "epoch  157  total number of batches is  20\n",
      "...completed  158  epochs of training. Current loss:  0.722607103735157 .\n",
      "epoch  158  total number of batches is  20\n",
      "...completed  159  epochs of training. Current loss:  0.7221363667340395 .\n",
      "epoch  159  total number of batches is  20\n",
      "...completed  160  epochs of training. Current loss:  0.7216584028900938 .\n",
      "epoch  160  total number of batches is  20\n",
      "...completed  161  epochs of training. Current loss:  0.7211731095548135 .\n",
      "epoch  161  total number of batches is  20\n",
      "...completed  162  epochs of training. Current loss:  0.720680383340949 .\n",
      "epoch  162  total number of batches is  20\n",
      "...completed  163  epochs of training. Current loss:  0.7201801201547497 .\n",
      "epoch  163  total number of batches is  20\n",
      "...completed  164  epochs of training. Current loss:  0.7196722152305688 .\n",
      "epoch  164  total number of batches is  20\n",
      "...completed  165  epochs of training. Current loss:  0.7191565631679097 .\n",
      "epoch  165  total number of batches is  20\n",
      "...completed  166  epochs of training. Current loss:  0.7186330579709999 .\n",
      "epoch  166  total number of batches is  20\n",
      "...completed  167  epochs of training. Current loss:  0.7181015930909725 .\n",
      "epoch  167  total number of batches is  20\n",
      "...completed  168  epochs of training. Current loss:  0.7175620614707402 .\n",
      "epoch  168  total number of batches is  20\n",
      "...completed  169  epochs of training. Current loss:  0.7170143555926377 .\n",
      "epoch  169  total number of batches is  20\n",
      "...completed  170  epochs of training. Current loss:  0.7164583675289138 .\n",
      "epoch  170  total number of batches is  20\n",
      "...completed  171  epochs of training. Current loss:  0.7158939889951487 .\n",
      "epoch  171  total number of batches is  20\n",
      "...completed  172  epochs of training. Current loss:  0.7153211114066679 .\n",
      "epoch  172  total number of batches is  20\n",
      "...completed  173  epochs of training. Current loss:  0.7147396259380242 .\n",
      "epoch  173  total number of batches is  20\n",
      "...completed  174  epochs of training. Current loss:  0.7141494235856131 .\n",
      "epoch  174  total number of batches is  20\n",
      "...completed  175  epochs of training. Current loss:  0.7135503952334812 .\n",
      "epoch  175  total number of batches is  20\n",
      "...completed  176  epochs of training. Current loss:  0.7129424317223892 .\n",
      "epoch  176  total number of batches is  20\n",
      "...completed  177  epochs of training. Current loss:  0.7123254239221721 .\n",
      "epoch  177  total number of batches is  20\n",
      "...completed  178  epochs of training. Current loss:  0.7116992628074458 .\n",
      "epoch  178  total number of batches is  20\n",
      "...completed  179  epochs of training. Current loss:  0.7110638395366967 .\n",
      "epoch  179  total number of batches is  20\n",
      "...completed  180  epochs of training. Current loss:  0.7104190455347812 .\n",
      "epoch  180  total number of batches is  20\n",
      "...completed  181  epochs of training. Current loss:  0.7097647725788547 .\n",
      "epoch  181  total number of batches is  20\n",
      "...completed  182  epochs of training. Current loss:  0.7091009128877435 .\n",
      "epoch  182  total number of batches is  20\n",
      "...completed  183  epochs of training. Current loss:  0.7084273592147569 .\n",
      "epoch  183  total number of batches is  20\n",
      "...completed  184  epochs of training. Current loss:  0.7077440049439314 .\n",
      "epoch  184  total number of batches is  20\n",
      "...completed  185  epochs of training. Current loss:  0.707050744189683 .\n",
      "epoch  185  total number of batches is  20\n",
      "...completed  186  epochs of training. Current loss:  0.7063474718998317 .\n",
      "epoch  186  total number of batches is  20\n",
      "...completed  187  epochs of training. Current loss:  0.705634083961954 .\n",
      "epoch  187  total number of batches is  20\n",
      "...completed  188  epochs of training. Current loss:  0.7049104773129943 .\n",
      "epoch  188  total number of batches is  20\n",
      "...completed  189  epochs of training. Current loss:  0.7041765500520641 .\n",
      "epoch  189  total number of batches is  20\n",
      "...completed  190  epochs of training. Current loss:  0.7034322015563343 .\n",
      "epoch  190  total number of batches is  20\n",
      "...completed  191  epochs of training. Current loss:  0.7026773325999096 .\n",
      "epoch  191  total number of batches is  20\n",
      "...completed  192  epochs of training. Current loss:  0.7019118454755631 .\n",
      "epoch  192  total number of batches is  20\n",
      "...completed  193  epochs of training. Current loss:  0.7011356441191872 .\n",
      "epoch  193  total number of batches is  20\n",
      "...completed  194  epochs of training. Current loss:  0.7003486342368037 .\n",
      "epoch  194  total number of batches is  20\n",
      "...completed  195  epochs of training. Current loss:  0.6995507234339493 .\n",
      "epoch  195  total number of batches is  20\n",
      "...completed  196  epochs of training. Current loss:  0.6987418213472508 .\n",
      "epoch  196  total number of batches is  20\n",
      "...completed  197  epochs of training. Current loss:  0.6979218397779694 .\n",
      "epoch  197  total number of batches is  20\n",
      "...completed  198  epochs of training. Current loss:  0.6970906928272866 .\n",
      "epoch  198  total number of batches is  20\n",
      "...completed  199  epochs of training. Current loss:  0.6962482970330846 .\n",
      "epoch  199  total number of batches is  20\n",
      "...completed  200  epochs of training. Current loss:  0.6953945715079519 .\n",
      "epoch  200  total number of batches is  20\n",
      "...completed  201  epochs of training. Current loss:  0.6945294380781374 .\n",
      "epoch  201  total number of batches is  20\n",
      "...completed  202  epochs of training. Current loss:  0.6936528214231481 .\n",
      "epoch  202  total number of batches is  20\n",
      "...completed  203  epochs of training. Current loss:  0.6927646492156859 .\n",
      "epoch  203  total number of batches is  20\n",
      "...completed  204  epochs of training. Current loss:  0.6918648522615876 .\n",
      "epoch  204  total number of batches is  20\n",
      "...completed  205  epochs of training. Current loss:  0.6909533646394336 .\n",
      "epoch  205  total number of batches is  20\n",
      "...completed  206  epochs of training. Current loss:  0.6900301238394738 .\n",
      "epoch  206  total number of batches is  20\n",
      "...completed  207  epochs of training. Current loss:  0.6890950709015043 .\n",
      "epoch  207  total number of batches is  20\n",
      "...completed  208  epochs of training. Current loss:  0.6881481505513324 .\n",
      "epoch  208  total number of batches is  20\n",
      "...completed  209  epochs of training. Current loss:  0.6871893113354464 .\n",
      "epoch  209  total number of batches is  20\n",
      "...completed  210  epochs of training. Current loss:  0.6862185057535153 .\n",
      "epoch  210  total number of batches is  20\n",
      "...completed  211  epochs of training. Current loss:  0.6852356903883317 .\n",
      "epoch  211  total number of batches is  20\n",
      "...completed  212  epochs of training. Current loss:  0.6842408260328161 .\n",
      "epoch  212  total number of batches is  20\n",
      "...completed  213  epochs of training. Current loss:  0.6832338778136998 .\n",
      "epoch  213  total number of batches is  20\n",
      "...completed  214  epochs of training. Current loss:  0.6822148153115123 .\n",
      "epoch  214  total number of batches is  20\n",
      "...completed  215  epochs of training. Current loss:  0.6811836126764963 .\n",
      "epoch  215  total number of batches is  20\n",
      "...completed  216  epochs of training. Current loss:  0.6801402487400978 .\n",
      "epoch  216  total number of batches is  20\n",
      "...completed  217  epochs of training. Current loss:  0.6790847071216715 .\n",
      "epoch  217  total number of batches is  20\n",
      "...completed  218  epochs of training. Current loss:  0.6780169763300687 .\n",
      "epoch  218  total number of batches is  20\n",
      "...completed  219  epochs of training. Current loss:  0.6769370498597863 .\n",
      "epoch  219  total number of batches is  20\n",
      "...completed  220  epochs of training. Current loss:  0.6758449262813706 .\n",
      "epoch  220  total number of batches is  20\n",
      "...completed  221  epochs of training. Current loss:  0.6747406093257896 .\n",
      "epoch  221  total number of batches is  20\n",
      "...completed  222  epochs of training. Current loss:  0.6736241079625128 .\n",
      "epoch  222  total number of batches is  20\n",
      "...completed  223  epochs of training. Current loss:  0.6724954364710511 .\n",
      "epoch  223  total number of batches is  20\n",
      "...completed  224  epochs of training. Current loss:  0.6713546145057423 .\n",
      "epoch  224  total number of batches is  20\n",
      "...completed  225  epochs of training. Current loss:  0.6702016671535848 .\n",
      "epoch  225  total number of batches is  20\n",
      "...completed  226  epochs of training. Current loss:  0.6690366249849492 .\n",
      "epoch  226  total number of batches is  20\n",
      "...completed  227  epochs of training. Current loss:  0.6678595240970281 .\n",
      "epoch  227  total number of batches is  20\n",
      "...completed  228  epochs of training. Current loss:  0.6666704061498966 .\n",
      "epoch  228  total number of batches is  20\n",
      "...completed  229  epochs of training. Current loss:  0.6654693183951017 .\n",
      "epoch  229  total number of batches is  20\n",
      "...completed  230  epochs of training. Current loss:  0.6642563136966998 .\n",
      "epoch  230  total number of batches is  20\n",
      "...completed  231  epochs of training. Current loss:  0.6630314505447088 .\n",
      "epoch  231  total number of batches is  20\n",
      "...completed  232  epochs of training. Current loss:  0.6617947930609479 .\n",
      "epoch  232  total number of batches is  20\n",
      "...completed  233  epochs of training. Current loss:  0.6605464109972693 .\n",
      "epoch  233  total number of batches is  20\n",
      "...completed  234  epochs of training. Current loss:  0.6592863797262043 .\n",
      "epoch  234  total number of batches is  20\n",
      "...completed  235  epochs of training. Current loss:  0.658014780224068 .\n",
      "epoch  235  total number of batches is  20\n",
      "...completed  236  epochs of training. Current loss:  0.6567316990465808 .\n",
      "epoch  236  total number of batches is  20\n",
      "...completed  237  epochs of training. Current loss:  0.6554372282970868 .\n",
      "epoch  237  total number of batches is  20\n",
      "...completed  238  epochs of training. Current loss:  0.6541314655874626 .\n",
      "epoch  238  total number of batches is  20\n",
      "...completed  239  epochs of training. Current loss:  0.6528145139918183 .\n",
      "epoch  239  total number of batches is  20\n",
      "...completed  240  epochs of training. Current loss:  0.6514864819931143 .\n",
      "epoch  240  total number of batches is  20\n",
      "...completed  241  epochs of training. Current loss:  0.6501474834228178 .\n",
      "epoch  241  total number of batches is  20\n",
      "...completed  242  epochs of training. Current loss:  0.6487976373937381 .\n",
      "epoch  242  total number of batches is  20\n",
      "...completed  243  epochs of training. Current loss:  0.6474370682261851 .\n",
      "epoch  243  total number of batches is  20\n",
      "...completed  244  epochs of training. Current loss:  0.6460659053676017 .\n",
      "epoch  244  total number of batches is  20\n",
      "...completed  245  epochs of training. Current loss:  0.644684283305828 .\n",
      "epoch  245  total number of batches is  20\n",
      "...completed  246  epochs of training. Current loss:  0.6432923414761562 .\n",
      "epoch  246  total number of batches is  20\n",
      "...completed  247  epochs of training. Current loss:  0.6418902241623446 .\n",
      "epoch  247  total number of batches is  20\n",
      "...completed  248  epochs of training. Current loss:  0.6404780803917585 .\n",
      "epoch  248  total number of batches is  20\n",
      "...completed  249  epochs of training. Current loss:  0.6390560638248107 .\n",
      "epoch  249  total number of batches is  20\n",
      "...completed  250  epochs of training. Current loss:  0.6376243326388799 .\n",
      "epoch  250  total number of batches is  20\n",
      "...completed  251  epochs of training. Current loss:  0.6361830494068861 .\n",
      "epoch  251  total number of batches is  20\n",
      "...completed  252  epochs of training. Current loss:  0.6347323809707062 .\n",
      "epoch  252  total number of batches is  20\n",
      "...completed  253  epochs of training. Current loss:  0.6332724983096238 .\n",
      "epoch  253  total number of batches is  20\n",
      "...completed  254  epochs of training. Current loss:  0.631803576404002 .\n",
      "epoch  254  total number of batches is  20\n",
      "...completed  255  epochs of training. Current loss:  0.6303257940943827 .\n",
      "epoch  255  total number of batches is  20\n",
      "...completed  256  epochs of training. Current loss:  0.6288393339362187 .\n",
      "epoch  256  total number of batches is  20\n",
      "...completed  257  epochs of training. Current loss:  0.627344382050452 .\n",
      "epoch  257  total number of batches is  20\n",
      "...completed  258  epochs of training. Current loss:  0.625841127970159 .\n",
      "epoch  258  total number of batches is  20\n",
      "...completed  259  epochs of training. Current loss:  0.6243297644834954 .\n",
      "epoch  259  total number of batches is  20\n",
      "...completed  260  epochs of training. Current loss:  0.622810487473177 .\n",
      "epoch  260  total number of batches is  20\n",
      "...completed  261  epochs of training. Current loss:  0.6212834957527472 .\n",
      "epoch  261  total number of batches is  20\n",
      "...completed  262  epochs of training. Current loss:  0.6197489908998873 .\n",
      "epoch  262  total number of batches is  20\n",
      "...completed  263  epochs of training. Current loss:  0.61820717708704 .\n",
      "epoch  263  total number of batches is  20\n",
      "...completed  264  epochs of training. Current loss:  0.6166582609096217 .\n",
      "epoch  264  total number of batches is  20\n",
      "...completed  265  epochs of training. Current loss:  0.6151024512121129 .\n",
      "epoch  265  total number of batches is  20\n",
      "...completed  266  epochs of training. Current loss:  0.6135399589123229 .\n",
      "epoch  266  total number of batches is  20\n",
      "...completed  267  epochs of training. Current loss:  0.6119709968241336 .\n",
      "epoch  267  total number of batches is  20\n",
      "...completed  268  epochs of training. Current loss:  0.6103957794790372 .\n",
      "epoch  268  total number of batches is  20\n",
      "...completed  269  epochs of training. Current loss:  0.6088145229467874 .\n",
      "epoch  269  total number of batches is  20\n",
      "...completed  270  epochs of training. Current loss:  0.6072274446554924 .\n",
      "epoch  270  total number of batches is  20\n",
      "...completed  271  epochs of training. Current loss:  0.6056347632114789 .\n",
      "epoch  271  total number of batches is  20\n",
      "...completed  272  epochs of training. Current loss:  0.6040366982192638 .\n",
      "epoch  272  total number of batches is  20\n",
      "...completed  273  epochs of training. Current loss:  0.6024334701019698 .\n",
      "epoch  273  total number of batches is  20\n",
      "...completed  274  epochs of training. Current loss:  0.6008252999225234 .\n",
      "epoch  274  total number of batches is  20\n",
      "...completed  275  epochs of training. Current loss:  0.5992124092059712 .\n",
      "epoch  275  total number of batches is  20\n",
      "...completed  276  epochs of training. Current loss:  0.5975950197632462 .\n",
      "epoch  276  total number of batches is  20\n",
      "...completed  277  epochs of training. Current loss:  0.5959733535167159 .\n",
      "epoch  277  total number of batches is  20\n",
      "...completed  278  epochs of training. Current loss:  0.5943476323278352 .\n",
      "epoch  278  total number of batches is  20\n",
      "...completed  279  epochs of training. Current loss:  0.5927180778272154 .\n",
      "epoch  279  total number of batches is  20\n",
      "...completed  280  epochs of training. Current loss:  0.5910849112474192 .\n",
      "epoch  280  total number of batches is  20\n",
      "...completed  281  epochs of training. Current loss:  0.5894483532587746 .\n",
      "epoch  281  total number of batches is  20\n",
      "...completed  282  epochs of training. Current loss:  0.5878086238084896 .\n",
      "epoch  282  total number of batches is  20\n",
      "...completed  283  epochs of training. Current loss:  0.5861659419633365 .\n",
      "epoch  283  total number of batches is  20\n",
      "...completed  284  epochs of training. Current loss:  0.5845205257561602 .\n",
      "epoch  284  total number of batches is  20\n",
      "...completed  285  epochs of training. Current loss:  0.5828725920364459 .\n",
      "epoch  285  total number of batches is  20\n",
      "...completed  286  epochs of training. Current loss:  0.5812223563251695 .\n",
      "epoch  286  total number of batches is  20\n",
      "...completed  287  epochs of training. Current loss:  0.5795700326741301 .\n",
      "epoch  287  total number of batches is  20\n",
      "...completed  288  epochs of training. Current loss:  0.5779158335299508 .\n",
      "epoch  288  total number of batches is  20\n",
      "...completed  289  epochs of training. Current loss:  0.576259969602911 .\n",
      "epoch  289  total number of batches is  20\n",
      "...completed  290  epochs of training. Current loss:  0.5746026497407589 .\n",
      "epoch  290  total number of batches is  20\n",
      "...completed  291  epochs of training. Current loss:  0.5729440808076256 .\n",
      "epoch  291  total number of batches is  20\n",
      "...completed  292  epochs of training. Current loss:  0.571284467568151 .\n",
      "epoch  292  total number of batches is  20\n",
      "...completed  293  epochs of training. Current loss:  0.569624012576907 .\n",
      "epoch  293  total number of batches is  20\n",
      "...completed  294  epochs of training. Current loss:  0.5679629160731866 .\n",
      "epoch  294  total number of batches is  20\n",
      "...completed  295  epochs of training. Current loss:  0.5663013758812042 .\n",
      "epoch  295  total number of batches is  20\n",
      "...completed  296  epochs of training. Current loss:  0.5646395873157422 .\n",
      "epoch  296  total number of batches is  20\n",
      "...completed  297  epochs of training. Current loss:  0.562977743093253 .\n",
      "epoch  297  total number of batches is  20\n",
      "...completed  298  epochs of training. Current loss:  0.5613160332484114 .\n",
      "epoch  298  total number of batches is  20\n",
      "...completed  299  epochs of training. Current loss:  0.5596546450560976 .\n",
      "epoch  299  total number of batches is  20\n",
      "...completed  300  epochs of training. Current loss:  0.5579937629587731 .\n",
      "epoch  300  total number of batches is  20\n",
      "...completed  301  epochs of training. Current loss:  0.5563335684991975 .\n",
      "epoch  301  total number of batches is  20\n",
      "...completed  302  epochs of training. Current loss:  0.5546742402584204 .\n",
      "epoch  302  total number of batches is  20\n",
      "...completed  303  epochs of training. Current loss:  0.553015953798972 .\n",
      "epoch  303  total number of batches is  20\n",
      "...completed  304  epochs of training. Current loss:  0.551358881613159 .\n",
      "epoch  304  total number of batches is  20\n",
      "...completed  305  epochs of training. Current loss:  0.549703193076369 .\n",
      "epoch  305  total number of batches is  20\n",
      "...completed  306  epochs of training. Current loss:  0.5480490544052681 .\n",
      "epoch  306  total number of batches is  20\n",
      "...completed  307  epochs of training. Current loss:  0.5463966286207775 .\n",
      "epoch  307  total number of batches is  20\n",
      "...completed  308  epochs of training. Current loss:  0.5447460755156979 .\n",
      "epoch  308  total number of batches is  20\n",
      "...completed  309  epochs of training. Current loss:  0.5430975516268495 .\n",
      "epoch  309  total number of batches is  20\n",
      "...completed  310  epochs of training. Current loss:  0.5414512102115906 .\n",
      "epoch  310  total number of batches is  20\n",
      "...completed  311  epochs of training. Current loss:  0.5398072012285647 .\n",
      "epoch  311  total number of batches is  20\n",
      "...completed  312  epochs of training. Current loss:  0.5381656713225365 .\n",
      "epoch  312  total number of batches is  20\n",
      "...completed  313  epochs of training. Current loss:  0.5365267638131571 .\n",
      "epoch  313  total number of batches is  20\n",
      "...completed  314  epochs of training. Current loss:  0.5348906186875132 .\n",
      "epoch  314  total number of batches is  20\n",
      "...completed  315  epochs of training. Current loss:  0.5332573725962999 .\n",
      "epoch  315  total number of batches is  20\n",
      "...completed  316  epochs of training. Current loss:  0.5316271588534655 .\n",
      "epoch  316  total number of batches is  20\n",
      "...completed  317  epochs of training. Current loss:  0.5300001074391728 .\n",
      "epoch  317  total number of batches is  20\n",
      "...completed  318  epochs of training. Current loss:  0.5283763450059216 .\n",
      "epoch  318  total number of batches is  20\n",
      "...completed  319  epochs of training. Current loss:  0.526755994887681 .\n",
      "epoch  319  total number of batches is  20\n",
      "...completed  320  epochs of training. Current loss:  0.5251391771118781 .\n",
      "epoch  320  total number of batches is  20\n",
      "...completed  321  epochs of training. Current loss:  0.523526008414098 .\n",
      "epoch  321  total number of batches is  20\n",
      "...completed  322  epochs of training. Current loss:  0.5219166022553435 .\n",
      "epoch  322  total number of batches is  20\n",
      "...completed  323  epochs of training. Current loss:  0.5203110688417176 .\n",
      "epoch  323  total number of batches is  20\n",
      "...completed  324  epochs of training. Current loss:  0.5187095151463835 .\n",
      "epoch  324  total number of batches is  20\n",
      "...completed  325  epochs of training. Current loss:  0.5171120449336698 .\n",
      "epoch  325  total number of batches is  20\n",
      "...completed  326  epochs of training. Current loss:  0.5155187587851873 .\n",
      "epoch  326  total number of batches is  20\n",
      "...completed  327  epochs of training. Current loss:  0.5139297541278289 .\n",
      "epoch  327  total number of batches is  20\n",
      "...completed  328  epochs of training. Current loss:  0.5123451252635296 .\n",
      "epoch  328  total number of batches is  20\n",
      "...completed  329  epochs of training. Current loss:  0.5107649634006676 .\n",
      "epoch  329  total number of batches is  20\n",
      "...completed  330  epochs of training. Current loss:  0.5091893566869894 .\n",
      "epoch  330  total number of batches is  20\n",
      "...completed  331  epochs of training. Current loss:  0.5076183902439506 .\n",
      "epoch  331  total number of batches is  20\n",
      "...completed  332  epochs of training. Current loss:  0.506052146202365 .\n",
      "epoch  332  total number of batches is  20\n",
      "...completed  333  epochs of training. Current loss:  0.504490703739261 .\n",
      "epoch  333  total number of batches is  20\n",
      "...completed  334  epochs of training. Current loss:  0.5029341391158466 .\n",
      "epoch  334  total number of batches is  20\n",
      "...completed  335  epochs of training. Current loss:  0.5013825257164934 .\n",
      "epoch  335  total number of batches is  20\n",
      "...completed  336  epochs of training. Current loss:  0.4998359340886474 .\n",
      "epoch  336  total number of batches is  20\n",
      "...completed  337  epochs of training. Current loss:  0.498294431983584 .\n",
      "epoch  337  total number of batches is  20\n",
      "...completed  338  epochs of training. Current loss:  0.49675808439792773 .\n",
      "epoch  338  total number of batches is  20\n",
      "...completed  339  epochs of training. Current loss:  0.4952269536158592 .\n",
      "epoch  339  total number of batches is  20\n",
      "...completed  340  epochs of training. Current loss:  0.4937010992519366 .\n",
      "epoch  340  total number of batches is  20\n",
      "...completed  341  epochs of training. Current loss:  0.49218057829446404 .\n",
      "epoch  341  total number of batches is  20\n",
      "...completed  342  epochs of training. Current loss:  0.49066544514934135 .\n",
      "epoch  342  total number of batches is  20\n",
      "...completed  343  epochs of training. Current loss:  0.4891557516843321 .\n",
      "epoch  343  total number of batches is  20\n",
      "...completed  344  epochs of training. Current loss:  0.4876515472736945 .\n",
      "epoch  344  total number of batches is  20\n",
      "...completed  345  epochs of training. Current loss:  0.4861528788431143 .\n",
      "epoch  345  total number of batches is  20\n",
      "...completed  346  epochs of training. Current loss:  0.4846597909148943 .\n",
      "epoch  346  total number of batches is  20\n",
      "...completed  347  epochs of training. Current loss:  0.48317232565334317 .\n",
      "epoch  347  total number of batches is  20\n",
      "...completed  348  epochs of training. Current loss:  0.4816905229103231 .\n",
      "epoch  348  total number of batches is  20\n",
      "...completed  349  epochs of training. Current loss:  0.4802144202709061 .\n",
      "epoch  349  total number of batches is  20\n",
      "...completed  350  epochs of training. Current loss:  0.47874405309910056 .\n",
      "epoch  350  total number of batches is  20\n",
      "...completed  351  epochs of training. Current loss:  0.47727945458360627 .\n",
      "epoch  351  total number of batches is  20\n",
      "...completed  352  epochs of training. Current loss:  0.47582065578355914 .\n",
      "epoch  352  total number of batches is  20\n",
      "...completed  353  epochs of training. Current loss:  0.47436768567423154 .\n",
      "epoch  353  total number of batches is  20\n",
      "...completed  354  epochs of training. Current loss:  0.4729205711926517 .\n",
      "epoch  354  total number of batches is  20\n",
      "...completed  355  epochs of training. Current loss:  0.4714793372831108 .\n",
      "epoch  355  total number of batches is  20\n",
      "...completed  356  epochs of training. Current loss:  0.47004400694252674 .\n",
      "epoch  356  total number of batches is  20\n",
      "...completed  357  epochs of training. Current loss:  0.4686146012656349 .\n",
      "epoch  357  total number of batches is  20\n",
      "...completed  358  epochs of training. Current loss:  0.4671911394899771 .\n",
      "epoch  358  total number of batches is  20\n",
      "...completed  359  epochs of training. Current loss:  0.46577363904066216 .\n",
      "epoch  359  total number of batches is  20\n",
      "...completed  360  epochs of training. Current loss:  0.46436211557487433 .\n",
      "epoch  360  total number of batches is  20\n",
      "...completed  361  epochs of training. Current loss:  0.4629565830261018 .\n",
      "epoch  361  total number of batches is  20\n",
      "...completed  362  epochs of training. Current loss:  0.46155705364806493 .\n",
      "epoch  362  total number of batches is  20\n",
      "...completed  363  epochs of training. Current loss:  0.46016353805832044 .\n",
      "epoch  363  total number of batches is  20\n",
      "...completed  364  epochs of training. Current loss:  0.4587760452815203 .\n",
      "epoch  364  total number of batches is  20\n",
      "...completed  365  epochs of training. Current loss:  0.45739458279230605 .\n",
      "epoch  365  total number of batches is  20\n",
      "...completed  366  epochs of training. Current loss:  0.4560191565578173 .\n",
      "epoch  366  total number of batches is  20\n",
      "...completed  367  epochs of training. Current loss:  0.4546497710797971 .\n",
      "epoch  367  total number of batches is  20\n",
      "...completed  368  epochs of training. Current loss:  0.4532864294362772 .\n",
      "epoch  368  total number of batches is  20\n",
      "...completed  369  epochs of training. Current loss:  0.4519291333228234 .\n",
      "epoch  369  total number of batches is  20\n",
      "...completed  370  epochs of training. Current loss:  0.45057788309332747 .\n",
      "epoch  370  total number of batches is  20\n",
      "...completed  371  epochs of training. Current loss:  0.4492326778003294 .\n",
      "epoch  371  total number of batches is  20\n",
      "...completed  372  epochs of training. Current loss:  0.44789351523485416 .\n",
      "epoch  372  total number of batches is  20\n",
      "...completed  373  epochs of training. Current loss:  0.44656039196575115 .\n",
      "epoch  373  total number of batches is  20\n",
      "...completed  374  epochs of training. Current loss:  0.44523330337852063 .\n",
      "epoch  374  total number of batches is  20\n",
      "...completed  375  epochs of training. Current loss:  0.44391224371361665 .\n",
      "epoch  375  total number of batches is  20\n",
      "...completed  376  epochs of training. Current loss:  0.4425972061042128 .\n",
      "epoch  376  total number of batches is  20\n",
      "...completed  377  epochs of training. Current loss:  0.44128818261342095 .\n",
      "epoch  377  total number of batches is  20\n",
      "...completed  378  epochs of training. Current loss:  0.43998516427095113 .\n",
      "epoch  378  total number of batches is  20\n",
      "...completed  379  epochs of training. Current loss:  0.43868814110920523 .\n",
      "epoch  379  total number of batches is  20\n",
      "...completed  380  epochs of training. Current loss:  0.4373971021987907 .\n",
      "epoch  380  total number of batches is  20\n",
      "...completed  381  epochs of training. Current loss:  0.43611203568345075 .\n",
      "epoch  381  total number of batches is  20\n",
      "...completed  382  epochs of training. Current loss:  0.43483292881440005 .\n",
      "epoch  382  total number of batches is  20\n",
      "...completed  383  epochs of training. Current loss:  0.43355976798405976 .\n",
      "epoch  383  total number of batches is  20\n",
      "...completed  384  epochs of training. Current loss:  0.4322925387591842 .\n",
      "epoch  384  total number of batches is  20\n",
      "...completed  385  epochs of training. Current loss:  0.4310312259133757 .\n",
      "epoch  385  total number of batches is  20\n",
      "...completed  386  epochs of training. Current loss:  0.42977581345897975 .\n",
      "epoch  386  total number of batches is  20\n",
      "...completed  387  epochs of training. Current loss:  0.42852628467835685 .\n",
      "epoch  387  total number of batches is  20\n",
      "...completed  388  epochs of training. Current loss:  0.42728262215452717 .\n",
      "epoch  388  total number of batches is  20\n",
      "...completed  389  epochs of training. Current loss:  0.4260448078011853 .\n",
      "epoch  389  total number of batches is  20\n",
      "...completed  390  epochs of training. Current loss:  0.42481282289207994 .\n",
      "epoch  390  total number of batches is  20\n",
      "...completed  391  epochs of training. Current loss:  0.4235866480897598 .\n",
      "epoch  391  total number of batches is  20\n",
      "...completed  392  epochs of training. Current loss:  0.42236626347368106 .\n",
      "epoch  392  total number of batches is  20\n",
      "...completed  393  epochs of training. Current loss:  0.4211516485676773 .\n",
      "epoch  393  total number of batches is  20\n",
      "...completed  394  epochs of training. Current loss:  0.41994278236679156 .\n",
      "epoch  394  total number of batches is  20\n",
      "...completed  395  epochs of training. Current loss:  0.4187396433634697 .\n",
      "epoch  395  total number of batches is  20\n",
      "...completed  396  epochs of training. Current loss:  0.41754220957311633 .\n",
      "epoch  396  total number of batches is  20\n",
      "...completed  397  epochs of training. Current loss:  0.4163504585590157 .\n",
      "epoch  397  total number of batches is  20\n",
      "...completed  398  epochs of training. Current loss:  0.4151643674566175 .\n",
      "epoch  398  total number of batches is  20\n",
      "...completed  399  epochs of training. Current loss:  0.4139839129971921 .\n",
      "epoch  399  total number of batches is  20\n",
      "...completed  400  epochs of training. Current loss:  0.41280907153085666 .\n",
      "epoch  400  total number of batches is  20\n",
      "...completed  401  epochs of training. Current loss:  0.411639819048976 .\n",
      "epoch  401  total number of batches is  20\n",
      "...completed  402  epochs of training. Current loss:  0.41047613120594245 .\n",
      "epoch  402  total number of batches is  20\n",
      "...completed  403  epochs of training. Current loss:  0.40931798334034 .\n",
      "epoch  403  total number of batches is  20\n",
      "...completed  404  epochs of training. Current loss:  0.40816535049549374 .\n",
      "epoch  404  total number of batches is  20\n",
      "...completed  405  epochs of training. Current loss:  0.40701820743941597 .\n",
      "epoch  405  total number of batches is  20\n",
      "...completed  406  epochs of training. Current loss:  0.4058765286841497 .\n",
      "epoch  406  total number of batches is  20\n",
      "...completed  407  epochs of training. Current loss:  0.4047402885045172 .\n",
      "epoch  407  total number of batches is  20\n",
      "...completed  408  epochs of training. Current loss:  0.4036094609562806 .\n",
      "epoch  408  total number of batches is  20\n",
      "...completed  409  epochs of training. Current loss:  0.4024840198937221 .\n",
      "epoch  409  total number of batches is  20\n",
      "...completed  410  epochs of training. Current loss:  0.40136393898664846 .\n",
      "epoch  410  total number of batches is  20\n",
      "...completed  411  epochs of training. Current loss:  0.4002491917368295 .\n",
      "epoch  411  total number of batches is  20\n",
      "...completed  412  epochs of training. Current loss:  0.39913975149387804 .\n",
      "epoch  412  total number of batches is  20\n",
      "...completed  413  epochs of training. Current loss:  0.39803559147057865 .\n",
      "epoch  413  total number of batches is  20\n",
      "...completed  414  epochs of training. Current loss:  0.39693668475767485 .\n",
      "epoch  414  total number of batches is  20\n",
      "...completed  415  epochs of training. Current loss:  0.3958430043381216 .\n",
      "epoch  415  total number of batches is  20\n",
      "...completed  416  epochs of training. Current loss:  0.39475452310081416 .\n",
      "epoch  416  total number of batches is  20\n",
      "...completed  417  epochs of training. Current loss:  0.3936712138538005 .\n",
      "epoch  417  total number of batches is  20\n",
      "...completed  418  epochs of training. Current loss:  0.3925930493369888 .\n",
      "epoch  418  total number of batches is  20\n",
      "...completed  419  epochs of training. Current loss:  0.39152000223435524 .\n",
      "epoch  419  total number of batches is  20\n",
      "...completed  420  epochs of training. Current loss:  0.39045204518566834 .\n",
      "epoch  420  total number of batches is  20\n",
      "...completed  421  epochs of training. Current loss:  0.389389150797733 .\n",
      "epoch  421  total number of batches is  20\n",
      "...completed  422  epochs of training. Current loss:  0.38833129165516855 .\n",
      "epoch  422  total number of batches is  20\n",
      "...completed  423  epochs of training. Current loss:  0.38727844033072867 .\n",
      "epoch  423  total number of batches is  20\n",
      "...completed  424  epochs of training. Current loss:  0.3862305693951745 .\n",
      "epoch  424  total number of batches is  20\n",
      "...completed  425  epochs of training. Current loss:  0.38518765142671035 .\n",
      "epoch  425  total number of batches is  20\n",
      "...completed  426  epochs of training. Current loss:  0.3841496590199935 .\n",
      "epoch  426  total number of batches is  20\n",
      "...completed  427  epochs of training. Current loss:  0.3831165647947254 .\n",
      "epoch  427  total number of batches is  20\n",
      "...completed  428  epochs of training. Current loss:  0.3820883414038393 .\n",
      "epoch  428  total number of batches is  20\n",
      "...completed  429  epochs of training. Current loss:  0.38106496154129127 .\n",
      "epoch  429  total number of batches is  20\n",
      "...completed  430  epochs of training. Current loss:  0.38004639794946476 .\n",
      "epoch  430  total number of batches is  20\n",
      "...completed  431  epochs of training. Current loss:  0.379032623426202 .\n",
      "epoch  431  total number of batches is  20\n",
      "...completed  432  epochs of training. Current loss:  0.3780236108314698 .\n",
      "epoch  432  total number of batches is  20\n",
      "...completed  433  epochs of training. Current loss:  0.3770193330936722 .\n",
      "epoch  433  total number of batches is  20\n",
      "...completed  434  epochs of training. Current loss:  0.3760197632156177 .\n",
      "epoch  434  total number of batches is  20\n",
      "...completed  435  epochs of training. Current loss:  0.3750248742801549 .\n",
      "epoch  435  total number of batches is  20\n",
      "...completed  436  epochs of training. Current loss:  0.3740346394554834 .\n",
      "epoch  436  total number of batches is  20\n",
      "...completed  437  epochs of training. Current loss:  0.37304903200015166 .\n",
      "epoch  437  total number of batches is  20\n",
      "...completed  438  epochs of training. Current loss:  0.37206802526775257 .\n",
      "epoch  438  total number of batches is  20\n",
      "...completed  439  epochs of training. Current loss:  0.3710915927113245 .\n",
      "epoch  439  total number of batches is  20\n",
      "...completed  440  epochs of training. Current loss:  0.370119707887471 .\n",
      "epoch  440  total number of batches is  20\n",
      "...completed  441  epochs of training. Current loss:  0.36915234446020445 .\n",
      "epoch  441  total number of batches is  20\n",
      "...completed  442  epochs of training. Current loss:  0.36818947620452813 .\n",
      "epoch  442  total number of batches is  20\n",
      "...completed  443  epochs of training. Current loss:  0.36723107700976254 .\n",
      "epoch  443  total number of batches is  20\n",
      "...completed  444  epochs of training. Current loss:  0.36627712088262715 .\n",
      "epoch  444  total number of batches is  20\n",
      "...completed  445  epochs of training. Current loss:  0.3653275819500871 .\n",
      "epoch  445  total number of batches is  20\n",
      "...completed  446  epochs of training. Current loss:  0.36438243446197166 .\n",
      "epoch  446  total number of batches is  20\n",
      "...completed  447  epochs of training. Current loss:  0.3634416527933777 .\n",
      "epoch  447  total number of batches is  20\n",
      "...completed  448  epochs of training. Current loss:  0.3625052114468618 .\n",
      "epoch  448  total number of batches is  20\n",
      "...completed  449  epochs of training. Current loss:  0.36157308505443364 .\n",
      "epoch  449  total number of batches is  20\n",
      "...completed  450  epochs of training. Current loss:  0.360645248379358 .\n",
      "epoch  450  total number of batches is  20\n",
      "...completed  451  epochs of training. Current loss:  0.3597216763177729 .\n",
      "epoch  451  total number of batches is  20\n",
      "...completed  452  epochs of training. Current loss:  0.3588023439001325 .\n",
      "epoch  452  total number of batches is  20\n",
      "...completed  453  epochs of training. Current loss:  0.3578872262924841 .\n",
      "epoch  453  total number of batches is  20\n",
      "...completed  454  epochs of training. Current loss:  0.3569762987975849 .\n",
      "epoch  454  total number of batches is  20\n",
      "...completed  455  epochs of training. Current loss:  0.3560695368558668 .\n",
      "epoch  455  total number of batches is  20\n",
      "...completed  456  epochs of training. Current loss:  0.3551669160462588 .\n",
      "epoch  456  total number of batches is  20\n",
      "...completed  457  epochs of training. Current loss:  0.3542684120868706 .\n",
      "epoch  457  total number of batches is  20\n",
      "...completed  458  epochs of training. Current loss:  0.3533740008355477 .\n",
      "epoch  458  total number of batches is  20\n",
      "...completed  459  epochs of training. Current loss:  0.352483658290303 .\n",
      "epoch  459  total number of batches is  20\n",
      "...completed  460  epochs of training. Current loss:  0.3515973605896326 .\n",
      "epoch  460  total number of batches is  20\n",
      "...completed  461  epochs of training. Current loss:  0.35071508401272206 .\n",
      "epoch  461  total number of batches is  20\n",
      "...completed  462  epochs of training. Current loss:  0.349836804979549 .\n",
      "epoch  462  total number of batches is  20\n",
      "...completed  463  epochs of training. Current loss:  0.34896250005088875 .\n",
      "epoch  463  total number of batches is  20\n",
      "...completed  464  epochs of training. Current loss:  0.34809214592822924 .\n",
      "epoch  464  total number of batches is  20\n",
      "...completed  465  epochs of training. Current loss:  0.34722571945359954 .\n",
      "epoch  465  total number of batches is  20\n",
      "...completed  466  epochs of training. Current loss:  0.34636319760931955 .\n",
      "epoch  466  total number of batches is  20\n",
      "...completed  467  epochs of training. Current loss:  0.34550455751767506 .\n",
      "epoch  467  total number of batches is  20\n",
      "...completed  468  epochs of training. Current loss:  0.34464977644052275 .\n",
      "epoch  468  total number of batches is  20\n",
      "...completed  469  epochs of training. Current loss:  0.34379883177883214 .\n",
      "epoch  469  total number of batches is  20\n",
      "...completed  470  epochs of training. Current loss:  0.3429517010721669 .\n",
      "epoch  470  total number of batches is  20\n",
      "...completed  471  epochs of training. Current loss:  0.3421083619981129 .\n",
      "epoch  471  total number of batches is  20\n",
      "...completed  472  epochs of training. Current loss:  0.34126879237165436 .\n",
      "epoch  472  total number of batches is  20\n",
      "...completed  473  epochs of training. Current loss:  0.34043297014450485 .\n",
      "epoch  473  total number of batches is  20\n",
      "...completed  474  epochs of training. Current loss:  0.33960087340439654 .\n",
      "epoch  474  total number of batches is  20\n",
      "...completed  475  epochs of training. Current loss:  0.338772480374331 .\n",
      "epoch  475  total number of batches is  20\n",
      "...completed  476  epochs of training. Current loss:  0.3379477694117952 .\n",
      "epoch  476  total number of batches is  20\n",
      "...completed  477  epochs of training. Current loss:  0.3371267190079481 .\n",
      "epoch  477  total number of batches is  20\n",
      "...completed  478  epochs of training. Current loss:  0.33630930778677953 .\n",
      "epoch  478  total number of batches is  20\n",
      "...completed  479  epochs of training. Current loss:  0.33549551450424364 .\n",
      "epoch  479  total number of batches is  20\n",
      "...completed  480  epochs of training. Current loss:  0.3346853180473739 .\n",
      "epoch  480  total number of batches is  20\n",
      "...completed  481  epochs of training. Current loss:  0.33387869743337767 .\n",
      "epoch  481  total number of batches is  20\n",
      "...completed  482  epochs of training. Current loss:  0.33307563180871724 .\n",
      "epoch  482  total number of batches is  20\n",
      "...completed  483  epochs of training. Current loss:  0.3322761004481768 .\n",
      "epoch  483  total number of batches is  20\n",
      "...completed  484  epochs of training. Current loss:  0.33148008275392093 .\n",
      "epoch  484  total number of batches is  20\n",
      "...completed  485  epochs of training. Current loss:  0.3306875582545432 .\n",
      "epoch  485  total number of batches is  20\n",
      "...completed  486  epochs of training. Current loss:  0.3298985066041105 .\n",
      "epoch  486  total number of batches is  20\n",
      "...completed  487  epochs of training. Current loss:  0.32911290758120365 .\n",
      "epoch  487  total number of batches is  20\n",
      "...completed  488  epochs of training. Current loss:  0.3283307410879555 .\n",
      "epoch  488  total number of batches is  20\n",
      "...completed  489  epochs of training. Current loss:  0.3275519871490896 .\n",
      "epoch  489  total number of batches is  20\n",
      "...completed  490  epochs of training. Current loss:  0.32677662591096074 .\n",
      "epoch  490  total number of batches is  20\n",
      "...completed  491  epochs of training. Current loss:  0.32600463764059817 .\n",
      "epoch  491  total number of batches is  20\n",
      "...completed  492  epochs of training. Current loss:  0.3252360027247543 .\n",
      "epoch  492  total number of batches is  20\n",
      "...completed  493  epochs of training. Current loss:  0.32447070166895814 .\n",
      "epoch  493  total number of batches is  20\n",
      "...completed  494  epochs of training. Current loss:  0.3237087150965773 .\n",
      "epoch  494  total number of batches is  20\n",
      "...completed  495  epochs of training. Current loss:  0.32295002374788717 .\n",
      "epoch  495  total number of batches is  20\n",
      "...completed  496  epochs of training. Current loss:  0.32219460847914977 .\n",
      "epoch  496  total number of batches is  20\n",
      "...completed  497  epochs of training. Current loss:  0.3214424502617031 .\n",
      "epoch  497  total number of batches is  20\n",
      "...completed  498  epochs of training. Current loss:  0.32069353018106017 .\n",
      "epoch  498  total number of batches is  20\n",
      "...completed  499  epochs of training. Current loss:  0.3199478294360216 .\n",
      "epoch  499  total number of batches is  20\n",
      "...completed  500  epochs of training. Current loss:  0.3192053293377984 .\n",
      "Training complete.\n",
      "Training starting...\n",
      "epoch  0  total number of batches is  20\n",
      "...completed  1  epochs of training. Current loss:  0.7488372342367385 .\n",
      "epoch  1  total number of batches is  20\n",
      "...completed  2  epochs of training. Current loss:  0.7487986021804435 .\n",
      "epoch  2  total number of batches is  20\n",
      "...completed  3  epochs of training. Current loss:  0.7487600230291352 .\n",
      "epoch  3  total number of batches is  20\n",
      "...completed  4  epochs of training. Current loss:  0.7487214748030021 .\n",
      "epoch  4  total number of batches is  20\n",
      "...completed  5  epochs of training. Current loss:  0.7486829517158895 .\n",
      "epoch  5  total number of batches is  20\n",
      "...completed  6  epochs of training. Current loss:  0.7486444479830108 .\n",
      "epoch  6  total number of batches is  20\n",
      "...completed  7  epochs of training. Current loss:  0.7486059578203699 .\n",
      "epoch  7  total number of batches is  20\n",
      "...completed  8  epochs of training. Current loss:  0.748567475444774 .\n",
      "epoch  8  total number of batches is  20\n",
      "...completed  9  epochs of training. Current loss:  0.748528995073844 .\n",
      "epoch  9  total number of batches is  20\n",
      "...completed  10  epochs of training. Current loss:  0.7484905109260244 .\n",
      "epoch  10  total number of batches is  20\n",
      "...completed  11  epochs of training. Current loss:  0.7484520172205932 .\n",
      "epoch  11  total number of batches is  20\n",
      "...completed  12  epochs of training. Current loss:  0.74841350817767 .\n",
      "epoch  12  total number of batches is  20\n",
      "...completed  13  epochs of training. Current loss:  0.7483749780182248 .\n",
      "epoch  13  total number of batches is  20\n",
      "...completed  14  epochs of training. Current loss:  0.7483364209640844 .\n",
      "epoch  14  total number of batches is  20\n",
      "...completed  15  epochs of training. Current loss:  0.7482978312379398 .\n",
      "epoch  15  total number of batches is  20\n",
      "...completed  16  epochs of training. Current loss:  0.7482592030633515 .\n",
      "epoch  16  total number of batches is  20\n",
      "...completed  17  epochs of training. Current loss:  0.7482205306647547 .\n",
      "epoch  17  total number of batches is  20\n",
      "...completed  18  epochs of training. Current loss:  0.7481818082674648 .\n",
      "epoch  18  total number of batches is  20\n",
      "...completed  19  epochs of training. Current loss:  0.7481430300976802 .\n",
      "epoch  19  total number of batches is  20\n",
      "...completed  20  epochs of training. Current loss:  0.7481041903824855 .\n",
      "epoch  20  total number of batches is  20\n",
      "...completed  21  epochs of training. Current loss:  0.7480652833498557 .\n",
      "epoch  21  total number of batches is  20\n",
      "...completed  22  epochs of training. Current loss:  0.7480263032286562 .\n",
      "epoch  22  total number of batches is  20\n",
      "...completed  23  epochs of training. Current loss:  0.7479872442486454 .\n",
      "epoch  23  total number of batches is  20\n",
      "...completed  24  epochs of training. Current loss:  0.7479481006404757 .\n",
      "epoch  24  total number of batches is  20\n",
      "...completed  25  epochs of training. Current loss:  0.7479088666356921 .\n",
      "epoch  25  total number of batches is  20\n",
      "...completed  26  epochs of training. Current loss:  0.747869536466734 .\n",
      "epoch  26  total number of batches is  20\n",
      "...completed  27  epochs of training. Current loss:  0.7478301043669324 .\n",
      "epoch  27  total number of batches is  20\n",
      "...completed  28  epochs of training. Current loss:  0.7477905645705093 .\n",
      "epoch  28  total number of batches is  20\n",
      "...completed  29  epochs of training. Current loss:  0.7477509113125755 .\n",
      "epoch  29  total number of batches is  20\n",
      "...completed  30  epochs of training. Current loss:  0.7477111388291273 .\n",
      "epoch  30  total number of batches is  20\n",
      "...completed  31  epochs of training. Current loss:  0.7476712413570449 .\n",
      "epoch  31  total number of batches is  20\n",
      "...completed  32  epochs of training. Current loss:  0.7476312131340868 .\n",
      "epoch  32  total number of batches is  20\n",
      "...completed  33  epochs of training. Current loss:  0.7475910483988866 .\n",
      "epoch  33  total number of batches is  20\n",
      "...completed  34  epochs of training. Current loss:  0.7475507413909482 .\n",
      "epoch  34  total number of batches is  20\n",
      "...completed  35  epochs of training. Current loss:  0.7475102863506412 .\n",
      "epoch  35  total number of batches is  20\n",
      "...completed  36  epochs of training. Current loss:  0.7474696775191936 .\n",
      "epoch  36  total number of batches is  20\n",
      "...completed  37  epochs of training. Current loss:  0.7474289091386874 .\n",
      "epoch  37  total number of batches is  20\n",
      "...completed  38  epochs of training. Current loss:  0.7473879754520509 .\n",
      "epoch  38  total number of batches is  20\n",
      "...completed  39  epochs of training. Current loss:  0.7473468707030518 .\n",
      "epoch  39  total number of batches is  20\n",
      "...completed  40  epochs of training. Current loss:  0.7473055891362899 .\n",
      "epoch  40  total number of batches is  20\n",
      "...completed  41  epochs of training. Current loss:  0.7472641249971897 .\n",
      "epoch  41  total number of batches is  20\n",
      "...completed  42  epochs of training. Current loss:  0.7472224725319909 .\n",
      "epoch  42  total number of batches is  20\n",
      "...completed  43  epochs of training. Current loss:  0.7471806259877405 .\n",
      "epoch  43  total number of batches is  20\n",
      "...completed  44  epochs of training. Current loss:  0.7471385796122838 .\n",
      "epoch  44  total number of batches is  20\n",
      "...completed  45  epochs of training. Current loss:  0.7470963276542538 .\n",
      "epoch  45  total number of batches is  20\n",
      "...completed  46  epochs of training. Current loss:  0.7470538643630628 .\n",
      "epoch  46  total number of batches is  20\n",
      "...completed  47  epochs of training. Current loss:  0.7470111839888909 .\n",
      "epoch  47  total number of batches is  20\n",
      "...completed  48  epochs of training. Current loss:  0.7469682807826764 .\n",
      "epoch  48  total number of batches is  20\n",
      "...completed  49  epochs of training. Current loss:  0.7469251489961045 .\n",
      "epoch  49  total number of batches is  20\n",
      "...completed  50  epochs of training. Current loss:  0.7468817828815963 .\n",
      "epoch  50  total number of batches is  20\n",
      "...completed  51  epochs of training. Current loss:  0.7468381766922965 .\n",
      "epoch  51  total number of batches is  20\n",
      "...completed  52  epochs of training. Current loss:  0.7467943246820625 .\n",
      "epoch  52  total number of batches is  20\n",
      "...completed  53  epochs of training. Current loss:  0.7467502211054523 .\n",
      "epoch  53  total number of batches is  20\n",
      "...completed  54  epochs of training. Current loss:  0.7467058602177115 .\n",
      "epoch  54  total number of batches is  20\n",
      "...completed  55  epochs of training. Current loss:  0.7466612362747612 .\n",
      "epoch  55  total number of batches is  20\n",
      "...completed  56  epochs of training. Current loss:  0.7466163435331854 .\n",
      "epoch  56  total number of batches is  20\n",
      "...completed  57  epochs of training. Current loss:  0.746571176250217 .\n",
      "epoch  57  total number of batches is  20\n",
      "...completed  58  epochs of training. Current loss:  0.7465257286837256 .\n",
      "epoch  58  total number of batches is  20\n",
      "...completed  59  epochs of training. Current loss:  0.7464799950922034 .\n",
      "epoch  59  total number of batches is  20\n",
      "...completed  60  epochs of training. Current loss:  0.7464339697347525 .\n",
      "epoch  60  total number of batches is  20\n",
      "...completed  61  epochs of training. Current loss:  0.746387646871069 .\n",
      "epoch  61  total number of batches is  20\n",
      "...completed  62  epochs of training. Current loss:  0.7463410207614317 .\n",
      "epoch  62  total number of batches is  20\n",
      "...completed  63  epochs of training. Current loss:  0.7462940856666864 .\n",
      "epoch  63  total number of batches is  20\n",
      "...completed  64  epochs of training. Current loss:  0.7462468358482319 .\n",
      "epoch  64  total number of batches is  20\n",
      "...completed  65  epochs of training. Current loss:  0.7461992655680064 .\n",
      "epoch  65  total number of batches is  20\n",
      "...completed  66  epochs of training. Current loss:  0.7461513690884722 .\n",
      "epoch  66  total number of batches is  20\n",
      "...completed  67  epochs of training. Current loss:  0.7461031406726018 .\n",
      "epoch  67  total number of batches is  20\n",
      "...completed  68  epochs of training. Current loss:  0.746054574583863 .\n",
      "epoch  68  total number of batches is  20\n",
      "...completed  69  epochs of training. Current loss:  0.7460056650862057 .\n",
      "epoch  69  total number of batches is  20\n",
      "...completed  70  epochs of training. Current loss:  0.7459564064440453 .\n",
      "epoch  70  total number of batches is  20\n",
      "...completed  71  epochs of training. Current loss:  0.7459067929222493 .\n",
      "epoch  71  total number of batches is  20\n",
      "...completed  72  epochs of training. Current loss:  0.7458568187861229 .\n",
      "epoch  72  total number of batches is  20\n",
      "...completed  73  epochs of training. Current loss:  0.745806478301394 .\n",
      "epoch  73  total number of batches is  20\n",
      "...completed  74  epochs of training. Current loss:  0.7457557657341995 .\n",
      "epoch  74  total number of batches is  20\n",
      "...completed  75  epochs of training. Current loss:  0.7457046753510699 .\n",
      "epoch  75  total number of batches is  20\n",
      "...completed  76  epochs of training. Current loss:  0.7456532014189154 .\n",
      "epoch  76  total number of batches is  20\n",
      "...completed  77  epochs of training. Current loss:  0.7456013382050123 .\n",
      "epoch  77  total number of batches is  20\n",
      "...completed  78  epochs of training. Current loss:  0.7455490799769882 .\n",
      "epoch  78  total number of batches is  20\n",
      "...completed  79  epochs of training. Current loss:  0.7454964210028086 .\n",
      "epoch  79  total number of batches is  20\n",
      "...completed  80  epochs of training. Current loss:  0.7454433555507624 .\n",
      "epoch  80  total number of batches is  20\n",
      "...completed  81  epochs of training. Current loss:  0.7453898778894498 .\n",
      "epoch  81  total number of batches is  20\n",
      "...completed  82  epochs of training. Current loss:  0.7453359822877675 .\n",
      "epoch  82  total number of batches is  20\n",
      "...completed  83  epochs of training. Current loss:  0.7452816630148966 .\n",
      "epoch  83  total number of batches is  20\n",
      "...completed  84  epochs of training. Current loss:  0.7452269143402886 .\n",
      "epoch  84  total number of batches is  20\n",
      "...completed  85  epochs of training. Current loss:  0.7451717305336547 .\n",
      "epoch  85  total number of batches is  20\n",
      "...completed  86  epochs of training. Current loss:  0.7451161058649517 .\n",
      "epoch  86  total number of batches is  20\n",
      "...completed  87  epochs of training. Current loss:  0.7450600346043704 .\n",
      "epoch  87  total number of batches is  20\n",
      "...completed  88  epochs of training. Current loss:  0.7450035110223253 .\n",
      "epoch  88  total number of batches is  20\n",
      "...completed  89  epochs of training. Current loss:  0.7449465293894424 .\n",
      "epoch  89  total number of batches is  20\n",
      "...completed  90  epochs of training. Current loss:  0.744889083976547 .\n",
      "epoch  90  total number of batches is  20\n",
      "...completed  91  epochs of training. Current loss:  0.744831169054656 .\n",
      "epoch  91  total number of batches is  20\n",
      "...completed  92  epochs of training. Current loss:  0.7447727788949662 .\n",
      "epoch  92  total number of batches is  20\n",
      "...completed  93  epochs of training. Current loss:  0.744713907768845 .\n",
      "epoch  93  total number of batches is  20\n",
      "...completed  94  epochs of training. Current loss:  0.7446545499478213 .\n",
      "epoch  94  total number of batches is  20\n",
      "...completed  95  epochs of training. Current loss:  0.7445946997035773 .\n",
      "epoch  95  total number of batches is  20\n",
      "...completed  96  epochs of training. Current loss:  0.74453435130794 .\n",
      "epoch  96  total number of batches is  20\n",
      "...completed  97  epochs of training. Current loss:  0.7444734990328745 .\n",
      "epoch  97  total number of batches is  20\n",
      "...completed  98  epochs of training. Current loss:  0.7444121371504766 .\n",
      "epoch  98  total number of batches is  20\n",
      "...completed  99  epochs of training. Current loss:  0.7443502599329667 .\n",
      "epoch  99  total number of batches is  20\n",
      "...completed  100  epochs of training. Current loss:  0.7442878616526841 .\n",
      "epoch  100  total number of batches is  20\n",
      "...completed  101  epochs of training. Current loss:  0.744224936582083 .\n",
      "epoch  101  total number of batches is  20\n",
      "...completed  102  epochs of training. Current loss:  0.7441614789937268 .\n",
      "epoch  102  total number of batches is  20\n",
      "...completed  103  epochs of training. Current loss:  0.7440974831602857 .\n",
      "epoch  103  total number of batches is  20\n",
      "...completed  104  epochs of training. Current loss:  0.7440329433545342 .\n",
      "epoch  104  total number of batches is  20\n",
      "...completed  105  epochs of training. Current loss:  0.7439678538493485 .\n",
      "epoch  105  total number of batches is  20\n",
      "...completed  106  epochs of training. Current loss:  0.7439022089177056 .\n",
      "epoch  106  total number of batches is  20\n",
      "...completed  107  epochs of training. Current loss:  0.7438360028326837 .\n",
      "epoch  107  total number of batches is  20\n",
      "...completed  108  epochs of training. Current loss:  0.7437692298674621 .\n",
      "epoch  108  total number of batches is  20\n",
      "...completed  109  epochs of training. Current loss:  0.7437018842953235 .\n",
      "epoch  109  total number of batches is  20\n",
      "...completed  110  epochs of training. Current loss:  0.7436339603896563 .\n",
      "epoch  110  total number of batches is  20\n",
      "...completed  111  epochs of training. Current loss:  0.7435654524239572 .\n",
      "epoch  111  total number of batches is  20\n",
      "...completed  112  epochs of training. Current loss:  0.743496354671838 .\n",
      "epoch  112  total number of batches is  20\n",
      "...completed  113  epochs of training. Current loss:  0.7434266614070288 .\n",
      "epoch  113  total number of batches is  20\n",
      "...completed  114  epochs of training. Current loss:  0.7433563669033864 .\n",
      "epoch  114  total number of batches is  20\n",
      "...completed  115  epochs of training. Current loss:  0.7432854654349011 .\n",
      "epoch  115  total number of batches is  20\n",
      "...completed  116  epochs of training. Current loss:  0.7432139512757068 .\n",
      "epoch  116  total number of batches is  20\n",
      "...completed  117  epochs of training. Current loss:  0.7431418187000894 .\n",
      "epoch  117  total number of batches is  20\n",
      "...completed  118  epochs of training. Current loss:  0.7430690619825003 .\n",
      "epoch  118  total number of batches is  20\n",
      "...completed  119  epochs of training. Current loss:  0.7429956753975681 .\n",
      "epoch  119  total number of batches is  20\n",
      "...completed  120  epochs of training. Current loss:  0.7429216532201119 .\n",
      "epoch  120  total number of batches is  20\n",
      "...completed  121  epochs of training. Current loss:  0.7428469897251578 .\n",
      "epoch  121  total number of batches is  20\n",
      "...completed  122  epochs of training. Current loss:  0.7427716791879551 .\n",
      "epoch  122  total number of batches is  20\n",
      "...completed  123  epochs of training. Current loss:  0.7426957158839947 .\n",
      "epoch  123  total number of batches is  20\n",
      "...completed  124  epochs of training. Current loss:  0.742619094089028 .\n",
      "epoch  124  total number of batches is  20\n",
      "...completed  125  epochs of training. Current loss:  0.7425418080790892 .\n",
      "epoch  125  total number of batches is  20\n",
      "...completed  126  epochs of training. Current loss:  0.7424638521305168 .\n",
      "epoch  126  total number of batches is  20\n",
      "...completed  127  epochs of training. Current loss:  0.7423852205199784 .\n",
      "epoch  127  total number of batches is  20\n",
      "...completed  128  epochs of training. Current loss:  0.7423059075244963 .\n",
      "epoch  128  total number of batches is  20\n",
      "...completed  129  epochs of training. Current loss:  0.7422259074214757 .\n",
      "epoch  129  total number of batches is  20\n",
      "...completed  130  epochs of training. Current loss:  0.7421452144887329 .\n",
      "epoch  130  total number of batches is  20\n",
      "...completed  131  epochs of training. Current loss:  0.7420638230045277 .\n",
      "epoch  131  total number of batches is  20\n",
      "...completed  132  epochs of training. Current loss:  0.7419817272475941 .\n",
      "epoch  132  total number of batches is  20\n",
      "...completed  133  epochs of training. Current loss:  0.7418989214971775 .\n",
      "epoch  133  total number of batches is  20\n",
      "...completed  134  epochs of training. Current loss:  0.7418154000330689 .\n",
      "epoch  134  total number of batches is  20\n",
      "...completed  135  epochs of training. Current loss:  0.7417311571356455 .\n",
      "epoch  135  total number of batches is  20\n",
      "...completed  136  epochs of training. Current loss:  0.7416461870859092 .\n",
      "epoch  136  total number of batches is  20\n",
      "...completed  137  epochs of training. Current loss:  0.7415604841655306 .\n",
      "epoch  137  total number of batches is  20\n",
      "...completed  138  epochs of training. Current loss:  0.7414740426568929 .\n",
      "epoch  138  total number of batches is  20\n",
      "...completed  139  epochs of training. Current loss:  0.7413868568431382 .\n",
      "epoch  139  total number of batches is  20\n",
      "...completed  140  epochs of training. Current loss:  0.741298921008218 .\n",
      "epoch  140  total number of batches is  20\n",
      "...completed  141  epochs of training. Current loss:  0.7412102294369425 .\n",
      "epoch  141  total number of batches is  20\n",
      "...completed  142  epochs of training. Current loss:  0.7411207764150344 .\n",
      "epoch  142  total number of batches is  20\n",
      "...completed  143  epochs of training. Current loss:  0.7410305562291859 .\n",
      "epoch  143  total number of batches is  20\n",
      "...completed  144  epochs of training. Current loss:  0.7409395631671154 .\n",
      "epoch  144  total number of batches is  20\n",
      "...completed  145  epochs of training. Current loss:  0.7408477915176285 .\n",
      "epoch  145  total number of batches is  20\n",
      "...completed  146  epochs of training. Current loss:  0.740755235570681 .\n",
      "epoch  146  total number of batches is  20\n",
      "...completed  147  epochs of training. Current loss:  0.7406618896174448 .\n",
      "epoch  147  total number of batches is  20\n",
      "...completed  148  epochs of training. Current loss:  0.7405677479503756 .\n",
      "epoch  148  total number of batches is  20\n",
      "...completed  149  epochs of training. Current loss:  0.7404728048632838 .\n",
      "epoch  149  total number of batches is  20\n",
      "...completed  150  epochs of training. Current loss:  0.7403770546514085 .\n",
      "epoch  150  total number of batches is  20\n",
      "...completed  151  epochs of training. Current loss:  0.7402804916114929 .\n",
      "epoch  151  total number of batches is  20\n",
      "...completed  152  epochs of training. Current loss:  0.7401831100418641 .\n",
      "epoch  152  total number of batches is  20\n",
      "...completed  153  epochs of training. Current loss:  0.7400849042425156 .\n",
      "epoch  153  total number of batches is  20\n",
      "...completed  154  epochs of training. Current loss:  0.73998586851519 .\n",
      "epoch  154  total number of batches is  20\n",
      "...completed  155  epochs of training. Current loss:  0.7398859971634693 .\n",
      "epoch  155  total number of batches is  20\n",
      "...completed  156  epochs of training. Current loss:  0.7397852844928647 .\n",
      "epoch  156  total number of batches is  20\n",
      "...completed  157  epochs of training. Current loss:  0.7396837248109109 .\n",
      "epoch  157  total number of batches is  20\n",
      "...completed  158  epochs of training. Current loss:  0.7395813124272629 .\n",
      "epoch  158  total number of batches is  20\n",
      "...completed  159  epochs of training. Current loss:  0.7394780416537969 .\n",
      "epoch  159  total number of batches is  20\n",
      "...completed  160  epochs of training. Current loss:  0.7393739068047139 .\n",
      "epoch  160  total number of batches is  20\n",
      "...completed  161  epochs of training. Current loss:  0.7392689021966466 .\n",
      "epoch  161  total number of batches is  20\n",
      "...completed  162  epochs of training. Current loss:  0.73916302214877 .\n",
      "epoch  162  total number of batches is  20\n",
      "...completed  163  epochs of training. Current loss:  0.7390562609829157 .\n",
      "epoch  163  total number of batches is  20\n",
      "...completed  164  epochs of training. Current loss:  0.7389486130236883 .\n",
      "epoch  164  total number of batches is  20\n",
      "...completed  165  epochs of training. Current loss:  0.7388400725985879 .\n",
      "epoch  165  total number of batches is  20\n",
      "...completed  166  epochs of training. Current loss:  0.7387306340381339 .\n",
      "epoch  166  total number of batches is  20\n",
      "...completed  167  epochs of training. Current loss:  0.7386202916759937 .\n",
      "epoch  167  total number of batches is  20\n",
      "...completed  168  epochs of training. Current loss:  0.7385090398491152 .\n",
      "epoch  168  total number of batches is  20\n",
      "...completed  169  epochs of training. Current loss:  0.7383968728978627 .\n",
      "epoch  169  total number of batches is  20\n",
      "...completed  170  epochs of training. Current loss:  0.7382837851661567 .\n",
      "epoch  170  total number of batches is  20\n",
      "...completed  171  epochs of training. Current loss:  0.738169771001619 .\n",
      "epoch  171  total number of batches is  20\n",
      "...completed  172  epochs of training. Current loss:  0.7380548247557195 .\n",
      "epoch  172  total number of batches is  20\n",
      "...completed  173  epochs of training. Current loss:  0.7379389407839292 .\n",
      "epoch  173  total number of batches is  20\n",
      "...completed  174  epochs of training. Current loss:  0.7378221134458762 .\n",
      "epoch  174  total number of batches is  20\n",
      "...completed  175  epochs of training. Current loss:  0.7377043371055074 .\n",
      "epoch  175  total number of batches is  20\n",
      "...completed  176  epochs of training. Current loss:  0.737585606131252 .\n",
      "epoch  176  total number of batches is  20\n",
      "...completed  177  epochs of training. Current loss:  0.7374659148961921 .\n",
      "epoch  177  total number of batches is  20\n",
      "...completed  178  epochs of training. Current loss:  0.7373452577782362 .\n",
      "epoch  178  total number of batches is  20\n",
      "...completed  179  epochs of training. Current loss:  0.7372236291602976 .\n",
      "epoch  179  total number of batches is  20\n",
      "...completed  180  epochs of training. Current loss:  0.7371010234304771 .\n",
      "epoch  180  total number of batches is  20\n",
      "...completed  181  epochs of training. Current loss:  0.7369774349822519 .\n",
      "epoch  181  total number of batches is  20\n",
      "...completed  182  epochs of training. Current loss:  0.7368528582146665 .\n",
      "epoch  182  total number of batches is  20\n",
      "...completed  183  epochs of training. Current loss:  0.736727287532531 .\n",
      "epoch  183  total number of batches is  20\n",
      "...completed  184  epochs of training. Current loss:  0.7366007173466232 .\n",
      "epoch  184  total number of batches is  20\n",
      "...completed  185  epochs of training. Current loss:  0.7364731420738952 .\n",
      "epoch  185  total number of batches is  20\n",
      "...completed  186  epochs of training. Current loss:  0.7363445561376861 .\n",
      "epoch  186  total number of batches is  20\n",
      "...completed  187  epochs of training. Current loss:  0.7362149539679396 .\n",
      "epoch  187  total number of batches is  20\n",
      "...completed  188  epochs of training. Current loss:  0.7360843300014243 .\n",
      "epoch  188  total number of batches is  20\n",
      "...completed  189  epochs of training. Current loss:  0.7359526786819648 .\n",
      "epoch  189  total number of batches is  20\n",
      "...completed  190  epochs of training. Current loss:  0.7358199944606717 .\n",
      "epoch  190  total number of batches is  20\n",
      "...completed  191  epochs of training. Current loss:  0.7356862717961822 .\n",
      "epoch  191  total number of batches is  20\n",
      "...completed  192  epochs of training. Current loss:  0.7355515051549015 .\n",
      "epoch  192  total number of batches is  20\n",
      "...completed  193  epochs of training. Current loss:  0.7354156890112551 .\n",
      "epoch  193  total number of batches is  20\n",
      "...completed  194  epochs of training. Current loss:  0.7352788178479409 .\n",
      "epoch  194  total number of batches is  20\n",
      "...completed  195  epochs of training. Current loss:  0.7351408861561921 .\n",
      "epoch  195  total number of batches is  20\n",
      "...completed  196  epochs of training. Current loss:  0.7350018884360422 .\n",
      "epoch  196  total number of batches is  20\n",
      "...completed  197  epochs of training. Current loss:  0.7348618191965982 .\n",
      "epoch  197  total number of batches is  20\n",
      "...completed  198  epochs of training. Current loss:  0.7347206729563174 .\n",
      "epoch  198  total number of batches is  20\n",
      "...completed  199  epochs of training. Current loss:  0.7345784442432923 .\n",
      "epoch  199  total number of batches is  20\n",
      "...completed  200  epochs of training. Current loss:  0.7344351275955413 .\n",
      "epoch  200  total number of batches is  20\n",
      "...completed  201  epochs of training. Current loss:  0.7342907175613029 .\n",
      "epoch  201  total number of batches is  20\n",
      "...completed  202  epochs of training. Current loss:  0.73414520869934 .\n",
      "epoch  202  total number of batches is  20\n",
      "...completed  203  epochs of training. Current loss:  0.7339985955792471 .\n",
      "epoch  203  total number of batches is  20\n",
      "...completed  204  epochs of training. Current loss:  0.7338508727817659 .\n",
      "epoch  204  total number of batches is  20\n",
      "...completed  205  epochs of training. Current loss:  0.7337020348991055 .\n",
      "epoch  205  total number of batches is  20\n",
      "...completed  206  epochs of training. Current loss:  0.7335520765352715 .\n",
      "epoch  206  total number of batches is  20\n",
      "...completed  207  epochs of training. Current loss:  0.733400992306399 .\n",
      "epoch  207  total number of batches is  20\n",
      "...completed  208  epochs of training. Current loss:  0.7332487768410928 .\n",
      "epoch  208  total number of batches is  20\n",
      "...completed  209  epochs of training. Current loss:  0.733095424780776 .\n",
      "epoch  209  total number of batches is  20\n",
      "...completed  210  epochs of training. Current loss:  0.7329409307800435 .\n",
      "epoch  210  total number of batches is  20\n",
      "...completed  211  epochs of training. Current loss:  0.7327852895070224 .\n",
      "epoch  211  total number of batches is  20\n",
      "...completed  212  epochs of training. Current loss:  0.7326284956437392 .\n",
      "epoch  212  total number of batches is  20\n",
      "...completed  213  epochs of training. Current loss:  0.7324705438864951 .\n",
      "epoch  213  total number of batches is  20\n",
      "...completed  214  epochs of training. Current loss:  0.7323114289462466 .\n",
      "epoch  214  total number of batches is  20\n",
      "...completed  215  epochs of training. Current loss:  0.732151145548994 .\n",
      "epoch  215  total number of batches is  20\n",
      "...completed  216  epochs of training. Current loss:  0.7319896884361768 .\n",
      "epoch  216  total number of batches is  20\n",
      "...completed  217  epochs of training. Current loss:  0.7318270523650763 .\n",
      "epoch  217  total number of batches is  20\n",
      "...completed  218  epochs of training. Current loss:  0.7316632321092245 .\n",
      "epoch  218  total number of batches is  20\n",
      "...completed  219  epochs of training. Current loss:  0.7314982224588217 .\n",
      "epoch  219  total number of batches is  20\n",
      "...completed  220  epochs of training. Current loss:  0.7313320182211596 .\n",
      "epoch  220  total number of batches is  20\n",
      "...completed  221  epochs of training. Current loss:  0.7311646142210537 .\n",
      "epoch  221  total number of batches is  20\n",
      "...completed  222  epochs of training. Current loss:  0.7309960053012813 .\n",
      "epoch  222  total number of batches is  20\n",
      "...completed  223  epochs of training. Current loss:  0.7308261863230271 .\n",
      "epoch  223  total number of batches is  20\n",
      "...completed  224  epochs of training. Current loss:  0.7306551521663377 .\n",
      "epoch  224  total number of batches is  20\n",
      "...completed  225  epochs of training. Current loss:  0.7304828977305823 .\n",
      "epoch  225  total number of batches is  20\n",
      "...completed  226  epochs of training. Current loss:  0.7303094179349203 .\n",
      "epoch  226  total number of batches is  20\n",
      "...completed  227  epochs of training. Current loss:  0.730134707718779 .\n",
      "epoch  227  total number of batches is  20\n",
      "...completed  228  epochs of training. Current loss:  0.7299587620423351 .\n",
      "epoch  228  total number of batches is  20\n",
      "...completed  229  epochs of training. Current loss:  0.729781575887008 .\n",
      "epoch  229  total number of batches is  20\n",
      "...completed  230  epochs of training. Current loss:  0.729603144255957 .\n",
      "epoch  230  total number of batches is  20\n",
      "...completed  231  epochs of training. Current loss:  0.7294234621745891 .\n",
      "epoch  231  total number of batches is  20\n",
      "...completed  232  epochs of training. Current loss:  0.7292425246910728 .\n",
      "epoch  232  total number of batches is  20\n",
      "...completed  233  epochs of training. Current loss:  0.7290603268768592 .\n",
      "epoch  233  total number of batches is  20\n",
      "...completed  234  epochs of training. Current loss:  0.7288768638272137 .\n",
      "epoch  234  total number of batches is  20\n",
      "...completed  235  epochs of training. Current loss:  0.7286921306617514 .\n",
      "epoch  235  total number of batches is  20\n",
      "...completed  236  epochs of training. Current loss:  0.7285061225249838 .\n",
      "epoch  236  total number of batches is  20\n",
      "...completed  237  epochs of training. Current loss:  0.7283188345868711 .\n",
      "epoch  237  total number of batches is  20\n",
      "...completed  238  epochs of training. Current loss:  0.728130262043383 .\n",
      "epoch  238  total number of batches is  20\n",
      "...completed  239  epochs of training. Current loss:  0.7279404001170678 .\n",
      "epoch  239  total number of batches is  20\n",
      "...completed  240  epochs of training. Current loss:  0.7277492440576284 .\n",
      "epoch  240  total number of batches is  20\n",
      "...completed  241  epochs of training. Current loss:  0.7275567891425064 .\n",
      "epoch  241  total number of batches is  20\n",
      "...completed  242  epochs of training. Current loss:  0.7273630306774747 .\n",
      "epoch  242  total number of batches is  20\n",
      "...completed  243  epochs of training. Current loss:  0.727167963997236 .\n",
      "epoch  243  total number of batches is  20\n",
      "...completed  244  epochs of training. Current loss:  0.7269715844660314 .\n",
      "epoch  244  total number of batches is  20\n",
      "...completed  245  epochs of training. Current loss:  0.7267738874782552 .\n",
      "epoch  245  total number of batches is  20\n",
      "...completed  246  epochs of training. Current loss:  0.7265748684590785 .\n",
      "epoch  246  total number of batches is  20\n",
      "...completed  247  epochs of training. Current loss:  0.7263745228650776 .\n",
      "epoch  247  total number of batches is  20\n",
      "...completed  248  epochs of training. Current loss:  0.7261728461848754 .\n",
      "epoch  248  total number of batches is  20\n",
      "...completed  249  epochs of training. Current loss:  0.7259698339397845 .\n",
      "epoch  249  total number of batches is  20\n",
      "...completed  250  epochs of training. Current loss:  0.7257654816844623 .\n",
      "epoch  250  total number of batches is  20\n",
      "...completed  251  epochs of training. Current loss:  0.7255597850075716 .\n",
      "epoch  251  total number of batches is  20\n",
      "...completed  252  epochs of training. Current loss:  0.7253527395324482 .\n",
      "epoch  252  total number of batches is  20\n",
      "...completed  253  epochs of training. Current loss:  0.7251443409177781 .\n",
      "epoch  253  total number of batches is  20\n",
      "...completed  254  epochs of training. Current loss:  0.7249345848582797 .\n",
      "epoch  254  total number of batches is  20\n",
      "...completed  255  epochs of training. Current loss:  0.724723467085395 .\n",
      "epoch  255  total number of batches is  20\n",
      "...completed  256  epochs of training. Current loss:  0.7245109833679865 .\n",
      "epoch  256  total number of batches is  20\n",
      "...completed  257  epochs of training. Current loss:  0.7242971295130433 .\n",
      "epoch  257  total number of batches is  20\n",
      "...completed  258  epochs of training. Current loss:  0.7240819013663915 .\n",
      "epoch  258  total number of batches is  20\n",
      "...completed  259  epochs of training. Current loss:  0.7238652948134151 .\n",
      "epoch  259  total number of batches is  20\n",
      "...completed  260  epochs of training. Current loss:  0.723647305779779 .\n",
      "epoch  260  total number of batches is  20\n",
      "...completed  261  epochs of training. Current loss:  0.7234279302321652 .\n",
      "epoch  261  total number of batches is  20\n",
      "...completed  262  epochs of training. Current loss:  0.7232071641790084 .\n",
      "epoch  262  total number of batches is  20\n",
      "...completed  263  epochs of training. Current loss:  0.7229850036712454 .\n",
      "epoch  263  total number of batches is  20\n",
      "...completed  264  epochs of training. Current loss:  0.7227614448030639 .\n",
      "epoch  264  total number of batches is  20\n",
      "...completed  265  epochs of training. Current loss:  0.722536483712664 .\n",
      "epoch  265  total number of batches is  20\n",
      "...completed  266  epochs of training. Current loss:  0.7223101165830212 .\n",
      "epoch  266  total number of batches is  20\n",
      "...completed  267  epochs of training. Current loss:  0.7220823396426582 .\n",
      "epoch  267  total number of batches is  20\n",
      "...completed  268  epochs of training. Current loss:  0.7218531491664218 .\n",
      "epoch  268  total number of batches is  20\n",
      "...completed  269  epochs of training. Current loss:  0.7216225414762647 .\n",
      "epoch  269  total number of batches is  20\n",
      "...completed  270  epochs of training. Current loss:  0.7213905129420343 .\n",
      "epoch  270  total number of batches is  20\n",
      "...completed  271  epochs of training. Current loss:  0.721157059982266 .\n",
      "epoch  271  total number of batches is  20\n",
      "...completed  272  epochs of training. Current loss:  0.7209221790649823 .\n",
      "epoch  272  total number of batches is  20\n",
      "...completed  273  epochs of training. Current loss:  0.7206858667084974 .\n",
      "epoch  273  total number of batches is  20\n",
      "...completed  274  epochs of training. Current loss:  0.7204481194822249 .\n",
      "epoch  274  total number of batches is  20\n",
      "...completed  275  epochs of training. Current loss:  0.7202089340074925 .\n",
      "epoch  275  total number of batches is  20\n",
      "...completed  276  epochs of training. Current loss:  0.7199683069583594 .\n",
      "epoch  276  total number of batches is  20\n",
      "...completed  277  epochs of training. Current loss:  0.71972623506244 .\n",
      "epoch  277  total number of batches is  20\n",
      "...completed  278  epochs of training. Current loss:  0.7194827151017293 .\n",
      "epoch  278  total number of batches is  20\n",
      "...completed  279  epochs of training. Current loss:  0.7192377439134341 .\n",
      "epoch  279  total number of batches is  20\n",
      "...completed  280  epochs of training. Current loss:  0.7189913183908067 .\n",
      "epoch  280  total number of batches is  20\n",
      "...completed  281  epochs of training. Current loss:  0.7187434354839835 .\n",
      "epoch  281  total number of batches is  20\n",
      "...completed  282  epochs of training. Current loss:  0.7184940922008244 .\n",
      "epoch  282  total number of batches is  20\n",
      "...completed  283  epochs of training. Current loss:  0.718243285607758 .\n",
      "epoch  283  total number of batches is  20\n",
      "...completed  284  epochs of training. Current loss:  0.7179910128306266 .\n",
      "epoch  284  total number of batches is  20\n",
      "...completed  285  epochs of training. Current loss:  0.717737271055536 .\n",
      "epoch  285  total number of batches is  20\n",
      "...completed  286  epochs of training. Current loss:  0.7174820575297064 .\n",
      "epoch  286  total number of batches is  20\n",
      "...completed  287  epochs of training. Current loss:  0.7172253695623245 .\n",
      "epoch  287  total number of batches is  20\n",
      "...completed  288  epochs of training. Current loss:  0.7169672045253986 .\n",
      "epoch  288  total number of batches is  20\n",
      "...completed  289  epochs of training. Current loss:  0.7167075598546141 .\n",
      "epoch  289  total number of batches is  20\n",
      "...completed  290  epochs of training. Current loss:  0.7164464330501901 .\n",
      "epoch  290  total number of batches is  20\n",
      "...completed  291  epochs of training. Current loss:  0.716183821677736 .\n",
      "epoch  291  total number of batches is  20\n",
      "...completed  292  epochs of training. Current loss:  0.7159197233691104 .\n",
      "epoch  292  total number of batches is  20\n",
      "...completed  293  epochs of training. Current loss:  0.7156541358232776 .\n",
      "epoch  293  total number of batches is  20\n",
      "...completed  294  epochs of training. Current loss:  0.7153870568071663 .\n",
      "epoch  294  total number of batches is  20\n",
      "...completed  295  epochs of training. Current loss:  0.7151184841565243 .\n",
      "epoch  295  total number of batches is  20\n",
      "...completed  296  epochs of training. Current loss:  0.7148484157767756 .\n",
      "epoch  296  total number of batches is  20\n",
      "...completed  297  epochs of training. Current loss:  0.7145768496438748 .\n",
      "epoch  297  total number of batches is  20\n",
      "...completed  298  epochs of training. Current loss:  0.7143037838051582 .\n",
      "epoch  298  total number of batches is  20\n",
      "...completed  299  epochs of training. Current loss:  0.7140292163801965 .\n",
      "epoch  299  total number of batches is  20\n",
      "...completed  300  epochs of training. Current loss:  0.7137531455616408 .\n",
      "epoch  300  total number of batches is  20\n",
      "...completed  301  epochs of training. Current loss:  0.7134755696160692 .\n",
      "epoch  301  total number of batches is  20\n",
      "...completed  302  epochs of training. Current loss:  0.7131964868848276 .\n",
      "epoch  302  total number of batches is  20\n",
      "...completed  303  epochs of training. Current loss:  0.7129158957848689 .\n",
      "epoch  303  total number of batches is  20\n",
      "...completed  304  epochs of training. Current loss:  0.7126337948095869 .\n",
      "epoch  304  total number of batches is  20\n",
      "...completed  305  epochs of training. Current loss:  0.7123501825296465 .\n",
      "epoch  305  total number of batches is  20\n",
      "...completed  306  epochs of training. Current loss:  0.7120650575938082 .\n",
      "epoch  306  total number of batches is  20\n",
      "...completed  307  epochs of training. Current loss:  0.711778418729748 .\n",
      "epoch  307  total number of batches is  20\n",
      "...completed  308  epochs of training. Current loss:  0.7114902647448718 .\n",
      "epoch  308  total number of batches is  20\n",
      "...completed  309  epochs of training. Current loss:  0.7112005945271229 .\n",
      "epoch  309  total number of batches is  20\n",
      "...completed  310  epochs of training. Current loss:  0.7109094070457838 .\n",
      "epoch  310  total number of batches is  20\n",
      "...completed  311  epochs of training. Current loss:  0.71061670135227 .\n",
      "epoch  311  total number of batches is  20\n",
      "...completed  312  epochs of training. Current loss:  0.7103224765809175 .\n",
      "epoch  312  total number of batches is  20\n",
      "...completed  313  epochs of training. Current loss:  0.7100267319497616 .\n",
      "epoch  313  total number of batches is  20\n",
      "...completed  314  epochs of training. Current loss:  0.7097294667613079 .\n",
      "epoch  314  total number of batches is  20\n",
      "...completed  315  epochs of training. Current loss:  0.7094306804032938 .\n",
      "epoch  315  total number of batches is  20\n",
      "...completed  316  epochs of training. Current loss:  0.7091303723494418 .\n",
      "epoch  316  total number of batches is  20\n",
      "...completed  317  epochs of training. Current loss:  0.708828542160202 .\n",
      "epoch  317  total number of batches is  20\n",
      "...completed  318  epochs of training. Current loss:  0.7085251894834852 .\n",
      "epoch  318  total number of batches is  20\n",
      "...completed  319  epochs of training. Current loss:  0.708220314055385 .\n",
      "epoch  319  total number of batches is  20\n",
      "...completed  320  epochs of training. Current loss:  0.7079139157008888 .\n",
      "epoch  320  total number of batches is  20\n",
      "...completed  321  epochs of training. Current loss:  0.7076059943345776 .\n",
      "epoch  321  total number of batches is  20\n",
      "...completed  322  epochs of training. Current loss:  0.7072965499613137 .\n",
      "epoch  322  total number of batches is  20\n",
      "...completed  323  epochs of training. Current loss:  0.706985582676915 .\n",
      "epoch  323  total number of batches is  20\n",
      "...completed  324  epochs of training. Current loss:  0.7066730926688172 .\n",
      "epoch  324  total number of batches is  20\n",
      "...completed  325  epochs of training. Current loss:  0.7063590802167228 .\n",
      "epoch  325  total number of batches is  20\n",
      "...completed  326  epochs of training. Current loss:  0.7060435456932352 .\n",
      "epoch  326  total number of batches is  20\n",
      "...completed  327  epochs of training. Current loss:  0.7057264895644796 .\n",
      "epoch  327  total number of batches is  20\n",
      "...completed  328  epochs of training. Current loss:  0.7054079123907069 .\n",
      "epoch  328  total number of batches is  20\n",
      "...completed  329  epochs of training. Current loss:  0.7050878148268857 .\n",
      "epoch  329  total number of batches is  20\n",
      "...completed  330  epochs of training. Current loss:  0.7047661976232742 .\n",
      "epoch  330  total number of batches is  20\n",
      "...completed  331  epochs of training. Current loss:  0.7044430616259812 .\n",
      "epoch  331  total number of batches is  20\n",
      "...completed  332  epochs of training. Current loss:  0.7041184077775042 .\n",
      "epoch  332  total number of batches is  20\n",
      "...completed  333  epochs of training. Current loss:  0.703792237117255 .\n",
      "epoch  333  total number of batches is  20\n",
      "...completed  334  epochs of training. Current loss:  0.7034645507820665 .\n",
      "epoch  334  total number of batches is  20\n",
      "...completed  335  epochs of training. Current loss:  0.7031353500066799 .\n",
      "epoch  335  total number of batches is  20\n",
      "...completed  336  epochs of training. Current loss:  0.7028046361242154 .\n",
      "epoch  336  total number of batches is  20\n",
      "...completed  337  epochs of training. Current loss:  0.7024724105666232 .\n",
      "epoch  337  total number of batches is  20\n",
      "...completed  338  epochs of training. Current loss:  0.7021386748651135 .\n",
      "epoch  338  total number of batches is  20\n",
      "...completed  339  epochs of training. Current loss:  0.7018034306505702 .\n",
      "epoch  339  total number of batches is  20\n",
      "...completed  340  epochs of training. Current loss:  0.7014666796539409 .\n",
      "epoch  340  total number of batches is  20\n",
      "...completed  341  epochs of training. Current loss:  0.7011284237066089 .\n",
      "epoch  341  total number of batches is  20\n",
      "...completed  342  epochs of training. Current loss:  0.7007886647407411 .\n",
      "epoch  342  total number of batches is  20\n",
      "...completed  343  epochs of training. Current loss:  0.7004474047896189 .\n",
      "epoch  343  total number of batches is  20\n",
      "...completed  344  epochs of training. Current loss:  0.7001046459879422 .\n",
      "epoch  344  total number of batches is  20\n",
      "...completed  345  epochs of training. Current loss:  0.6997603905721155 .\n",
      "epoch  345  total number of batches is  20\n",
      "...completed  346  epochs of training. Current loss:  0.699414640880509 .\n",
      "epoch  346  total number of batches is  20\n",
      "...completed  347  epochs of training. Current loss:  0.6990673993536975 .\n",
      "epoch  347  total number of batches is  20\n",
      "...completed  348  epochs of training. Current loss:  0.698718668534676 .\n",
      "epoch  348  total number of batches is  20\n",
      "...completed  349  epochs of training. Current loss:  0.6983684510690522 .\n",
      "epoch  349  total number of batches is  20\n",
      "...completed  350  epochs of training. Current loss:  0.6980167497052137 .\n",
      "epoch  350  total number of batches is  20\n",
      "...completed  351  epochs of training. Current loss:  0.6976635672944733 .\n",
      "epoch  351  total number of batches is  20\n",
      "...completed  352  epochs of training. Current loss:  0.697308906791187 .\n",
      "epoch  352  total number of batches is  20\n",
      "...completed  353  epochs of training. Current loss:  0.6969527712528494 .\n",
      "epoch  353  total number of batches is  20\n",
      "...completed  354  epochs of training. Current loss:  0.696595163840164 .\n",
      "epoch  354  total number of batches is  20\n",
      "...completed  355  epochs of training. Current loss:  0.6962360878170866 .\n",
      "epoch  355  total number of batches is  20\n",
      "...completed  356  epochs of training. Current loss:  0.6958755465508434 .\n",
      "epoch  356  total number of batches is  20\n",
      "...completed  357  epochs of training. Current loss:  0.6955135435119263 .\n",
      "epoch  357  total number of batches is  20\n",
      "...completed  358  epochs of training. Current loss:  0.6951500822740584 .\n",
      "epoch  358  total number of batches is  20\n",
      "...completed  359  epochs of training. Current loss:  0.6947851665141346 .\n",
      "epoch  359  total number of batches is  20\n",
      "...completed  360  epochs of training. Current loss:  0.6944188000121376 .\n",
      "epoch  360  total number of batches is  20\n",
      "...completed  361  epochs of training. Current loss:  0.6940509866510254 .\n",
      "epoch  361  total number of batches is  20\n",
      "...completed  362  epochs of training. Current loss:  0.6936817304165923 .\n",
      "epoch  362  total number of batches is  20\n",
      "...completed  363  epochs of training. Current loss:  0.6933110353973048 .\n",
      "epoch  363  total number of batches is  20\n",
      "...completed  364  epochs of training. Current loss:  0.6929389057841088 .\n",
      "epoch  364  total number of batches is  20\n",
      "...completed  365  epochs of training. Current loss:  0.6925653458702108 .\n",
      "epoch  365  total number of batches is  20\n",
      "...completed  366  epochs of training. Current loss:  0.6921903600508321 .\n",
      "epoch  366  total number of batches is  20\n",
      "...completed  367  epochs of training. Current loss:  0.691813952822936 .\n",
      "epoch  367  total number of batches is  20\n",
      "...completed  368  epochs of training. Current loss:  0.6914361287849272 .\n",
      "epoch  368  total number of batches is  20\n",
      "...completed  369  epochs of training. Current loss:  0.6910568926363249 .\n",
      "epoch  369  total number of batches is  20\n",
      "...completed  370  epochs of training. Current loss:  0.6906762491774088 .\n",
      "epoch  370  total number of batches is  20\n",
      "...completed  371  epochs of training. Current loss:  0.6902942033088375 .\n",
      "epoch  371  total number of batches is  20\n",
      "...completed  372  epochs of training. Current loss:  0.6899107600312409 .\n",
      "epoch  372  total number of batches is  20\n",
      "...completed  373  epochs of training. Current loss:  0.6895259244447834 .\n",
      "epoch  373  total number of batches is  20\n",
      "...completed  374  epochs of training. Current loss:  0.6891397017487042 .\n",
      "epoch  374  total number of batches is  20\n",
      "...completed  375  epochs of training. Current loss:  0.6887520972408262 .\n",
      "epoch  375  total number of batches is  20\n",
      "...completed  376  epochs of training. Current loss:  0.6883631163170417 .\n",
      "epoch  376  total number of batches is  20\n",
      "...completed  377  epochs of training. Current loss:  0.6879727644707712 .\n",
      "epoch  377  total number of batches is  20\n",
      "...completed  378  epochs of training. Current loss:  0.6875810472923932 .\n",
      "epoch  378  total number of batches is  20\n",
      "...completed  379  epochs of training. Current loss:  0.6871879704686514 .\n",
      "epoch  379  total number of batches is  20\n",
      "...completed  380  epochs of training. Current loss:  0.6867935397820342 .\n",
      "epoch  380  total number of batches is  20\n",
      "...completed  381  epochs of training. Current loss:  0.6863977611101277 .\n",
      "epoch  381  total number of batches is  20\n",
      "...completed  382  epochs of training. Current loss:  0.6860006404249461 .\n",
      "epoch  382  total number of batches is  20\n",
      "...completed  383  epochs of training. Current loss:  0.6856021837922328 .\n",
      "epoch  383  total number of batches is  20\n",
      "...completed  384  epochs of training. Current loss:  0.6852023973707407 .\n",
      "epoch  384  total number of batches is  20\n",
      "...completed  385  epochs of training. Current loss:  0.6848012874114844 .\n",
      "epoch  385  total number of batches is  20\n",
      "...completed  386  epochs of training. Current loss:  0.6843988602569713 .\n",
      "epoch  386  total number of batches is  20\n",
      "...completed  387  epochs of training. Current loss:  0.6839951223404053 .\n",
      "epoch  387  total number of batches is  20\n",
      "...completed  388  epochs of training. Current loss:  0.6835900801848703 .\n",
      "epoch  388  total number of batches is  20\n",
      "...completed  389  epochs of training. Current loss:  0.6831837404024876 .\n",
      "epoch  389  total number of batches is  20\n",
      "...completed  390  epochs of training. Current loss:  0.682776109693552 .\n",
      "epoch  390  total number of batches is  20\n",
      "...completed  391  epochs of training. Current loss:  0.6823671948456449 .\n",
      "epoch  391  total number of batches is  20\n",
      "...completed  392  epochs of training. Current loss:  0.6819570027327264 .\n",
      "epoch  392  total number of batches is  20\n",
      "...completed  393  epochs of training. Current loss:  0.6815455403142036 .\n",
      "epoch  393  total number of batches is  20\n",
      "...completed  394  epochs of training. Current loss:  0.6811328146339789 .\n",
      "epoch  394  total number of batches is  20\n",
      "...completed  395  epochs of training. Current loss:  0.6807188328194794 .\n",
      "epoch  395  total number of batches is  20\n",
      "...completed  396  epochs of training. Current loss:  0.6803036020806629 .\n",
      "epoch  396  total number of batches is  20\n",
      "...completed  397  epochs of training. Current loss:  0.6798871297090056 .\n",
      "epoch  397  total number of batches is  20\n",
      "...completed  398  epochs of training. Current loss:  0.6794694230764716 .\n",
      "epoch  398  total number of batches is  20\n",
      "...completed  399  epochs of training. Current loss:  0.6790504896344619 .\n",
      "epoch  399  total number of batches is  20\n",
      "...completed  400  epochs of training. Current loss:  0.6786303369127463 .\n",
      "epoch  400  total number of batches is  20\n",
      "...completed  401  epochs of training. Current loss:  0.6782089725183778 .\n",
      "epoch  401  total number of batches is  20\n",
      "...completed  402  epochs of training. Current loss:  0.6777864041345883 .\n",
      "epoch  402  total number of batches is  20\n",
      "...completed  403  epochs of training. Current loss:  0.6773626395196707 .\n",
      "epoch  403  total number of batches is  20\n",
      "...completed  404  epochs of training. Current loss:  0.6769376865058419 .\n",
      "epoch  404  total number of batches is  20\n",
      "...completed  405  epochs of training. Current loss:  0.6765115529980932 .\n",
      "epoch  405  total number of batches is  20\n",
      "...completed  406  epochs of training. Current loss:  0.6760842469730235 .\n",
      "epoch  406  total number of batches is  20\n",
      "...completed  407  epochs of training. Current loss:  0.6756557764776603 .\n",
      "epoch  407  total number of batches is  20\n",
      "...completed  408  epochs of training. Current loss:  0.6752261496282659 .\n",
      "epoch  408  total number of batches is  20\n",
      "...completed  409  epochs of training. Current loss:  0.6747953746091311 .\n",
      "epoch  409  total number of batches is  20\n",
      "...completed  410  epochs of training. Current loss:  0.6743634596713566 .\n",
      "epoch  410  total number of batches is  20\n",
      "...completed  411  epochs of training. Current loss:  0.673930413131623 .\n",
      "epoch  411  total number of batches is  20\n",
      "...completed  412  epochs of training. Current loss:  0.6734962433709486 .\n",
      "epoch  412  total number of batches is  20\n",
      "...completed  413  epochs of training. Current loss:  0.6730609588334375 .\n",
      "epoch  413  total number of batches is  20\n",
      "...completed  414  epochs of training. Current loss:  0.6726245680250188 .\n",
      "epoch  414  total number of batches is  20\n",
      "...completed  415  epochs of training. Current loss:  0.672187079512175 .\n",
      "epoch  415  total number of batches is  20\n",
      "...completed  416  epochs of training. Current loss:  0.6717485019206619 .\n",
      "epoch  416  total number of batches is  20\n",
      "...completed  417  epochs of training. Current loss:  0.6713088439342224 .\n",
      "epoch  417  total number of batches is  20\n",
      "...completed  418  epochs of training. Current loss:  0.6708681142932905 .\n",
      "epoch  418  total number of batches is  20\n",
      "...completed  419  epochs of training. Current loss:  0.6704263217936909 .\n",
      "epoch  419  total number of batches is  20\n",
      "...completed  420  epochs of training. Current loss:  0.6699834752853312 .\n",
      "epoch  420  total number of batches is  20\n",
      "...completed  421  epochs of training. Current loss:  0.6695395836708892 .\n",
      "epoch  421  total number of batches is  20\n",
      "...completed  422  epochs of training. Current loss:  0.6690946559044948 .\n",
      "epoch  422  total number of batches is  20\n",
      "...completed  423  epochs of training. Current loss:  0.6686487009904092 .\n",
      "epoch  423  total number of batches is  20\n",
      "...completed  424  epochs of training. Current loss:  0.6682017279816992 .\n",
      "epoch  424  total number of batches is  20\n",
      "...completed  425  epochs of training. Current loss:  0.6677537459789082 .\n",
      "epoch  425  total number of batches is  20\n",
      "...completed  426  epochs of training. Current loss:  0.6673047641287281 .\n",
      "epoch  426  total number of batches is  20\n",
      "...completed  427  epochs of training. Current loss:  0.6668547916226639 .\n",
      "epoch  427  total number of batches is  20\n",
      "...completed  428  epochs of training. Current loss:  0.6664038376957037 .\n",
      "epoch  428  total number of batches is  20\n",
      "...completed  429  epochs of training. Current loss:  0.6659519116249824 .\n",
      "epoch  429  total number of batches is  20\n",
      "...completed  430  epochs of training. Current loss:  0.6654990227284491 .\n",
      "epoch  430  total number of batches is  20\n",
      "...completed  431  epochs of training. Current loss:  0.665045180363534 .\n",
      "epoch  431  total number of batches is  20\n",
      "...completed  432  epochs of training. Current loss:  0.6645903939258166 .\n",
      "epoch  432  total number of batches is  20\n",
      "...completed  433  epochs of training. Current loss:  0.664134672847695 .\n",
      "epoch  433  total number of batches is  20\n",
      "...completed  434  epochs of training. Current loss:  0.6636780265970588 .\n",
      "epoch  434  total number of batches is  20\n",
      "...completed  435  epochs of training. Current loss:  0.6632204646759632 .\n",
      "epoch  435  total number of batches is  20\n",
      "...completed  436  epochs of training. Current loss:  0.6627619966193081 .\n",
      "epoch  436  total number of batches is  20\n",
      "...completed  437  epochs of training. Current loss:  0.66230263199352 .\n",
      "epoch  437  total number of batches is  20\n",
      "...completed  438  epochs of training. Current loss:  0.6618423803952382 .\n",
      "epoch  438  total number of batches is  20\n",
      "...completed  439  epochs of training. Current loss:  0.6613812514500067 .\n",
      "epoch  439  total number of batches is  20\n",
      "...completed  440  epochs of training. Current loss:  0.6609192548109707 .\n",
      "epoch  440  total number of batches is  20\n",
      "...completed  441  epochs of training. Current loss:  0.6604564001575784 .\n",
      "epoch  441  total number of batches is  20\n",
      "...completed  442  epochs of training. Current loss:  0.6599926971942912 .\n",
      "epoch  442  total number of batches is  20\n",
      "...completed  443  epochs of training. Current loss:  0.6595281556492971 .\n",
      "epoch  443  total number of batches is  20\n",
      "...completed  444  epochs of training. Current loss:  0.6590627852732351 .\n",
      "epoch  444  total number of batches is  20\n",
      "...completed  445  epochs of training. Current loss:  0.6585965958379232 .\n",
      "epoch  445  total number of batches is  20\n",
      "...completed  446  epochs of training. Current loss:  0.6581295971350973 .\n",
      "epoch  446  total number of batches is  20\n",
      "...completed  447  epochs of training. Current loss:  0.6576617989751574 .\n",
      "epoch  447  total number of batches is  20\n",
      "...completed  448  epochs of training. Current loss:  0.6571932111859214 .\n",
      "epoch  448  total number of batches is  20\n",
      "...completed  449  epochs of training. Current loss:  0.6567238436113889 .\n",
      "epoch  449  total number of batches is  20\n",
      "...completed  450  epochs of training. Current loss:  0.6562537061105149 .\n",
      "epoch  450  total number of batches is  20\n",
      "...completed  451  epochs of training. Current loss:  0.655782808555992 .\n",
      "epoch  451  total number of batches is  20\n",
      "...completed  452  epochs of training. Current loss:  0.6553111608330421 .\n",
      "epoch  452  total number of batches is  20\n",
      "...completed  453  epochs of training. Current loss:  0.6548387728382203 .\n",
      "epoch  453  total number of batches is  20\n",
      "...completed  454  epochs of training. Current loss:  0.6543656544782277 .\n",
      "epoch  454  total number of batches is  20\n",
      "...completed  455  epochs of training. Current loss:  0.6538918156687352 .\n",
      "epoch  455  total number of batches is  20\n",
      "...completed  456  epochs of training. Current loss:  0.6534172663332193 .\n",
      "epoch  456  total number of batches is  20\n",
      "...completed  457  epochs of training. Current loss:  0.652942016401807 .\n",
      "epoch  457  total number of batches is  20\n",
      "...completed  458  epochs of training. Current loss:  0.6524660758101356 .\n",
      "epoch  458  total number of batches is  20\n",
      "...completed  459  epochs of training. Current loss:  0.6519894544982203 .\n",
      "epoch  459  total number of batches is  20\n",
      "...completed  460  epochs of training. Current loss:  0.6515121624093372 .\n",
      "epoch  460  total number of batches is  20\n",
      "...completed  461  epochs of training. Current loss:  0.6510342094889153 .\n",
      "epoch  461  total number of batches is  20\n",
      "...completed  462  epochs of training. Current loss:  0.6505556056834435 .\n",
      "epoch  462  total number of batches is  20\n",
      "...completed  463  epochs of training. Current loss:  0.6500763609393876 .\n",
      "epoch  463  total number of batches is  20\n",
      "...completed  464  epochs of training. Current loss:  0.6495964852021218 .\n",
      "epoch  464  total number of batches is  20\n",
      "...completed  465  epochs of training. Current loss:  0.6491159884148713 .\n",
      "epoch  465  total number of batches is  20\n",
      "...completed  466  epochs of training. Current loss:  0.6486348805176685 .\n",
      "epoch  466  total number of batches is  20\n",
      "...completed  467  epochs of training. Current loss:  0.6481531714463219 .\n",
      "epoch  467  total number of batches is  20\n",
      "...completed  468  epochs of training. Current loss:  0.6476708711313987 .\n",
      "epoch  468  total number of batches is  20\n",
      "...completed  469  epochs of training. Current loss:  0.6471879894972173 .\n",
      "epoch  469  total number of batches is  20\n",
      "...completed  470  epochs of training. Current loss:  0.6467045364608587 .\n",
      "epoch  470  total number of batches is  20\n",
      "...completed  471  epochs of training. Current loss:  0.6462205219311847 .\n",
      "epoch  471  total number of batches is  20\n",
      "...completed  472  epochs of training. Current loss:  0.6457359558078736 .\n",
      "epoch  472  total number of batches is  20\n",
      "...completed  473  epochs of training. Current loss:  0.6452508479804673 .\n",
      "epoch  473  total number of batches is  20\n",
      "...completed  474  epochs of training. Current loss:  0.6447652083274324 .\n",
      "epoch  474  total number of batches is  20\n",
      "...completed  475  epochs of training. Current loss:  0.6442790467152343 .\n",
      "epoch  475  total number of batches is  20\n",
      "...completed  476  epochs of training. Current loss:  0.6437923729974241 .\n",
      "epoch  476  total number of batches is  20\n",
      "...completed  477  epochs of training. Current loss:  0.643305197013739 .\n",
      "epoch  477  total number of batches is  20\n",
      "...completed  478  epochs of training. Current loss:  0.6428175285892175 .\n",
      "epoch  478  total number of batches is  20\n",
      "...completed  479  epochs of training. Current loss:  0.6423293775333243 .\n",
      "epoch  479  total number of batches is  20\n",
      "...completed  480  epochs of training. Current loss:  0.6418407536390928 .\n",
      "epoch  480  total number of batches is  20\n",
      "...completed  481  epochs of training. Current loss:  0.6413516666822766 .\n",
      "epoch  481  total number of batches is  20\n",
      "...completed  482  epochs of training. Current loss:  0.6408621264205169 .\n",
      "epoch  482  total number of batches is  20\n",
      "...completed  483  epochs of training. Current loss:  0.6403721425925223 .\n",
      "epoch  483  total number of batches is  20\n",
      "...completed  484  epochs of training. Current loss:  0.6398817249172607 .\n",
      "epoch  484  total number of batches is  20\n",
      "...completed  485  epochs of training. Current loss:  0.6393908830931649 .\n",
      "epoch  485  total number of batches is  20\n",
      "...completed  486  epochs of training. Current loss:  0.6388996267973511 .\n",
      "epoch  486  total number of batches is  20\n",
      "...completed  487  epochs of training. Current loss:  0.6384079656848495 .\n",
      "epoch  487  total number of batches is  20\n",
      "...completed  488  epochs of training. Current loss:  0.637915909387849 .\n",
      "epoch  488  total number of batches is  20\n",
      "...completed  489  epochs of training. Current loss:  0.6374234675149522 .\n",
      "epoch  489  total number of batches is  20\n",
      "...completed  490  epochs of training. Current loss:  0.6369306496504452 .\n",
      "epoch  490  total number of batches is  20\n",
      "...completed  491  epochs of training. Current loss:  0.6364374653535786 .\n",
      "epoch  491  total number of batches is  20\n",
      "...completed  492  epochs of training. Current loss:  0.635943924157861 .\n",
      "epoch  492  total number of batches is  20\n",
      "...completed  493  epochs of training. Current loss:  0.6354500355703648 .\n",
      "epoch  493  total number of batches is  20\n",
      "...completed  494  epochs of training. Current loss:  0.6349558090710437 .\n",
      "epoch  494  total number of batches is  20\n",
      "...completed  495  epochs of training. Current loss:  0.6344612541120628 .\n",
      "epoch  495  total number of batches is  20\n",
      "...completed  496  epochs of training. Current loss:  0.6339663801171402 .\n",
      "epoch  496  total number of batches is  20\n",
      "...completed  497  epochs of training. Current loss:  0.6334711964809008 .\n",
      "epoch  497  total number of batches is  20\n",
      "...completed  498  epochs of training. Current loss:  0.6329757125682401 .\n",
      "epoch  498  total number of batches is  20\n",
      "...completed  499  epochs of training. Current loss:  0.6324799377137023 .\n",
      "epoch  499  total number of batches is  20\n",
      "...completed  500  epochs of training. Current loss:  0.6319838812208667 .\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    " # The part for perturbation method is commented out since this method is not very relevant for the research objective. It works, just need to uncomment\n",
    "\n",
    "# get number of total updates that will happen\n",
    "numupdates = numepochs * int(cnp.floor(x_train_corpus.shape[1]/batchsize))\n",
    "\n",
    "print(\"total iteration to be run per training: \", numupdates)\n",
    "\n",
    "# initialize the loss and accuracy holders\n",
    "# losses_perturb_txt  = cnp.zeros((numupdates,1)) # commented out since this method is not very relevant for the research objective\n",
    "losses_backprop_txt = cnp.zeros((numupdates,1))\n",
    "losses_feedback_txt = cnp.zeros((numupdates,1))\n",
    "accuracy_perturb_txt  = cnp.zeros((numepochs,1))\n",
    "accuracy_backprop_txt = cnp.zeros((numepochs,1))\n",
    "accuracy_feedback_txt = cnp.zeros((numepochs,1))\n",
    "    \n",
    "# set the random seed to the current time\n",
    "seed = int(round(datetime.now().timestamp())) + 1\n",
    "cnp.random.seed(seed)\n",
    "\n",
    "# create a network and train it using weight perturbation\n",
    "# netperturb = MLP(x_train_corpus.shape[0],x_train_labels.shape[0], numhidden,batchsize,learnrate,alpha=initweight,algorithm='perturb',sigma=10)\n",
    "# (losses_perturb_txt[:,0],accuracy_perturb_txt[:,0]) = \\\n",
    "# netperturb.train(x_train_corpus,x_train_labels,numepochs,y_test_corpus[:,:],y_test_labels[:,:],report=True,report_rate=1)\n",
    "\n",
    "# create a network and train it using backprop\n",
    "netbackprop = MLP(x_train_corpus.shape[0], x_train_labels.shape[0], numhidden,batchsize,learnrate,alpha=initweight,algorithm='backprop')\n",
    "(losses_backprop_txt[:,0],accuracy_backprop_txt[:,0]) = \\\n",
    "netbackprop.train(x_train_corpus,x_train_labels,numepochs,y_test_corpus[:,:],y_test_labels[:,:],report=True,report_rate=1)\n",
    "\n",
    "# # create a network and train it using feedback alignment\n",
    "netfeedback = MLP(x_train_corpus.shape[0], x_train_labels.shape[0], numhidden,batchsize,learnrate,alpha=initweight,algorithm='feedback')\n",
    "(losses_feedback_txt[:,0],accuracy_feedback_txt[:,0]) = \\\n",
    "netfeedback.train(x_train_corpus,x_train_labels,numepochs,y_test_corpus[:,:],y_test_labels[:,:],report=True,report_rate=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_results([],[],losses_backprop_txt,accuracy_backprop_txt,losses_feedback_txt,accuracy_feedback_txt,numupdates,numepochs, \"text\",\\\n",
    "            learnrate,numhidden)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below sections are for image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(10000, 784)\n",
      "(10000, 784)\n",
      "(784, 10000)\n",
      "(784, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Load the MNIST dataset\n",
    "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "\n",
    "print(train_set[0].shape)\n",
    "print(valid_set[0].shape)\n",
    "print(test_set[0].shape)\n",
    "\n",
    "# Get the data into easy to use arrays\n",
    "train_images = train_set[0][0:10000].transpose()\n",
    "train_labels = np.zeros((10,10000))\n",
    "for label in range(10000):\n",
    "    train_labels[train_set[1][label],label] = 1\n",
    "test_images = test_set[0][0:1000].transpose()\n",
    "test_labels = np.zeros((10,1000))\n",
    "for label in range(1000):\n",
    "    test_labels[test_set[1][label],label] = 1\n",
    "\n",
    "\n",
    "print(train_images.shape)\n",
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert np array to cp array if cuda is available\n",
    "train_images = cnp.asarray(train_images)\n",
    "train_labels = cnp.asanyarray(train_labels)\n",
    "\n",
    "test_images = cnp.asarray(test_images)\n",
    "test_labels = cnp.asarray(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total iteration to be run per training:  10000\n",
      "Training starting...\n",
      "epoch  0  total number of batches is  20\n",
      "...completed  1  epochs of training. Current loss:  0.8892664830403386 .\n",
      "epoch  1  total number of batches is  20\n",
      "...completed  2  epochs of training. Current loss:  0.8675206712816135 .\n",
      "epoch  2  total number of batches is  20\n",
      "...completed  3  epochs of training. Current loss:  0.8391573212243184 .\n",
      "epoch  3  total number of batches is  20\n",
      "...completed  4  epochs of training. Current loss:  0.8031694931875775 .\n",
      "epoch  4  total number of batches is  20\n",
      "...completed  5  epochs of training. Current loss:  0.7610574998806484 .\n",
      "epoch  5  total number of batches is  20\n",
      "...completed  6  epochs of training. Current loss:  0.7165039524292857 .\n",
      "epoch  6  total number of batches is  20\n",
      "...completed  7  epochs of training. Current loss:  0.6729010301642365 .\n",
      "epoch  7  total number of batches is  20\n",
      "...completed  8  epochs of training. Current loss:  0.6322117065399518 .\n",
      "epoch  8  total number of batches is  20\n",
      "...completed  9  epochs of training. Current loss:  0.5953919344114292 .\n",
      "epoch  9  total number of batches is  20\n",
      "...completed  10  epochs of training. Current loss:  0.5626648582603861 .\n",
      "epoch  10  total number of batches is  20\n",
      "...completed  11  epochs of training. Current loss:  0.5337429745803397 .\n",
      "epoch  11  total number of batches is  20\n",
      "...completed  12  epochs of training. Current loss:  0.5081299605629453 .\n",
      "epoch  12  total number of batches is  20\n",
      "...completed  13  epochs of training. Current loss:  0.48532503874935634 .\n",
      "epoch  13  total number of batches is  20\n",
      "...completed  14  epochs of training. Current loss:  0.4648992959139297 .\n",
      "epoch  14  total number of batches is  20\n",
      "...completed  15  epochs of training. Current loss:  0.44650385512976626 .\n",
      "epoch  15  total number of batches is  20\n",
      "...completed  16  epochs of training. Current loss:  0.42985620812844605 .\n",
      "epoch  16  total number of batches is  20\n",
      "...completed  17  epochs of training. Current loss:  0.4147245451182454 .\n",
      "epoch  17  total number of batches is  20\n",
      "...completed  18  epochs of training. Current loss:  0.40091564515557526 .\n",
      "epoch  18  total number of batches is  20\n",
      "...completed  19  epochs of training. Current loss:  0.38826652638561726 .\n",
      "epoch  19  total number of batches is  20\n",
      "...completed  20  epochs of training. Current loss:  0.3766386955414912 .\n",
      "epoch  20  total number of batches is  20\n",
      "...completed  21  epochs of training. Current loss:  0.36591391992559535 .\n",
      "epoch  21  total number of batches is  20\n",
      "...completed  22  epochs of training. Current loss:  0.3559908700198604 .\n",
      "epoch  22  total number of batches is  20\n",
      "...completed  23  epochs of training. Current loss:  0.3467823342585195 .\n",
      "epoch  23  total number of batches is  20\n",
      "...completed  24  epochs of training. Current loss:  0.3382128978296688 .\n",
      "epoch  24  total number of batches is  20\n",
      "...completed  25  epochs of training. Current loss:  0.3302170409188307 .\n",
      "epoch  25  total number of batches is  20\n",
      "...completed  26  epochs of training. Current loss:  0.32273761105236526 .\n",
      "epoch  26  total number of batches is  20\n",
      "...completed  27  epochs of training. Current loss:  0.3157246065924287 .\n",
      "epoch  27  total number of batches is  20\n",
      "...completed  28  epochs of training. Current loss:  0.3091341984074599 .\n",
      "epoch  28  total number of batches is  20\n",
      "...completed  29  epochs of training. Current loss:  0.3029279208606547 .\n",
      "epoch  29  total number of batches is  20\n",
      "...completed  30  epochs of training. Current loss:  0.2970719781398053 .\n",
      "epoch  30  total number of batches is  20\n",
      "...completed  31  epochs of training. Current loss:  0.2915366308769743 .\n",
      "epoch  31  total number of batches is  20\n",
      "...completed  32  epochs of training. Current loss:  0.28629564507705535 .\n",
      "epoch  32  total number of batches is  20\n",
      "...completed  33  epochs of training. Current loss:  0.28132579727393064 .\n",
      "epoch  33  total number of batches is  20\n",
      "...completed  34  epochs of training. Current loss:  0.2766064358727371 .\n",
      "epoch  34  total number of batches is  20\n",
      "...completed  35  epochs of training. Current loss:  0.27211909992197064 .\n",
      "epoch  35  total number of batches is  20\n",
      "...completed  36  epochs of training. Current loss:  0.26784719496052695 .\n",
      "epoch  36  total number of batches is  20\n",
      "...completed  37  epochs of training. Current loss:  0.26377572291843104 .\n",
      "epoch  37  total number of batches is  20\n",
      "...completed  38  epochs of training. Current loss:  0.2598910606179043 .\n",
      "epoch  38  total number of batches is  20\n",
      "...completed  39  epochs of training. Current loss:  0.25618077989118926 .\n",
      "epoch  39  total number of batches is  20\n",
      "...completed  40  epochs of training. Current loss:  0.2526335018435322 .\n",
      "epoch  40  total number of batches is  20\n",
      "...completed  41  epochs of training. Current loss:  0.24923877815940837 .\n",
      "epoch  41  total number of batches is  20\n",
      "...completed  42  epochs of training. Current loss:  0.24598699326361004 .\n",
      "epoch  42  total number of batches is  20\n",
      "...completed  43  epochs of training. Current loss:  0.24286928229946977 .\n",
      "epoch  43  total number of batches is  20\n",
      "...completed  44  epochs of training. Current loss:  0.23987746104292304 .\n",
      "epoch  44  total number of batches is  20\n",
      "...completed  45  epochs of training. Current loss:  0.23700396489355807 .\n",
      "epoch  45  total number of batches is  20\n",
      "...completed  46  epochs of training. Current loss:  0.234241794910671 .\n",
      "epoch  46  total number of batches is  20\n",
      "...completed  47  epochs of training. Current loss:  0.231584469486092 .\n",
      "epoch  47  total number of batches is  20\n",
      "...completed  48  epochs of training. Current loss:  0.2290259806892354 .\n",
      "epoch  48  total number of batches is  20\n",
      "...completed  49  epochs of training. Current loss:  0.22656075461926967 .\n",
      "epoch  49  total number of batches is  20\n",
      "...completed  50  epochs of training. Current loss:  0.22418361529178218 .\n",
      "epoch  50  total number of batches is  20\n",
      "...completed  51  epochs of training. Current loss:  0.22188975170553898 .\n",
      "epoch  51  total number of batches is  20\n",
      "...completed  52  epochs of training. Current loss:  0.21967468780480026 .\n",
      "epoch  52  total number of batches is  20\n",
      "...completed  53  epochs of training. Current loss:  0.21753425509306884 .\n",
      "epoch  53  total number of batches is  20\n",
      "...completed  54  epochs of training. Current loss:  0.21546456767804045 .\n",
      "epoch  54  total number of batches is  20\n",
      "...completed  55  epochs of training. Current loss:  0.21346199954312253 .\n",
      "epoch  55  total number of batches is  20\n",
      "...completed  56  epochs of training. Current loss:  0.21152316385304146 .\n",
      "epoch  56  total number of batches is  20\n",
      "...completed  57  epochs of training. Current loss:  0.2096448941123232 .\n",
      "epoch  57  total number of batches is  20\n",
      "...completed  58  epochs of training. Current loss:  0.20782422700693348 .\n",
      "epoch  58  total number of batches is  20\n",
      "...completed  59  epochs of training. Current loss:  0.20605838677138624 .\n",
      "epoch  59  total number of batches is  20\n",
      "...completed  60  epochs of training. Current loss:  0.20434477093603462 .\n",
      "epoch  60  total number of batches is  20\n",
      "...completed  61  epochs of training. Current loss:  0.2026809373217408 .\n",
      "epoch  61  total number of batches is  20\n",
      "...completed  62  epochs of training. Current loss:  0.2010645921613429 .\n",
      "epoch  62  total number of batches is  20\n",
      "...completed  63  epochs of training. Current loss:  0.19949357923901848 .\n",
      "epoch  63  total number of batches is  20\n",
      "...completed  64  epochs of training. Current loss:  0.19796586994958032 .\n",
      "epoch  64  total number of batches is  20\n",
      "...completed  65  epochs of training. Current loss:  0.19647955418980684 .\n",
      "epoch  65  total number of batches is  20\n",
      "...completed  66  epochs of training. Current loss:  0.19503283200305485 .\n",
      "epoch  66  total number of batches is  20\n",
      "...completed  67  epochs of training. Current loss:  0.19362400590662915 .\n",
      "epoch  67  total number of batches is  20\n",
      "...completed  68  epochs of training. Current loss:  0.19225147383873145 .\n",
      "epoch  68  total number of batches is  20\n",
      "...completed  69  epochs of training. Current loss:  0.19091372266834244 .\n",
      "epoch  69  total number of batches is  20\n",
      "...completed  70  epochs of training. Current loss:  0.18960932221718496 .\n",
      "epoch  70  total number of batches is  20\n",
      "...completed  71  epochs of training. Current loss:  0.18833691974804861 .\n",
      "epoch  71  total number of batches is  20\n",
      "...completed  72  epochs of training. Current loss:  0.18709523487830892 .\n",
      "epoch  72  total number of batches is  20\n",
      "...completed  73  epochs of training. Current loss:  0.18588305488151352 .\n",
      "epoch  73  total number of batches is  20\n",
      "...completed  74  epochs of training. Current loss:  0.1846992303435059 .\n",
      "epoch  74  total number of batches is  20\n",
      "...completed  75  epochs of training. Current loss:  0.1835426711427629 .\n",
      "epoch  75  total number of batches is  20\n",
      "...completed  76  epochs of training. Current loss:  0.18241234272749057 .\n",
      "epoch  76  total number of batches is  20\n",
      "...completed  77  epochs of training. Current loss:  0.18130726266459357 .\n",
      "epoch  77  total number of batches is  20\n",
      "...completed  78  epochs of training. Current loss:  0.18022649743794208 .\n",
      "epoch  78  total number of batches is  20\n",
      "...completed  79  epochs of training. Current loss:  0.17916915947543582 .\n",
      "epoch  79  total number of batches is  20\n",
      "...completed  80  epochs of training. Current loss:  0.17813440438623857 .\n",
      "epoch  80  total number of batches is  20\n",
      "...completed  81  epochs of training. Current loss:  0.17712142839124334 .\n",
      "epoch  81  total number of batches is  20\n",
      "...completed  82  epochs of training. Current loss:  0.17612946593135284 .\n",
      "epoch  82  total number of batches is  20\n",
      "...completed  83  epochs of training. Current loss:  0.17515778743953836 .\n",
      "epoch  83  total number of batches is  20\n",
      "...completed  84  epochs of training. Current loss:  0.17420569726388332 .\n",
      "epoch  84  total number of batches is  20\n",
      "...completed  85  epochs of training. Current loss:  0.17327253172994436 .\n",
      "epoch  85  total number of batches is  20\n",
      "...completed  86  epochs of training. Current loss:  0.17235765733177996 .\n",
      "epoch  86  total number of batches is  20\n",
      "...completed  87  epochs of training. Current loss:  0.17146046904191822 .\n",
      "epoch  87  total number of batches is  20\n",
      "...completed  88  epochs of training. Current loss:  0.17058038873136838 .\n",
      "epoch  88  total number of batches is  20\n",
      "...completed  89  epochs of training. Current loss:  0.16971686369153463 .\n",
      "epoch  89  total number of batches is  20\n",
      "...completed  90  epochs of training. Current loss:  0.16886936525057553 .\n",
      "epoch  90  total number of batches is  20\n",
      "...completed  91  epochs of training. Current loss:  0.16803738747736963 .\n",
      "epoch  91  total number of batches is  20\n",
      "...completed  92  epochs of training. Current loss:  0.16722044596681163 .\n",
      "epoch  92  total number of batches is  20\n",
      "...completed  93  epochs of training. Current loss:  0.16641807670067052 .\n",
      "epoch  93  total number of batches is  20\n",
      "...completed  94  epochs of training. Current loss:  0.16562983497870618 .\n",
      "epoch  94  total number of batches is  20\n",
      "...completed  95  epochs of training. Current loss:  0.1648552944151605 .\n",
      "epoch  95  total number of batches is  20\n",
      "...completed  96  epochs of training. Current loss:  0.16409404599612146 .\n",
      "epoch  96  total number of batches is  20\n",
      "...completed  97  epochs of training. Current loss:  0.16334569719360933 .\n",
      "epoch  97  total number of batches is  20\n",
      "...completed  98  epochs of training. Current loss:  0.16260987113254985 .\n",
      "epoch  98  total number of batches is  20\n",
      "...completed  99  epochs of training. Current loss:  0.16188620580709137 .\n",
      "epoch  99  total number of batches is  20\n",
      "...completed  100  epochs of training. Current loss:  0.16117435334298758 .\n",
      "epoch  100  total number of batches is  20\n",
      "...completed  101  epochs of training. Current loss:  0.16047397930301127 .\n",
      "epoch  101  total number of batches is  20\n",
      "...completed  102  epochs of training. Current loss:  0.15978476203258576 .\n",
      "epoch  102  total number of batches is  20\n",
      "...completed  103  epochs of training. Current loss:  0.1591063920430264 .\n",
      "epoch  103  total number of batches is  20\n",
      "...completed  104  epochs of training. Current loss:  0.15843857142997092 .\n",
      "epoch  104  total number of batches is  20\n",
      "...completed  105  epochs of training. Current loss:  0.1577810133247508 .\n",
      "epoch  105  total number of batches is  20\n",
      "...completed  106  epochs of training. Current loss:  0.1571334413766135 .\n",
      "epoch  106  total number of batches is  20\n",
      "...completed  107  epochs of training. Current loss:  0.15649558926385188 .\n",
      "epoch  107  total number of batches is  20\n",
      "...completed  108  epochs of training. Current loss:  0.15586720023203304 .\n",
      "epoch  108  total number of batches is  20\n",
      "...completed  109  epochs of training. Current loss:  0.15524802665764129 .\n",
      "epoch  109  total number of batches is  20\n",
      "...completed  110  epochs of training. Current loss:  0.15463782963556605 .\n",
      "epoch  110  total number of batches is  20\n",
      "...completed  111  epochs of training. Current loss:  0.15403637858897248 .\n",
      "epoch  111  total number of batches is  20\n",
      "...completed  112  epochs of training. Current loss:  0.15344345090018888 .\n",
      "epoch  112  total number of batches is  20\n",
      "...completed  113  epochs of training. Current loss:  0.15285883156133775 .\n",
      "epoch  113  total number of batches is  20\n",
      "...completed  114  epochs of training. Current loss:  0.15228231284352126 .\n",
      "epoch  114  total number of batches is  20\n",
      "...completed  115  epochs of training. Current loss:  0.1517136939834499 .\n",
      "epoch  115  total number of batches is  20\n",
      "...completed  116  epochs of training. Current loss:  0.15115278088647602 .\n",
      "epoch  116  total number of batches is  20\n",
      "...completed  117  epochs of training. Current loss:  0.15059938584506058 .\n",
      "epoch  117  total number of batches is  20\n",
      "...completed  118  epochs of training. Current loss:  0.1500533272717655 .\n",
      "epoch  118  total number of batches is  20\n",
      "...completed  119  epochs of training. Current loss:  0.1495144294459211 .\n",
      "epoch  119  total number of batches is  20\n",
      "...completed  120  epochs of training. Current loss:  0.14898252227317244 .\n",
      "epoch  120  total number of batches is  20\n",
      "...completed  121  epochs of training. Current loss:  0.1484574410571586 .\n",
      "epoch  121  total number of batches is  20\n",
      "...completed  122  epochs of training. Current loss:  0.1479390262826274 .\n",
      "epoch  122  total number of batches is  20\n",
      "...completed  123  epochs of training. Current loss:  0.1474271234093289 .\n",
      "epoch  123  total number of batches is  20\n",
      "...completed  124  epochs of training. Current loss:  0.14692158267607439 .\n",
      "epoch  124  total number of batches is  20\n",
      "...completed  125  epochs of training. Current loss:  0.1464222589143847 .\n",
      "epoch  125  total number of batches is  20\n",
      "...completed  126  epochs of training. Current loss:  0.14592901137118627 .\n",
      "epoch  126  total number of batches is  20\n",
      "...completed  127  epochs of training. Current loss:  0.14544170354004773 .\n",
      "epoch  127  total number of batches is  20\n",
      "...completed  128  epochs of training. Current loss:  0.14496020300048038 .\n",
      "epoch  128  total number of batches is  20\n",
      "...completed  129  epochs of training. Current loss:  0.14448438126485377 .\n",
      "epoch  129  total number of batches is  20\n",
      "...completed  130  epochs of training. Current loss:  0.14401411363250669 .\n",
      "epoch  130  total number of batches is  20\n",
      "...completed  131  epochs of training. Current loss:  0.1435492790506563 .\n",
      "epoch  131  total number of batches is  20\n",
      "...completed  132  epochs of training. Current loss:  0.14308975998173507 .\n",
      "epoch  132  total number of batches is  20\n",
      "...completed  133  epochs of training. Current loss:  0.14263544227680414 .\n",
      "epoch  133  total number of batches is  20\n",
      "...completed  134  epochs of training. Current loss:  0.1421862150547147 .\n",
      "epoch  134  total number of batches is  20\n",
      "...completed  135  epochs of training. Current loss:  0.14174197058670754 .\n",
      "epoch  135  total number of batches is  20\n",
      "...completed  136  epochs of training. Current loss:  0.1413026041861587 .\n",
      "epoch  136  total number of batches is  20\n",
      "...completed  137  epochs of training. Current loss:  0.14086801410319758 .\n",
      "epoch  137  total number of batches is  20\n",
      "...completed  138  epochs of training. Current loss:  0.140438101423938 .\n",
      "epoch  138  total number of batches is  20\n",
      "...completed  139  epochs of training. Current loss:  0.1400127699740801 .\n",
      "epoch  139  total number of batches is  20\n",
      "...completed  140  epochs of training. Current loss:  0.13959192622665226 .\n",
      "epoch  140  total number of batches is  20\n",
      "...completed  141  epochs of training. Current loss:  0.13917547921367854 .\n",
      "epoch  141  total number of batches is  20\n",
      "...completed  142  epochs of training. Current loss:  0.1387633404415671 .\n",
      "epoch  142  total number of batches is  20\n",
      "...completed  143  epochs of training. Current loss:  0.13835542381002813 .\n",
      "epoch  143  total number of batches is  20\n",
      "...completed  144  epochs of training. Current loss:  0.13795164553434078 .\n",
      "epoch  144  total number of batches is  20\n",
      "...completed  145  epochs of training. Current loss:  0.1375519240707981 .\n",
      "epoch  145  total number of batches is  20\n",
      "...completed  146  epochs of training. Current loss:  0.13715618004517033 .\n",
      "epoch  146  total number of batches is  20\n",
      "...completed  147  epochs of training. Current loss:  0.13676433618403358 .\n",
      "epoch  147  total number of batches is  20\n",
      "...completed  148  epochs of training. Current loss:  0.13637631724882307 .\n",
      "epoch  148  total number of batches is  20\n",
      "...completed  149  epochs of training. Current loss:  0.1359920499724745 .\n",
      "epoch  149  total number of batches is  20\n",
      "...completed  150  epochs of training. Current loss:  0.1356114629985274 .\n",
      "epoch  150  total number of batches is  20\n",
      "...completed  151  epochs of training. Current loss:  0.13523448682257072 .\n",
      "epoch  151  total number of batches is  20\n",
      "...completed  152  epochs of training. Current loss:  0.1348610537359172 .\n",
      "epoch  152  total number of batches is  20\n",
      "...completed  153  epochs of training. Current loss:  0.13449109777140011 .\n",
      "epoch  153  total number of batches is  20\n",
      "...completed  154  epochs of training. Current loss:  0.13412455465119186 .\n",
      "epoch  154  total number of batches is  20\n",
      "...completed  155  epochs of training. Current loss:  0.13376136173654893 .\n",
      "epoch  155  total number of batches is  20\n",
      "...completed  156  epochs of training. Current loss:  0.13340145797939432 .\n",
      "epoch  156  total number of batches is  20\n",
      "...completed  157  epochs of training. Current loss:  0.1330447838756518 .\n",
      "epoch  157  total number of batches is  20\n",
      "...completed  158  epochs of training. Current loss:  0.13269128142025285 .\n",
      "epoch  158  total number of batches is  20\n",
      "...completed  159  epochs of training. Current loss:  0.13234089406374025 .\n",
      "epoch  159  total number of batches is  20\n",
      "...completed  160  epochs of training. Current loss:  0.1319935666703964 .\n",
      "epoch  160  total number of batches is  20\n",
      "...completed  161  epochs of training. Current loss:  0.13164924547783027 .\n",
      "epoch  161  total number of batches is  20\n",
      "...completed  162  epochs of training. Current loss:  0.1313078780579574 .\n",
      "epoch  162  total number of batches is  20\n",
      "...completed  163  epochs of training. Current loss:  0.13096941327931344 .\n",
      "epoch  163  total number of batches is  20\n",
      "...completed  164  epochs of training. Current loss:  0.1306338012706438 .\n",
      "epoch  164  total number of batches is  20\n",
      "...completed  165  epochs of training. Current loss:  0.130300993385715 .\n",
      "epoch  165  total number of batches is  20\n",
      "...completed  166  epochs of training. Current loss:  0.1299709421692964 .\n",
      "epoch  166  total number of batches is  20\n",
      "...completed  167  epochs of training. Current loss:  0.12964360132426353 .\n",
      "epoch  167  total number of batches is  20\n",
      "...completed  168  epochs of training. Current loss:  0.12931892567977651 .\n",
      "epoch  168  total number of batches is  20\n",
      "...completed  169  epochs of training. Current loss:  0.12899687116049016 .\n",
      "epoch  169  total number of batches is  20\n",
      "...completed  170  epochs of training. Current loss:  0.12867739475675313 .\n",
      "epoch  170  total number of batches is  20\n",
      "...completed  171  epochs of training. Current loss:  0.12836045449575664 .\n",
      "epoch  171  total number of batches is  20\n",
      "...completed  172  epochs of training. Current loss:  0.12804600941359565 .\n",
      "epoch  172  total number of batches is  20\n",
      "...completed  173  epochs of training. Current loss:  0.12773401952820504 .\n",
      "epoch  173  total number of batches is  20\n",
      "...completed  174  epochs of training. Current loss:  0.12742444581313722 .\n",
      "epoch  174  total number of batches is  20\n",
      "...completed  175  epochs of training. Current loss:  0.12711725017214848 .\n",
      "epoch  175  total number of batches is  20\n",
      "...completed  176  epochs of training. Current loss:  0.12681239541456152 .\n",
      "epoch  176  total number of batches is  20\n",
      "...completed  177  epochs of training. Current loss:  0.12650984523137554 .\n",
      "epoch  177  total number of batches is  20\n",
      "...completed  178  epochs of training. Current loss:  0.12620956417209425 .\n",
      "epoch  178  total number of batches is  20\n",
      "...completed  179  epochs of training. Current loss:  0.12591151762224423 .\n",
      "epoch  179  total number of batches is  20\n",
      "...completed  180  epochs of training. Current loss:  0.12561567178155764 .\n",
      "epoch  180  total number of batches is  20\n",
      "...completed  181  epochs of training. Current loss:  0.12532199364279356 .\n",
      "epoch  181  total number of batches is  20\n",
      "...completed  182  epochs of training. Current loss:  0.12503045097117377 .\n",
      "epoch  182  total number of batches is  20\n",
      "...completed  183  epochs of training. Current loss:  0.12474101228440941 .\n",
      "epoch  183  total number of batches is  20\n",
      "...completed  184  epochs of training. Current loss:  0.12445364683329621 .\n",
      "epoch  184  total number of batches is  20\n",
      "...completed  185  epochs of training. Current loss:  0.12416832458285643 .\n",
      "epoch  185  total number of batches is  20\n",
      "...completed  186  epochs of training. Current loss:  0.12388501619400674 .\n",
      "epoch  186  total number of batches is  20\n",
      "...completed  187  epochs of training. Current loss:  0.12360369300573198 .\n",
      "epoch  187  total number of batches is  20\n",
      "...completed  188  epochs of training. Current loss:  0.12332432701774557 .\n",
      "epoch  188  total number of batches is  20\n",
      "...completed  189  epochs of training. Current loss:  0.12304689087361755 .\n",
      "epoch  189  total number of batches is  20\n",
      "...completed  190  epochs of training. Current loss:  0.12277135784435307 .\n",
      "epoch  190  total number of batches is  20\n",
      "...completed  191  epochs of training. Current loss:  0.12249770181240296 .\n",
      "epoch  191  total number of batches is  20\n",
      "...completed  192  epochs of training. Current loss:  0.12222589725609102 .\n",
      "epoch  192  total number of batches is  20\n",
      "...completed  193  epochs of training. Current loss:  0.12195591923444113 .\n",
      "epoch  193  total number of batches is  20\n",
      "...completed  194  epochs of training. Current loss:  0.12168774337238884 .\n",
      "epoch  194  total number of batches is  20\n",
      "...completed  195  epochs of training. Current loss:  0.12142134584636351 .\n",
      "epoch  195  total number of batches is  20\n",
      "...completed  196  epochs of training. Current loss:  0.12115670337022533 .\n",
      "epoch  196  total number of batches is  20\n",
      "...completed  197  epochs of training. Current loss:  0.1208937931815453 .\n",
      "epoch  197  total number of batches is  20\n",
      "...completed  198  epochs of training. Current loss:  0.12063259302821358 .\n",
      "epoch  198  total number of batches is  20\n",
      "...completed  199  epochs of training. Current loss:  0.1203730811553647 .\n",
      "epoch  199  total number of batches is  20\n",
      "...completed  200  epochs of training. Current loss:  0.12011523629260736 .\n",
      "epoch  200  total number of batches is  20\n",
      "...completed  201  epochs of training. Current loss:  0.11985903764154715 .\n",
      "epoch  201  total number of batches is  20\n",
      "...completed  202  epochs of training. Current loss:  0.11960446486359196 .\n",
      "epoch  202  total number of batches is  20\n",
      "...completed  203  epochs of training. Current loss:  0.11935149806802912 .\n",
      "epoch  203  total number of batches is  20\n",
      "...completed  204  epochs of training. Current loss:  0.11910011780036475 .\n",
      "epoch  204  total number of batches is  20\n",
      "...completed  205  epochs of training. Current loss:  0.11885030503091606 .\n",
      "epoch  205  total number of batches is  20\n",
      "...completed  206  epochs of training. Current loss:  0.11860204114364768 .\n",
      "epoch  206  total number of batches is  20\n",
      "...completed  207  epochs of training. Current loss:  0.11835530792524404 .\n",
      "epoch  207  total number of batches is  20\n",
      "...completed  208  epochs of training. Current loss:  0.11811008755440928 .\n",
      "epoch  208  total number of batches is  20\n",
      "...completed  209  epochs of training. Current loss:  0.11786636259138876 .\n",
      "epoch  209  total number of batches is  20\n",
      "...completed  210  epochs of training. Current loss:  0.11762411596770368 .\n",
      "epoch  210  total number of batches is  20\n",
      "...completed  211  epochs of training. Current loss:  0.11738333097609381 .\n",
      "epoch  211  total number of batches is  20\n",
      "...completed  212  epochs of training. Current loss:  0.1171439912606618 .\n",
      "epoch  212  total number of batches is  20\n",
      "...completed  213  epochs of training. Current loss:  0.11690608080721376 .\n",
      "epoch  213  total number of batches is  20\n",
      "...completed  214  epochs of training. Current loss:  0.11666958393379093 .\n",
      "epoch  214  total number of batches is  20\n",
      "...completed  215  epochs of training. Current loss:  0.11643448528138829 .\n",
      "epoch  215  total number of batches is  20\n",
      "...completed  216  epochs of training. Current loss:  0.1162007698048553 .\n",
      "epoch  216  total number of batches is  20\n",
      "...completed  217  epochs of training. Current loss:  0.11596842276397533 .\n",
      "epoch  217  total number of batches is  20\n",
      "...completed  218  epochs of training. Current loss:  0.11573742971472019 .\n",
      "epoch  218  total number of batches is  20\n",
      "...completed  219  epochs of training. Current loss:  0.11550777650067694 .\n",
      "epoch  219  total number of batches is  20\n",
      "...completed  220  epochs of training. Current loss:  0.11527944924464331 .\n",
      "epoch  220  total number of batches is  20\n",
      "...completed  221  epochs of training. Current loss:  0.11505243434039064 .\n",
      "epoch  221  total number of batches is  20\n",
      "...completed  222  epochs of training. Current loss:  0.1148267184445909 .\n",
      "epoch  222  total number of batches is  20\n",
      "...completed  223  epochs of training. Current loss:  0.1146022884689065 .\n",
      "epoch  223  total number of batches is  20\n",
      "...completed  224  epochs of training. Current loss:  0.11437913157224139 .\n",
      "epoch  224  total number of batches is  20\n",
      "...completed  225  epochs of training. Current loss:  0.11415723515315139 .\n",
      "epoch  225  total number of batches is  20\n",
      "...completed  226  epochs of training. Current loss:  0.11393658684241327 .\n",
      "epoch  226  total number of batches is  20\n",
      "...completed  227  epochs of training. Current loss:  0.11371717449575115 .\n",
      "epoch  227  total number of batches is  20\n",
      "...completed  228  epochs of training. Current loss:  0.11349898618671952 .\n",
      "epoch  228  total number of batches is  20\n",
      "...completed  229  epochs of training. Current loss:  0.11328201019974218 .\n",
      "epoch  229  total number of batches is  20\n",
      "...completed  230  epochs of training. Current loss:  0.11306623502330669 .\n",
      "epoch  230  total number of batches is  20\n",
      "...completed  231  epochs of training. Current loss:  0.11285164934331393 .\n",
      "epoch  231  total number of batches is  20\n",
      "...completed  232  epochs of training. Current loss:  0.11263824203658214 .\n",
      "epoch  232  total number of batches is  20\n",
      "...completed  233  epochs of training. Current loss:  0.11242600216450588 .\n",
      "epoch  233  total number of batches is  20\n",
      "...completed  234  epochs of training. Current loss:  0.11221491896686918 .\n",
      "epoch  234  total number of batches is  20\n",
      "...completed  235  epochs of training. Current loss:  0.11200498185581334 .\n",
      "epoch  235  total number of batches is  20\n",
      "...completed  236  epochs of training. Current loss:  0.11179618040995888 .\n",
      "epoch  236  total number of batches is  20\n",
      "...completed  237  epochs of training. Current loss:  0.11158850436868212 .\n",
      "epoch  237  total number of batches is  20\n",
      "...completed  238  epochs of training. Current loss:  0.11138194362654634 .\n",
      "epoch  238  total number of batches is  20\n",
      "...completed  239  epochs of training. Current loss:  0.11117648822788732 .\n",
      "epoch  239  total number of batches is  20\n",
      "...completed  240  epochs of training. Current loss:  0.11097212836155376 .\n",
      "epoch  240  total number of batches is  20\n",
      "...completed  241  epochs of training. Current loss:  0.11076885435580243 .\n",
      "epoch  241  total number of batches is  20\n",
      "...completed  242  epochs of training. Current loss:  0.11056665667334835 .\n",
      "epoch  242  total number of batches is  20\n",
      "...completed  243  epochs of training. Current loss:  0.11036552590656959 .\n",
      "epoch  243  total number of batches is  20\n",
      "...completed  244  epochs of training. Current loss:  0.11016545277286732 .\n",
      "epoch  244  total number of batches is  20\n",
      "...completed  245  epochs of training. Current loss:  0.10996642811018097 .\n",
      "epoch  245  total number of batches is  20\n",
      "...completed  246  epochs of training. Current loss:  0.10976844287265798 .\n",
      "epoch  246  total number of batches is  20\n",
      "...completed  247  epochs of training. Current loss:  0.10957148812647872 .\n",
      "epoch  247  total number of batches is  20\n",
      "...completed  248  epochs of training. Current loss:  0.10937555504583643 .\n",
      "epoch  248  total number of batches is  20\n",
      "...completed  249  epochs of training. Current loss:  0.10918063490907122 .\n",
      "epoch  249  total number of batches is  20\n",
      "...completed  250  epochs of training. Current loss:  0.10898671909495918 .\n",
      "epoch  250  total number of batches is  20\n",
      "...completed  251  epochs of training. Current loss:  0.10879379907915505 .\n",
      "epoch  251  total number of batches is  20\n",
      "...completed  252  epochs of training. Current loss:  0.10860186643078919 .\n",
      "epoch  252  total number of batches is  20\n",
      "...completed  253  epochs of training. Current loss:  0.10841091280921747 .\n",
      "epoch  253  total number of batches is  20\n",
      "...completed  254  epochs of training. Current loss:  0.10822092996092422 .\n",
      "epoch  254  total number of batches is  20\n",
      "...completed  255  epochs of training. Current loss:  0.10803190971657764 .\n",
      "epoch  255  total number of batches is  20\n",
      "...completed  256  epochs of training. Current loss:  0.1078438439882362 .\n",
      "epoch  256  total number of batches is  20\n",
      "...completed  257  epochs of training. Current loss:  0.10765672476670624 .\n",
      "epoch  257  total number of batches is  20\n",
      "...completed  258  epochs of training. Current loss:  0.10747054411904923 .\n",
      "epoch  258  total number of batches is  20\n",
      "...completed  259  epochs of training. Current loss:  0.10728529418623768 .\n",
      "epoch  259  total number of batches is  20\n",
      "...completed  260  epochs of training. Current loss:  0.10710096718095856 .\n",
      "epoch  260  total number of batches is  20\n",
      "...completed  261  epochs of training. Current loss:  0.10691755538556313 .\n",
      "epoch  261  total number of batches is  20\n",
      "...completed  262  epochs of training. Current loss:  0.10673505115016099 .\n",
      "epoch  262  total number of batches is  20\n",
      "...completed  263  epochs of training. Current loss:  0.1065534468908576 .\n",
      "epoch  263  total number of batches is  20\n",
      "...completed  264  epochs of training. Current loss:  0.10637273508813237 .\n",
      "epoch  264  total number of batches is  20\n",
      "...completed  265  epochs of training. Current loss:  0.10619290828535607 .\n",
      "epoch  265  total number of batches is  20\n",
      "...completed  266  epochs of training. Current loss:  0.10601395908744471 .\n",
      "epoch  266  total number of batches is  20\n",
      "...completed  267  epochs of training. Current loss:  0.1058358801596477 .\n",
      "epoch  267  total number of batches is  20\n",
      "...completed  268  epochs of training. Current loss:  0.10565866422646727 .\n",
      "epoch  268  total number of batches is  20\n",
      "...completed  269  epochs of training. Current loss:  0.10548230407070619 .\n",
      "epoch  269  total number of batches is  20\n",
      "...completed  270  epochs of training. Current loss:  0.10530679253264064 .\n",
      "epoch  270  total number of batches is  20\n",
      "...completed  271  epochs of training. Current loss:  0.10513212250931443 .\n",
      "epoch  271  total number of batches is  20\n",
      "...completed  272  epochs of training. Current loss:  0.10495828695395112 .\n",
      "epoch  272  total number of batches is  20\n",
      "...completed  273  epochs of training. Current loss:  0.10478527887547923 .\n",
      "epoch  273  total number of batches is  20\n",
      "...completed  274  epochs of training. Current loss:  0.10461309133816708 .\n",
      "epoch  274  total number of batches is  20\n",
      "...completed  275  epochs of training. Current loss:  0.10444171746136172 .\n",
      "epoch  275  total number of batches is  20\n",
      "...completed  276  epochs of training. Current loss:  0.10427115041932752 .\n",
      "epoch  276  total number of batches is  20\n",
      "...completed  277  epochs of training. Current loss:  0.10410138344117899 .\n",
      "epoch  277  total number of batches is  20\n",
      "...completed  278  epochs of training. Current loss:  0.1039324098109021 .\n",
      "epoch  278  total number of batches is  20\n",
      "...completed  279  epochs of training. Current loss:  0.10376422286745894 .\n",
      "epoch  279  total number of batches is  20\n",
      "...completed  280  epochs of training. Current loss:  0.10359681600496912 .\n",
      "epoch  280  total number of batches is  20\n",
      "...completed  281  epochs of training. Current loss:  0.10343018267296182 .\n",
      "epoch  281  total number of batches is  20\n",
      "...completed  282  epochs of training. Current loss:  0.10326431637669228 .\n",
      "epoch  282  total number of batches is  20\n",
      "...completed  283  epochs of training. Current loss:  0.10309921067751597 .\n",
      "epoch  283  total number of batches is  20\n",
      "...completed  284  epochs of training. Current loss:  0.10293485919331355 .\n",
      "epoch  284  total number of batches is  20\n",
      "...completed  285  epochs of training. Current loss:  0.1027712555989601 .\n",
      "epoch  285  total number of batches is  20\n",
      "...completed  286  epochs of training. Current loss:  0.10260839362683129 .\n",
      "epoch  286  total number of batches is  20\n",
      "...completed  287  epochs of training. Current loss:  0.1024462670673397 .\n",
      "epoch  287  total number of batches is  20\n",
      "...completed  288  epochs of training. Current loss:  0.10228486976949415 .\n",
      "epoch  288  total number of batches is  20\n",
      "...completed  289  epochs of training. Current loss:  0.10212419564147533 .\n",
      "epoch  289  total number of batches is  20\n",
      "...completed  290  epochs of training. Current loss:  0.1019642386512203 .\n",
      "epoch  290  total number of batches is  20\n",
      "...completed  291  epochs of training. Current loss:  0.10180499282700953 .\n",
      "epoch  291  total number of batches is  20\n",
      "...completed  292  epochs of training. Current loss:  0.10164645225804966 .\n",
      "epoch  292  total number of batches is  20\n",
      "...completed  293  epochs of training. Current loss:  0.1014886110950454 .\n",
      "epoch  293  total number of batches is  20\n",
      "...completed  294  epochs of training. Current loss:  0.10133146355075424 .\n",
      "epoch  294  total number of batches is  20\n",
      "...completed  295  epochs of training. Current loss:  0.10117500390051866 .\n",
      "epoch  295  total number of batches is  20\n",
      "...completed  296  epochs of training. Current loss:  0.10101922648276909 .\n",
      "epoch  296  total number of batches is  20\n",
      "...completed  297  epochs of training. Current loss:  0.10086412569949317 .\n",
      "epoch  297  total number of batches is  20\n",
      "...completed  298  epochs of training. Current loss:  0.10070969601666611 .\n",
      "epoch  298  total number of batches is  20\n",
      "...completed  299  epochs of training. Current loss:  0.10055593196463736 .\n",
      "epoch  299  total number of batches is  20\n",
      "...completed  300  epochs of training. Current loss:  0.1004028281384695 .\n",
      "epoch  300  total number of batches is  20\n",
      "...completed  301  epochs of training. Current loss:  0.10025037919822635 .\n",
      "epoch  301  total number of batches is  20\n",
      "...completed  302  epochs of training. Current loss:  0.10009857986920552 .\n",
      "epoch  302  total number of batches is  20\n",
      "...completed  303  epochs of training. Current loss:  0.09994742494211457 .\n",
      "epoch  303  total number of batches is  20\n",
      "...completed  304  epochs of training. Current loss:  0.0997969092731867 .\n",
      "epoch  304  total number of batches is  20\n",
      "...completed  305  epochs of training. Current loss:  0.09964702778423545 .\n",
      "epoch  305  total number of batches is  20\n",
      "...completed  306  epochs of training. Current loss:  0.09949777546264645 .\n",
      "epoch  306  total number of batches is  20\n",
      "...completed  307  epochs of training. Current loss:  0.0993491473613055 .\n",
      "epoch  307  total number of batches is  20\n",
      "...completed  308  epochs of training. Current loss:  0.09920113859846254 .\n",
      "epoch  308  total number of batches is  20\n",
      "...completed  309  epochs of training. Current loss:  0.09905374435753181 .\n",
      "epoch  309  total number of batches is  20\n",
      "...completed  310  epochs of training. Current loss:  0.0989069598868285 .\n",
      "epoch  310  total number of batches is  20\n",
      "...completed  311  epochs of training. Current loss:  0.09876078049924274 .\n",
      "epoch  311  total number of batches is  20\n",
      "...completed  312  epochs of training. Current loss:  0.09861520157185255 .\n",
      "epoch  312  total number of batches is  20\n",
      "...completed  313  epochs of training. Current loss:  0.09847021854547705 .\n",
      "epoch  313  total number of batches is  20\n",
      "...completed  314  epochs of training. Current loss:  0.09832582692417238 .\n",
      "epoch  314  total number of batches is  20\n",
      "...completed  315  epochs of training. Current loss:  0.09818202227467222 .\n",
      "epoch  315  total number of batches is  20\n",
      "...completed  316  epochs of training. Current loss:  0.09803880022577567 .\n",
      "epoch  316  total number of batches is  20\n",
      "...completed  317  epochs of training. Current loss:  0.09789615646768562 .\n",
      "epoch  317  total number of batches is  20\n",
      "...completed  318  epochs of training. Current loss:  0.09775408675130022 .\n",
      "epoch  318  total number of batches is  20\n",
      "...completed  319  epochs of training. Current loss:  0.09761258688746072 .\n",
      "epoch  319  total number of batches is  20\n",
      "...completed  320  epochs of training. Current loss:  0.09747165274615945 .\n",
      "epoch  320  total number of batches is  20\n",
      "...completed  321  epochs of training. Current loss:  0.09733128025571111 .\n",
      "epoch  321  total number of batches is  20\n",
      "...completed  322  epochs of training. Current loss:  0.09719146540189054 .\n",
      "epoch  322  total number of batches is  20\n",
      "...completed  323  epochs of training. Current loss:  0.09705220422704162 .\n",
      "epoch  323  total number of batches is  20\n",
      "...completed  324  epochs of training. Current loss:  0.09691349282915954 .\n",
      "epoch  324  total number of batches is  20\n",
      "...completed  325  epochs of training. Current loss:  0.0967753273609512 .\n",
      "epoch  325  total number of batches is  20\n",
      "...completed  326  epochs of training. Current loss:  0.09663770402887652 .\n",
      "epoch  326  total number of batches is  20\n",
      "...completed  327  epochs of training. Current loss:  0.0965006190921747 .\n",
      "epoch  327  total number of batches is  20\n",
      "...completed  328  epochs of training. Current loss:  0.09636406886187811 .\n",
      "epoch  328  total number of batches is  20\n",
      "...completed  329  epochs of training. Current loss:  0.09622804969981796 .\n",
      "epoch  329  total number of batches is  20\n",
      "...completed  330  epochs of training. Current loss:  0.09609255801762413 .\n",
      "epoch  330  total number of batches is  20\n",
      "...completed  331  epochs of training. Current loss:  0.09595759027572263 .\n",
      "epoch  331  total number of batches is  20\n",
      "...completed  332  epochs of training. Current loss:  0.0958231429823335 .\n",
      "epoch  332  total number of batches is  20\n",
      "...completed  333  epochs of training. Current loss:  0.09568921269247163 .\n",
      "epoch  333  total number of batches is  20\n",
      "...completed  334  epochs of training. Current loss:  0.09555579600695309 .\n",
      "epoch  334  total number of batches is  20\n",
      "...completed  335  epochs of training. Current loss:  0.09542288957140967 .\n",
      "epoch  335  total number of batches is  20\n",
      "...completed  336  epochs of training. Current loss:  0.0952904900753132 .\n",
      "epoch  336  total number of batches is  20\n",
      "...completed  337  epochs of training. Current loss:  0.09515859425101202 .\n",
      "epoch  337  total number of batches is  20\n",
      "...completed  338  epochs of training. Current loss:  0.09502719887278127 .\n",
      "epoch  338  total number of batches is  20\n",
      "...completed  339  epochs of training. Current loss:  0.09489630075588854 .\n",
      "epoch  339  total number of batches is  20\n",
      "...completed  340  epochs of training. Current loss:  0.09476589675567641 .\n",
      "epoch  340  total number of batches is  20\n",
      "...completed  341  epochs of training. Current loss:  0.09463598376666285 .\n",
      "epoch  341  total number of batches is  20\n",
      "...completed  342  epochs of training. Current loss:  0.09450655872166085 .\n",
      "epoch  342  total number of batches is  20\n",
      "...completed  343  epochs of training. Current loss:  0.09437761859091803 .\n",
      "epoch  343  total number of batches is  20\n",
      "...completed  344  epochs of training. Current loss:  0.09424916038127668 .\n",
      "epoch  344  total number of batches is  20\n",
      "...completed  345  epochs of training. Current loss:  0.09412118113535523 .\n",
      "epoch  345  total number of batches is  20\n",
      "...completed  346  epochs of training. Current loss:  0.09399367793075113 .\n",
      "epoch  346  total number of batches is  20\n",
      "...completed  347  epochs of training. Current loss:  0.0938666478792658 .\n",
      "epoch  347  total number of batches is  20\n",
      "...completed  348  epochs of training. Current loss:  0.09374008812615142 .\n",
      "epoch  348  total number of batches is  20\n",
      "...completed  349  epochs of training. Current loss:  0.0936139958493797 .\n",
      "epoch  349  total number of batches is  20\n",
      "...completed  350  epochs of training. Current loss:  0.09348836825893261 .\n",
      "epoch  350  total number of batches is  20\n",
      "...completed  351  epochs of training. Current loss:  0.09336320259611486 .\n",
      "epoch  351  total number of batches is  20\n",
      "...completed  352  epochs of training. Current loss:  0.09323849613288754 .\n",
      "epoch  352  total number of batches is  20\n",
      "...completed  353  epochs of training. Current loss:  0.09311424617122291 .\n",
      "epoch  353  total number of batches is  20\n",
      "...completed  354  epochs of training. Current loss:  0.0929904500424796 .\n",
      "epoch  354  total number of batches is  20\n",
      "...completed  355  epochs of training. Current loss:  0.09286710510679795 .\n",
      "epoch  355  total number of batches is  20\n",
      "...completed  356  epochs of training. Current loss:  0.09274420875251449 .\n",
      "epoch  356  total number of batches is  20\n",
      "...completed  357  epochs of training. Current loss:  0.09262175839559546 .\n",
      "epoch  357  total number of batches is  20\n",
      "...completed  358  epochs of training. Current loss:  0.09249975147908816 .\n",
      "epoch  358  total number of batches is  20\n",
      "...completed  359  epochs of training. Current loss:  0.09237818547258975 .\n",
      "epoch  359  total number of batches is  20\n",
      "...completed  360  epochs of training. Current loss:  0.09225705787173265 .\n",
      "epoch  360  total number of batches is  20\n",
      "...completed  361  epochs of training. Current loss:  0.09213636619768599 .\n",
      "epoch  361  total number of batches is  20\n",
      "...completed  362  epochs of training. Current loss:  0.09201610799667162 .\n",
      "epoch  362  total number of batches is  20\n",
      "...completed  363  epochs of training. Current loss:  0.09189628083949505 .\n",
      "epoch  363  total number of batches is  20\n",
      "...completed  364  epochs of training. Current loss:  0.09177688232108946 .\n",
      "epoch  364  total number of batches is  20\n",
      "...completed  365  epochs of training. Current loss:  0.09165791006007251 .\n",
      "epoch  365  total number of batches is  20\n",
      "...completed  366  epochs of training. Current loss:  0.09153936169831539 .\n",
      "epoch  366  total number of batches is  20\n",
      "...completed  367  epochs of training. Current loss:  0.09142123490052254 .\n",
      "epoch  367  total number of batches is  20\n",
      "...completed  368  epochs of training. Current loss:  0.09130352735382241 .\n",
      "epoch  368  total number of batches is  20\n",
      "...completed  369  epochs of training. Current loss:  0.09118623676736755 .\n",
      "epoch  369  total number of batches is  20\n",
      "...completed  370  epochs of training. Current loss:  0.09106936087194369 .\n",
      "epoch  370  total number of batches is  20\n",
      "...completed  371  epochs of training. Current loss:  0.0909528974195876 .\n",
      "epoch  371  total number of batches is  20\n",
      "...completed  372  epochs of training. Current loss:  0.0908368441832121 .\n",
      "epoch  372  total number of batches is  20\n",
      "...completed  373  epochs of training. Current loss:  0.09072119895623848 .\n",
      "epoch  373  total number of batches is  20\n",
      "...completed  374  epochs of training. Current loss:  0.0906059595522353 .\n",
      "epoch  374  total number of batches is  20\n",
      "...completed  375  epochs of training. Current loss:  0.09049112380456316 .\n",
      "epoch  375  total number of batches is  20\n",
      "...completed  376  epochs of training. Current loss:  0.09037668956602475 .\n",
      "epoch  376  total number of batches is  20\n",
      "...completed  377  epochs of training. Current loss:  0.09026265470851991 .\n",
      "epoch  377  total number of batches is  20\n",
      "...completed  378  epochs of training. Current loss:  0.09014901712270522 .\n",
      "epoch  378  total number of batches is  20\n",
      "...completed  379  epochs of training. Current loss:  0.09003577471765732 .\n",
      "epoch  379  total number of batches is  20\n",
      "...completed  380  epochs of training. Current loss:  0.08992292542054038 .\n",
      "epoch  380  total number of batches is  20\n",
      "...completed  381  epochs of training. Current loss:  0.0898104671762762 .\n",
      "epoch  381  total number of batches is  20\n",
      "...completed  382  epochs of training. Current loss:  0.08969839794721812 .\n",
      "epoch  382  total number of batches is  20\n",
      "...completed  383  epochs of training. Current loss:  0.08958671571282677 .\n",
      "epoch  383  total number of batches is  20\n",
      "...completed  384  epochs of training. Current loss:  0.08947541846934894 .\n",
      "epoch  384  total number of batches is  20\n",
      "...completed  385  epochs of training. Current loss:  0.0893645042294985 .\n",
      "epoch  385  total number of batches is  20\n",
      "...completed  386  epochs of training. Current loss:  0.0892539710221392 .\n",
      "epoch  386  total number of batches is  20\n",
      "...completed  387  epochs of training. Current loss:  0.08914381689196964 .\n",
      "epoch  387  total number of batches is  20\n",
      "...completed  388  epochs of training. Current loss:  0.08903403989921001 .\n",
      "epoch  388  total number of batches is  20\n",
      "...completed  389  epochs of training. Current loss:  0.08892463811929043 .\n",
      "epoch  389  total number of batches is  20\n",
      "...completed  390  epochs of training. Current loss:  0.08881560964254101 .\n",
      "epoch  390  total number of batches is  20\n",
      "...completed  391  epochs of training. Current loss:  0.08870695257388368 .\n",
      "epoch  391  total number of batches is  20\n",
      "...completed  392  epochs of training. Current loss:  0.08859866503252532 .\n",
      "epoch  392  total number of batches is  20\n",
      "...completed  393  epochs of training. Current loss:  0.08849074515165284 .\n",
      "epoch  393  total number of batches is  20\n",
      "...completed  394  epochs of training. Current loss:  0.08838319107812974 .\n",
      "epoch  394  total number of batches is  20\n",
      "...completed  395  epochs of training. Current loss:  0.08827600097219451 .\n",
      "epoch  395  total number of batches is  20\n",
      "...completed  396  epochs of training. Current loss:  0.08816917300716069 .\n",
      "epoch  396  total number of batches is  20\n",
      "...completed  397  epochs of training. Current loss:  0.08806270536911905 .\n",
      "epoch  397  total number of batches is  20\n",
      "...completed  398  epochs of training. Current loss:  0.08795659625664155 .\n",
      "epoch  398  total number of batches is  20\n",
      "...completed  399  epochs of training. Current loss:  0.08785084388048756 .\n",
      "epoch  399  total number of batches is  20\n",
      "...completed  400  epochs of training. Current loss:  0.08774544646331237 .\n",
      "epoch  400  total number of batches is  20\n",
      "...completed  401  epochs of training. Current loss:  0.08764040223937809 .\n",
      "epoch  401  total number of batches is  20\n",
      "...completed  402  epochs of training. Current loss:  0.08753570945426704 .\n",
      "epoch  402  total number of batches is  20\n",
      "...completed  403  epochs of training. Current loss:  0.08743136636459807 .\n",
      "epoch  403  total number of batches is  20\n",
      "...completed  404  epochs of training. Current loss:  0.08732737123774562 .\n",
      "epoch  404  total number of batches is  20\n",
      "...completed  405  epochs of training. Current loss:  0.08722372235156181 .\n",
      "epoch  405  total number of batches is  20\n",
      "...completed  406  epochs of training. Current loss:  0.08712041799410224 .\n",
      "epoch  406  total number of batches is  20\n",
      "...completed  407  epochs of training. Current loss:  0.08701745646335464 .\n",
      "epoch  407  total number of batches is  20\n",
      "...completed  408  epochs of training. Current loss:  0.08691483606697147 .\n",
      "epoch  408  total number of batches is  20\n",
      "...completed  409  epochs of training. Current loss:  0.08681255512200632 .\n",
      "epoch  409  total number of batches is  20\n",
      "...completed  410  epochs of training. Current loss:  0.08671061195465421 .\n",
      "epoch  410  total number of batches is  20\n",
      "...completed  411  epochs of training. Current loss:  0.08660900489999611 .\n",
      "epoch  411  total number of batches is  20\n",
      "...completed  412  epochs of training. Current loss:  0.08650773230174763 .\n",
      "epoch  412  total number of batches is  20\n",
      "...completed  413  epochs of training. Current loss:  0.08640679251201239 .\n",
      "epoch  413  total number of batches is  20\n",
      "...completed  414  epochs of training. Current loss:  0.0863061838910398 .\n",
      "epoch  414  total number of batches is  20\n",
      "...completed  415  epochs of training. Current loss:  0.08620590480698784 .\n",
      "epoch  415  total number of batches is  20\n",
      "...completed  416  epochs of training. Current loss:  0.08610595363569053 .\n",
      "epoch  416  total number of batches is  20\n",
      "...completed  417  epochs of training. Current loss:  0.08600632876043043 .\n",
      "epoch  417  total number of batches is  20\n",
      "...completed  418  epochs of training. Current loss:  0.08590702857171666 .\n",
      "epoch  418  total number of batches is  20\n",
      "...completed  419  epochs of training. Current loss:  0.08580805146706746 .\n",
      "epoch  419  total number of batches is  20\n",
      "...completed  420  epochs of training. Current loss:  0.08570939585079872 .\n",
      "epoch  420  total number of batches is  20\n",
      "...completed  421  epochs of training. Current loss:  0.08561106013381757 .\n",
      "epoch  421  total number of batches is  20\n",
      "...completed  422  epochs of training. Current loss:  0.08551304273342157 .\n",
      "epoch  422  total number of batches is  20\n",
      "...completed  423  epochs of training. Current loss:  0.08541534207310328 .\n",
      "epoch  423  total number of batches is  20\n",
      "...completed  424  epochs of training. Current loss:  0.08531795658236044 .\n",
      "epoch  424  total number of batches is  20\n",
      "...completed  425  epochs of training. Current loss:  0.0852208846965118 .\n",
      "epoch  425  total number of batches is  20\n",
      "...completed  426  epochs of training. Current loss:  0.08512412485651825 .\n",
      "epoch  426  total number of batches is  20\n",
      "...completed  427  epochs of training. Current loss:  0.08502767550880992 .\n",
      "epoch  427  total number of batches is  20\n",
      "...completed  428  epochs of training. Current loss:  0.0849315351051183 .\n",
      "epoch  428  total number of batches is  20\n",
      "...completed  429  epochs of training. Current loss:  0.08483570210231431 .\n",
      "epoch  429  total number of batches is  20\n",
      "...completed  430  epochs of training. Current loss:  0.08474017496225153 .\n",
      "epoch  430  total number of batches is  20\n",
      "...completed  431  epochs of training. Current loss:  0.08464495215161488 .\n",
      "epoch  431  total number of batches is  20\n",
      "...completed  432  epochs of training. Current loss:  0.08455003214177474 .\n",
      "epoch  432  total number of batches is  20\n",
      "...completed  433  epochs of training. Current loss:  0.08445541340864592 .\n",
      "epoch  433  total number of batches is  20\n",
      "...completed  434  epochs of training. Current loss:  0.08436109443255212 .\n",
      "epoch  434  total number of batches is  20\n",
      "...completed  435  epochs of training. Current loss:  0.08426707369809529 .\n",
      "epoch  435  total number of batches is  20\n",
      "...completed  436  epochs of training. Current loss:  0.08417334969402958 .\n",
      "epoch  436  total number of batches is  20\n",
      "...completed  437  epochs of training. Current loss:  0.0840799209131404 .\n",
      "epoch  437  total number of batches is  20\n",
      "...completed  438  epochs of training. Current loss:  0.08398678585212785 .\n",
      "epoch  438  total number of batches is  20\n",
      "...completed  439  epochs of training. Current loss:  0.08389394301149461 .\n",
      "epoch  439  total number of batches is  20\n",
      "...completed  440  epochs of training. Current loss:  0.08380139089543823 .\n",
      "epoch  440  total number of batches is  20\n",
      "...completed  441  epochs of training. Current loss:  0.08370912801174733 .\n",
      "epoch  441  total number of batches is  20\n",
      "...completed  442  epochs of training. Current loss:  0.08361715287170203 .\n",
      "epoch  442  total number of batches is  20\n",
      "...completed  443  epochs of training. Current loss:  0.0835254639899779 .\n",
      "epoch  443  total number of batches is  20\n",
      "...completed  444  epochs of training. Current loss:  0.08343405988455355 .\n",
      "epoch  444  total number of batches is  20\n",
      "...completed  445  epochs of training. Current loss:  0.0833429390766219 .\n",
      "epoch  445  total number of batches is  20\n",
      "...completed  446  epochs of training. Current loss:  0.08325210009050424 .\n",
      "epoch  446  total number of batches is  20\n",
      "...completed  447  epochs of training. Current loss:  0.08316154145356791 .\n",
      "epoch  447  total number of batches is  20\n",
      "...completed  448  epochs of training. Current loss:  0.08307126169614633 .\n",
      "epoch  448  total number of batches is  20\n",
      "...completed  449  epochs of training. Current loss:  0.08298125935146206 .\n",
      "epoch  449  total number of batches is  20\n",
      "...completed  450  epochs of training. Current loss:  0.08289153295555246 .\n",
      "epoch  450  total number of batches is  20\n",
      "...completed  451  epochs of training. Current loss:  0.08280208104719751 .\n",
      "epoch  451  total number of batches is  20\n",
      "...completed  452  epochs of training. Current loss:  0.08271290216784996 .\n",
      "epoch  452  total number of batches is  20\n",
      "...completed  453  epochs of training. Current loss:  0.08262399486156764 .\n",
      "epoch  453  total number of batches is  20\n",
      "...completed  454  epochs of training. Current loss:  0.08253535767494762 .\n",
      "epoch  454  total number of batches is  20\n",
      "...completed  455  epochs of training. Current loss:  0.08244698915706215 .\n",
      "epoch  455  total number of batches is  20\n",
      "...completed  456  epochs of training. Current loss:  0.08235888785939634 .\n",
      "epoch  456  total number of batches is  20\n",
      "...completed  457  epochs of training. Current loss:  0.08227105233578742 .\n",
      "epoch  457  total number of batches is  20\n",
      "...completed  458  epochs of training. Current loss:  0.08218348114236546 .\n",
      "epoch  458  total number of batches is  20\n",
      "...completed  459  epochs of training. Current loss:  0.0820961728374954 .\n",
      "epoch  459  total number of batches is  20\n",
      "...completed  460  epochs of training. Current loss:  0.08200912598172053 .\n",
      "epoch  460  total number of batches is  20\n",
      "...completed  461  epochs of training. Current loss:  0.08192233913770723 .\n",
      "epoch  461  total number of batches is  20\n",
      "...completed  462  epochs of training. Current loss:  0.08183581087019075 .\n",
      "epoch  462  total number of batches is  20\n",
      "...completed  463  epochs of training. Current loss:  0.08174953974592239 .\n",
      "epoch  463  total number of batches is  20\n",
      "...completed  464  epochs of training. Current loss:  0.0816635243336178 .\n",
      "epoch  464  total number of batches is  20\n",
      "...completed  465  epochs of training. Current loss:  0.08157776320390654 .\n",
      "epoch  465  total number of batches is  20\n",
      "...completed  466  epochs of training. Current loss:  0.08149225492928261 .\n",
      "epoch  466  total number of batches is  20\n",
      "...completed  467  epochs of training. Current loss:  0.08140699808405658 .\n",
      "epoch  467  total number of batches is  20\n",
      "...completed  468  epochs of training. Current loss:  0.08132199124430887 .\n",
      "epoch  468  total number of batches is  20\n",
      "...completed  469  epochs of training. Current loss:  0.08123723298784438 .\n",
      "epoch  469  total number of batches is  20\n",
      "...completed  470  epochs of training. Current loss:  0.08115272189414874 .\n",
      "epoch  470  total number of batches is  20\n",
      "...completed  471  epochs of training. Current loss:  0.08106845654434604 .\n",
      "epoch  471  total number of batches is  20\n",
      "...completed  472  epochs of training. Current loss:  0.08098443552115832 .\n",
      "epoch  472  total number of batches is  20\n",
      "...completed  473  epochs of training. Current loss:  0.08090065740886698 .\n",
      "epoch  473  total number of batches is  20\n",
      "...completed  474  epochs of training. Current loss:  0.08081712079327608 .\n",
      "epoch  474  total number of batches is  20\n",
      "...completed  475  epochs of training. Current loss:  0.0807338242616779 .\n",
      "epoch  475  total number of batches is  20\n",
      "...completed  476  epochs of training. Current loss:  0.0806507664028208 .\n",
      "epoch  476  total number of batches is  20\n",
      "...completed  477  epochs of training. Current loss:  0.0805679458068798 .\n",
      "epoch  477  total number of batches is  20\n",
      "...completed  478  epochs of training. Current loss:  0.08048536106542961 .\n",
      "epoch  478  total number of batches is  20\n",
      "...completed  479  epochs of training. Current loss:  0.08040301077142081 .\n",
      "epoch  479  total number of batches is  20\n",
      "...completed  480  epochs of training. Current loss:  0.08032089351915932 .\n",
      "epoch  480  total number of batches is  20\n",
      "...completed  481  epochs of training. Current loss:  0.08023900790428892 .\n",
      "epoch  481  total number of batches is  20\n",
      "...completed  482  epochs of training. Current loss:  0.08015735252377783 .\n",
      "epoch  482  total number of batches is  20\n",
      "...completed  483  epochs of training. Current loss:  0.0800759259759088 .\n",
      "epoch  483  total number of batches is  20\n",
      "...completed  484  epochs of training. Current loss:  0.07999472686027345 .\n",
      "epoch  484  total number of batches is  20\n",
      "...completed  485  epochs of training. Current loss:  0.07991375377777094 .\n",
      "epoch  485  total number of batches is  20\n",
      "...completed  486  epochs of training. Current loss:  0.07983300533061115 .\n",
      "epoch  486  total number of batches is  20\n",
      "...completed  487  epochs of training. Current loss:  0.0797524801223226 .\n",
      "epoch  487  total number of batches is  20\n",
      "...completed  488  epochs of training. Current loss:  0.0796721767577655 .\n",
      "epoch  488  total number of batches is  20\n",
      "...completed  489  epochs of training. Current loss:  0.07959209384314976 .\n",
      "epoch  489  total number of batches is  20\n",
      "...completed  490  epochs of training. Current loss:  0.07951222998605856 .\n",
      "epoch  490  total number of batches is  20\n",
      "...completed  491  epochs of training. Current loss:  0.07943258379547742 .\n",
      "epoch  491  total number of batches is  20\n",
      "...completed  492  epochs of training. Current loss:  0.07935315388182904 .\n",
      "epoch  492  total number of batches is  20\n",
      "...completed  493  epochs of training. Current loss:  0.0792739388570138 .\n",
      "epoch  493  total number of batches is  20\n",
      "...completed  494  epochs of training. Current loss:  0.07919493733445654 .\n",
      "epoch  494  total number of batches is  20\n",
      "...completed  495  epochs of training. Current loss:  0.0791161479291592 .\n",
      "epoch  495  total number of batches is  20\n",
      "...completed  496  epochs of training. Current loss:  0.07903756925775983 .\n",
      "epoch  496  total number of batches is  20\n",
      "...completed  497  epochs of training. Current loss:  0.07895919993859737 .\n",
      "epoch  497  total number of batches is  20\n",
      "...completed  498  epochs of training. Current loss:  0.07888103859178335 .\n",
      "epoch  498  total number of batches is  20\n",
      "...completed  499  epochs of training. Current loss:  0.07880308383927906 .\n",
      "epoch  499  total number of batches is  20\n",
      "...completed  500  epochs of training. Current loss:  0.07872533430497952 .\n",
      "Training complete.\n",
      "Training starting...\n",
      "epoch  0  total number of batches is  20\n",
      "...completed  1  epochs of training. Current loss:  0.9004680828590035 .\n",
      "epoch  1  total number of batches is  20\n",
      "...completed  2  epochs of training. Current loss:  0.889753214005671 .\n",
      "epoch  2  total number of batches is  20\n",
      "...completed  3  epochs of training. Current loss:  0.8767827022247235 .\n",
      "epoch  3  total number of batches is  20\n",
      "...completed  4  epochs of training. Current loss:  0.8603021718608461 .\n",
      "epoch  4  total number of batches is  20\n",
      "...completed  5  epochs of training. Current loss:  0.8394116303104191 .\n",
      "epoch  5  total number of batches is  20\n",
      "...completed  6  epochs of training. Current loss:  0.8138660844134074 .\n",
      "epoch  6  total number of batches is  20\n",
      "...completed  7  epochs of training. Current loss:  0.7846852373062692 .\n",
      "epoch  7  total number of batches is  20\n",
      "...completed  8  epochs of training. Current loss:  0.7538420202602164 .\n",
      "epoch  8  total number of batches is  20\n",
      "...completed  9  epochs of training. Current loss:  0.7231997712522195 .\n",
      "epoch  9  total number of batches is  20\n",
      "...completed  10  epochs of training. Current loss:  0.6939089024839327 .\n",
      "epoch  10  total number of batches is  20\n",
      "...completed  11  epochs of training. Current loss:  0.6664953171894336 .\n",
      "epoch  11  total number of batches is  20\n",
      "...completed  12  epochs of training. Current loss:  0.6411093975291859 .\n",
      "epoch  12  total number of batches is  20\n",
      "...completed  13  epochs of training. Current loss:  0.6177012404890376 .\n",
      "epoch  13  total number of batches is  20\n",
      "...completed  14  epochs of training. Current loss:  0.5961310995665849 .\n",
      "epoch  14  total number of batches is  20\n",
      "...completed  15  epochs of training. Current loss:  0.5762349381250488 .\n",
      "epoch  15  total number of batches is  20\n",
      "...completed  16  epochs of training. Current loss:  0.5578548429600451 .\n",
      "epoch  16  total number of batches is  20\n",
      "...completed  17  epochs of training. Current loss:  0.5408476854241359 .\n",
      "epoch  17  total number of batches is  20\n",
      "...completed  18  epochs of training. Current loss:  0.5250847762815969 .\n",
      "epoch  18  total number of batches is  20\n",
      "...completed  19  epochs of training. Current loss:  0.5104497274127191 .\n",
      "epoch  19  total number of batches is  20\n",
      "...completed  20  epochs of training. Current loss:  0.4968368217916525 .\n",
      "epoch  20  total number of batches is  20\n",
      "...completed  21  epochs of training. Current loss:  0.48414996209583294 .\n",
      "epoch  21  total number of batches is  20\n",
      "...completed  22  epochs of training. Current loss:  0.47230186775966637 .\n",
      "epoch  22  total number of batches is  20\n",
      "...completed  23  epochs of training. Current loss:  0.46121336603502466 .\n",
      "epoch  23  total number of batches is  20\n",
      "...completed  24  epochs of training. Current loss:  0.45081276561661243 .\n",
      "epoch  24  total number of batches is  20\n",
      "...completed  25  epochs of training. Current loss:  0.4410353218148085 .\n",
      "epoch  25  total number of batches is  20\n",
      "...completed  26  epochs of training. Current loss:  0.43182277494136995 .\n",
      "epoch  26  total number of batches is  20\n",
      "...completed  27  epochs of training. Current loss:  0.4231229259100995 .\n",
      "epoch  27  total number of batches is  20\n",
      "...completed  28  epochs of training. Current loss:  0.4148892169868159 .\n",
      "epoch  28  total number of batches is  20\n",
      "...completed  29  epochs of training. Current loss:  0.4070803011477889 .\n",
      "epoch  29  total number of batches is  20\n",
      "...completed  30  epochs of training. Current loss:  0.39965959908026455 .\n",
      "epoch  30  total number of batches is  20\n",
      "...completed  31  epochs of training. Current loss:  0.3925948525808621 .\n",
      "epoch  31  total number of batches is  20\n",
      "...completed  32  epochs of training. Current loss:  0.38585768623007355 .\n",
      "epoch  32  total number of batches is  20\n",
      "...completed  33  epochs of training. Current loss:  0.37942318764783295 .\n",
      "epoch  33  total number of batches is  20\n",
      "...completed  34  epochs of training. Current loss:  0.37326951288588517 .\n",
      "epoch  34  total number of batches is  20\n",
      "...completed  35  epochs of training. Current loss:  0.3673775196267796 .\n",
      "epoch  35  total number of batches is  20\n",
      "...completed  36  epochs of training. Current loss:  0.36173042802387917 .\n",
      "epoch  36  total number of batches is  20\n",
      "...completed  37  epochs of training. Current loss:  0.3563135076185006 .\n",
      "epoch  37  total number of batches is  20\n",
      "...completed  38  epochs of training. Current loss:  0.3511137886356014 .\n",
      "epoch  38  total number of batches is  20\n",
      "...completed  39  epochs of training. Current loss:  0.34611979662926445 .\n",
      "epoch  39  total number of batches is  20\n",
      "...completed  40  epochs of training. Current loss:  0.34132131040490965 .\n",
      "epoch  40  total number of batches is  20\n",
      "...completed  41  epochs of training. Current loss:  0.3367091439559091 .\n",
      "epoch  41  total number of batches is  20\n",
      "...completed  42  epochs of training. Current loss:  0.3322749535479278 .\n",
      "epoch  42  total number of batches is  20\n",
      "...completed  43  epochs of training. Current loss:  0.3280110709693092 .\n",
      "epoch  43  total number of batches is  20\n",
      "...completed  44  epochs of training. Current loss:  0.3239103633935106 .\n",
      "epoch  44  total number of batches is  20\n",
      "...completed  45  epochs of training. Current loss:  0.3199661194234503 .\n",
      "epoch  45  total number of batches is  20\n",
      "...completed  46  epochs of training. Current loss:  0.3161719599044334 .\n",
      "epoch  46  total number of batches is  20\n",
      "...completed  47  epochs of training. Current loss:  0.31252177118991575 .\n",
      "epoch  47  total number of batches is  20\n",
      "...completed  48  epochs of training. Current loss:  0.30900965786370144 .\n",
      "epoch  48  total number of batches is  20\n",
      "...completed  49  epochs of training. Current loss:  0.30562991153833696 .\n",
      "epoch  49  total number of batches is  20\n",
      "...completed  50  epochs of training. Current loss:  0.3023769922717377 .\n",
      "epoch  50  total number of batches is  20\n",
      "...completed  51  epochs of training. Current loss:  0.2992455193299061 .\n",
      "epoch  51  total number of batches is  20\n",
      "...completed  52  epochs of training. Current loss:  0.29623026839987393 .\n",
      "epoch  52  total number of batches is  20\n",
      "...completed  53  epochs of training. Current loss:  0.29332617284146467 .\n",
      "epoch  53  total number of batches is  20\n",
      "...completed  54  epochs of training. Current loss:  0.2905283270844691 .\n",
      "epoch  54  total number of batches is  20\n",
      "...completed  55  epochs of training. Current loss:  0.2878319907721227 .\n",
      "epoch  55  total number of batches is  20\n",
      "...completed  56  epochs of training. Current loss:  0.2852325926857783 .\n",
      "epoch  56  total number of batches is  20\n",
      "...completed  57  epochs of training. Current loss:  0.2827257338417274 .\n",
      "epoch  57  total number of batches is  20\n",
      "...completed  58  epochs of training. Current loss:  0.28030718942617283 .\n",
      "epoch  58  total number of batches is  20\n",
      "...completed  59  epochs of training. Current loss:  0.27797290943486974 .\n",
      "epoch  59  total number of batches is  20\n",
      "...completed  60  epochs of training. Current loss:  0.2757190180213477 .\n",
      "epoch  60  total number of batches is  20\n",
      "...completed  61  epochs of training. Current loss:  0.27354181164495595 .\n",
      "epoch  61  total number of batches is  20\n",
      "...completed  62  epochs of training. Current loss:  0.27143775615968085 .\n",
      "epoch  62  total number of batches is  20\n",
      "...completed  63  epochs of training. Current loss:  0.26940348300752964 .\n",
      "epoch  63  total number of batches is  20\n",
      "...completed  64  epochs of training. Current loss:  0.26743578468492923 .\n",
      "epoch  64  total number of batches is  20\n",
      "...completed  65  epochs of training. Current loss:  0.2655316096436462 .\n",
      "epoch  65  total number of batches is  20\n",
      "...completed  66  epochs of training. Current loss:  0.26368805677402 .\n",
      "epoch  66  total number of batches is  20\n",
      "...completed  67  epochs of training. Current loss:  0.2619023696011899 .\n",
      "epoch  67  total number of batches is  20\n",
      "...completed  68  epochs of training. Current loss:  0.26017193030675145 .\n",
      "epoch  68  total number of batches is  20\n",
      "...completed  69  epochs of training. Current loss:  0.2584942536703495 .\n",
      "epoch  69  total number of batches is  20\n",
      "...completed  70  epochs of training. Current loss:  0.2568669810089642 .\n",
      "epoch  70  total number of batches is  20\n",
      "...completed  71  epochs of training. Current loss:  0.2552878741765385 .\n",
      "epoch  71  total number of batches is  20\n",
      "...completed  72  epochs of training. Current loss:  0.2537548096733231 .\n",
      "epoch  72  total number of batches is  20\n",
      "...completed  73  epochs of training. Current loss:  0.25226577290288765 .\n",
      "epoch  73  total number of batches is  20\n",
      "...completed  74  epochs of training. Current loss:  0.2508188526050818 .\n",
      "epoch  74  total number of batches is  20\n",
      "...completed  75  epochs of training. Current loss:  0.2494122354851783 .\n",
      "epoch  75  total number of batches is  20\n",
      "...completed  76  epochs of training. Current loss:  0.2480442010528106 .\n",
      "epoch  76  total number of batches is  20\n",
      "...completed  77  epochs of training. Current loss:  0.246713116678949 .\n",
      "epoch  77  total number of batches is  20\n",
      "...completed  78  epochs of training. Current loss:  0.24541743287486012 .\n",
      "epoch  78  total number of batches is  20\n",
      "...completed  79  epochs of training. Current loss:  0.2441556787936056 .\n",
      "epoch  79  total number of batches is  20\n",
      "...completed  80  epochs of training. Current loss:  0.24292645795199325 .\n",
      "epoch  80  total number of batches is  20\n",
      "...completed  81  epochs of training. Current loss:  0.24172844416888412 .\n",
      "epoch  81  total number of batches is  20\n",
      "...completed  82  epochs of training. Current loss:  0.24056037771425348 .\n",
      "epoch  82  total number of batches is  20\n",
      "...completed  83  epochs of training. Current loss:  0.23942106166231256 .\n",
      "epoch  83  total number of batches is  20\n",
      "...completed  84  epochs of training. Current loss:  0.23830935844124332 .\n",
      "epoch  84  total number of batches is  20\n",
      "...completed  85  epochs of training. Current loss:  0.23722418657160185 .\n",
      "epoch  85  total number of batches is  20\n",
      "...completed  86  epochs of training. Current loss:  0.2361645175851638 .\n",
      "epoch  86  total number of batches is  20\n",
      "...completed  87  epochs of training. Current loss:  0.23512937311586427 .\n",
      "epoch  87  total number of batches is  20\n",
      "...completed  88  epochs of training. Current loss:  0.23411782215448856 .\n",
      "epoch  88  total number of batches is  20\n",
      "...completed  89  epochs of training. Current loss:  0.23312897845887204 .\n",
      "epoch  89  total number of batches is  20\n",
      "...completed  90  epochs of training. Current loss:  0.23216199811154023 .\n",
      "epoch  90  total number of batches is  20\n",
      "...completed  91  epochs of training. Current loss:  0.23121607721694568 .\n",
      "epoch  91  total number of batches is  20\n",
      "...completed  92  epochs of training. Current loss:  0.23029044973072152 .\n",
      "epoch  92  total number of batches is  20\n",
      "...completed  93  epochs of training. Current loss:  0.22938438541365985 .\n",
      "epoch  93  total number of batches is  20\n",
      "...completed  94  epochs of training. Current loss:  0.22849718790342757 .\n",
      "epoch  94  total number of batches is  20\n",
      "...completed  95  epochs of training. Current loss:  0.2276281928973444 .\n",
      "epoch  95  total number of batches is  20\n",
      "...completed  96  epochs of training. Current loss:  0.22677676643986516 .\n",
      "epoch  96  total number of batches is  20\n",
      "...completed  97  epochs of training. Current loss:  0.2259423033087202 .\n",
      "epoch  97  total number of batches is  20\n",
      "...completed  98  epochs of training. Current loss:  0.22512422549397978 .\n",
      "epoch  98  total number of batches is  20\n",
      "...completed  99  epochs of training. Current loss:  0.22432198076460816 .\n",
      "epoch  99  total number of batches is  20\n",
      "...completed  100  epochs of training. Current loss:  0.22353504131736804 .\n",
      "epoch  100  total number of batches is  20\n",
      "...completed  101  epochs of training. Current loss:  0.2227629025032186 .\n",
      "epoch  101  total number of batches is  20\n",
      "...completed  102  epochs of training. Current loss:  0.222005081626621 .\n",
      "epoch  102  total number of batches is  20\n",
      "...completed  103  epochs of training. Current loss:  0.22126111681342883 .\n",
      "epoch  103  total number of batches is  20\n",
      "...completed  104  epochs of training. Current loss:  0.22053056594328563 .\n",
      "epoch  104  total number of batches is  20\n",
      "...completed  105  epochs of training. Current loss:  0.21981300564269204 .\n",
      "epoch  105  total number of batches is  20\n",
      "...completed  106  epochs of training. Current loss:  0.21910803033512774 .\n",
      "epoch  106  total number of batches is  20\n",
      "...completed  107  epochs of training. Current loss:  0.21841525134482795 .\n",
      "epoch  107  total number of batches is  20\n",
      "...completed  108  epochs of training. Current loss:  0.2177342960510165 .\n",
      "epoch  108  total number of batches is  20\n",
      "...completed  109  epochs of training. Current loss:  0.21706480708958764 .\n",
      "epoch  109  total number of batches is  20\n",
      "...completed  110  epochs of training. Current loss:  0.21640644159941033 .\n",
      "epoch  110  total number of batches is  20\n",
      "...completed  111  epochs of training. Current loss:  0.21575887051059942 .\n",
      "epoch  111  total number of batches is  20\n",
      "...completed  112  epochs of training. Current loss:  0.21512177787225784 .\n",
      "epoch  112  total number of batches is  20\n",
      "...completed  113  epochs of training. Current loss:  0.2144948602173457 .\n",
      "epoch  113  total number of batches is  20\n",
      "...completed  114  epochs of training. Current loss:  0.21387782596247587 .\n",
      "epoch  114  total number of batches is  20\n",
      "...completed  115  epochs of training. Current loss:  0.21327039484056734 .\n",
      "epoch  115  total number of batches is  20\n",
      "...completed  116  epochs of training. Current loss:  0.2126722973644165 .\n",
      "epoch  116  total number of batches is  20\n",
      "...completed  117  epochs of training. Current loss:  0.212083274319363 .\n",
      "epoch  117  total number of batches is  20\n",
      "...completed  118  epochs of training. Current loss:  0.2115030762833398 .\n",
      "epoch  118  total number of batches is  20\n",
      "...completed  119  epochs of training. Current loss:  0.2109314631726989 .\n",
      "epoch  119  total number of batches is  20\n",
      "...completed  120  epochs of training. Current loss:  0.21036820381230756 .\n",
      "epoch  120  total number of batches is  20\n",
      "...completed  121  epochs of training. Current loss:  0.20981307552849549 .\n",
      "epoch  121  total number of batches is  20\n",
      "...completed  122  epochs of training. Current loss:  0.20926586376352554 .\n",
      "epoch  122  total number of batches is  20\n",
      "...completed  123  epochs of training. Current loss:  0.20872636171033895 .\n",
      "epoch  123  total number of batches is  20\n",
      "...completed  124  epochs of training. Current loss:  0.20819436996640214 .\n",
      "epoch  124  total number of batches is  20\n",
      "...completed  125  epochs of training. Current loss:  0.20766969620555556 .\n",
      "epoch  125  total number of batches is  20\n",
      "...completed  126  epochs of training. Current loss:  0.2071521548668291 .\n",
      "epoch  126  total number of batches is  20\n",
      "...completed  127  epochs of training. Current loss:  0.20664156685925403 .\n",
      "epoch  127  total number of batches is  20\n",
      "...completed  128  epochs of training. Current loss:  0.20613775928175931 .\n",
      "epoch  128  total number of batches is  20\n",
      "...completed  129  epochs of training. Current loss:  0.20564056515729523 .\n",
      "epoch  129  total number of batches is  20\n",
      "...completed  130  epochs of training. Current loss:  0.20514982318037872 .\n",
      "epoch  130  total number of batches is  20\n",
      "...completed  131  epochs of training. Current loss:  0.20466537747730415 .\n",
      "epoch  131  total number of batches is  20\n",
      "...completed  132  epochs of training. Current loss:  0.20418707737830982 .\n",
      "epoch  132  total number of batches is  20\n",
      "...completed  133  epochs of training. Current loss:  0.203714777201029 .\n",
      "epoch  133  total number of batches is  20\n",
      "...completed  134  epochs of training. Current loss:  0.2032483360446007 .\n",
      "epoch  134  total number of batches is  20\n",
      "...completed  135  epochs of training. Current loss:  0.2027876175938459 .\n",
      "epoch  135  total number of batches is  20\n",
      "...completed  136  epochs of training. Current loss:  0.2023324899329569 .\n",
      "epoch  136  total number of batches is  20\n",
      "...completed  137  epochs of training. Current loss:  0.20188282536817526 .\n",
      "epoch  137  total number of batches is  20\n",
      "...completed  138  epochs of training. Current loss:  0.20143850025896723 .\n",
      "epoch  138  total number of batches is  20\n",
      "...completed  139  epochs of training. Current loss:  0.20099939485723486 .\n",
      "epoch  139  total number of batches is  20\n",
      "...completed  140  epochs of training. Current loss:  0.20056539315412655 .\n",
      "epoch  140  total number of batches is  20\n",
      "...completed  141  epochs of training. Current loss:  0.20013638273403836 .\n",
      "epoch  141  total number of batches is  20\n",
      "...completed  142  epochs of training. Current loss:  0.19971225463541975 .\n",
      "epoch  142  total number of batches is  20\n",
      "...completed  143  epochs of training. Current loss:  0.19929290321802104 .\n",
      "epoch  143  total number of batches is  20\n",
      "...completed  144  epochs of training. Current loss:  0.19887822603624045 .\n",
      "epoch  144  total number of batches is  20\n",
      "...completed  145  epochs of training. Current loss:  0.19846812371824835 .\n",
      "epoch  145  total number of batches is  20\n",
      "...completed  146  epochs of training. Current loss:  0.19806249985058594 .\n",
      "epoch  146  total number of batches is  20\n",
      "...completed  147  epochs of training. Current loss:  0.1976612608679513 .\n",
      "epoch  147  total number of batches is  20\n",
      "...completed  148  epochs of training. Current loss:  0.19726431594790433 .\n",
      "epoch  148  total number of batches is  20\n",
      "...completed  149  epochs of training. Current loss:  0.1968715769102355 .\n",
      "epoch  149  total number of batches is  20\n",
      "...completed  150  epochs of training. Current loss:  0.1964829581207592 .\n",
      "epoch  150  total number of batches is  20\n",
      "...completed  151  epochs of training. Current loss:  0.19609837639930577 .\n",
      "epoch  151  total number of batches is  20\n",
      "...completed  152  epochs of training. Current loss:  0.1957177509316987 .\n",
      "epoch  152  total number of batches is  20\n",
      "...completed  153  epochs of training. Current loss:  0.19534100318551575 .\n",
      "epoch  153  total number of batches is  20\n",
      "...completed  154  epochs of training. Current loss:  0.19496805682944446 .\n",
      "epoch  154  total number of batches is  20\n",
      "...completed  155  epochs of training. Current loss:  0.19459883765605226 .\n",
      "epoch  155  total number of batches is  20\n",
      "...completed  156  epochs of training. Current loss:  0.19423327350780248 .\n",
      "epoch  156  total number of batches is  20\n",
      "...completed  157  epochs of training. Current loss:  0.19387129420615565 .\n",
      "epoch  157  total number of batches is  20\n",
      "...completed  158  epochs of training. Current loss:  0.19351283148360607 .\n",
      "epoch  158  total number of batches is  20\n",
      "...completed  159  epochs of training. Current loss:  0.1931578189185096 .\n",
      "epoch  159  total number of batches is  20\n",
      "...completed  160  epochs of training. Current loss:  0.19280619187256942 .\n",
      "epoch  160  total number of batches is  20\n",
      "...completed  161  epochs of training. Current loss:  0.19245788743085032 .\n",
      "epoch  161  total number of batches is  20\n",
      "...completed  162  epochs of training. Current loss:  0.1921128443442029 .\n",
      "epoch  162  total number of batches is  20\n",
      "...completed  163  epochs of training. Current loss:  0.19177100297398206 .\n",
      "epoch  163  total number of batches is  20\n",
      "...completed  164  epochs of training. Current loss:  0.191432305238953 .\n",
      "epoch  164  total number of batches is  20\n",
      "...completed  165  epochs of training. Current loss:  0.19109669456428124 .\n",
      "epoch  165  total number of batches is  20\n",
      "...completed  166  epochs of training. Current loss:  0.19076411583251218 .\n",
      "epoch  166  total number of batches is  20\n",
      "...completed  167  epochs of training. Current loss:  0.1904345153364459 .\n",
      "epoch  167  total number of batches is  20\n",
      "...completed  168  epochs of training. Current loss:  0.19010784073382317 .\n",
      "epoch  168  total number of batches is  20\n",
      "...completed  169  epochs of training. Current loss:  0.18978404100373847 .\n",
      "epoch  169  total number of batches is  20\n",
      "...completed  170  epochs of training. Current loss:  0.1894630664047035 .\n",
      "epoch  170  total number of batches is  20\n",
      "...completed  171  epochs of training. Current loss:  0.1891448684342871 .\n",
      "epoch  171  total number of batches is  20\n",
      "...completed  172  epochs of training. Current loss:  0.18882939979026078 .\n",
      "epoch  172  total number of batches is  20\n",
      "...completed  173  epochs of training. Current loss:  0.18851661433318515 .\n",
      "epoch  173  total number of batches is  20\n",
      "...completed  174  epochs of training. Current loss:  0.18820646705037256 .\n",
      "epoch  174  total number of batches is  20\n",
      "...completed  175  epochs of training. Current loss:  0.18789891402116785 .\n",
      "epoch  175  total number of batches is  20\n",
      "...completed  176  epochs of training. Current loss:  0.187593912383489 .\n",
      "epoch  176  total number of batches is  20\n",
      "...completed  177  epochs of training. Current loss:  0.1872914203015749 .\n",
      "epoch  177  total number of batches is  20\n",
      "...completed  178  epochs of training. Current loss:  0.18699139693488862 .\n",
      "epoch  178  total number of batches is  20\n",
      "...completed  179  epochs of training. Current loss:  0.1866938024081278 .\n",
      "epoch  179  total number of batches is  20\n",
      "...completed  180  epochs of training. Current loss:  0.18639859778229573 .\n",
      "epoch  180  total number of batches is  20\n",
      "...completed  181  epochs of training. Current loss:  0.1861057450267895 .\n",
      "epoch  181  total number of batches is  20\n",
      "...completed  182  epochs of training. Current loss:  0.18581520699246262 .\n",
      "epoch  182  total number of batches is  20\n",
      "...completed  183  epochs of training. Current loss:  0.18552694738562422 .\n",
      "epoch  183  total number of batches is  20\n",
      "...completed  184  epochs of training. Current loss:  0.18524093074293435 .\n",
      "epoch  184  total number of batches is  20\n",
      "...completed  185  epochs of training. Current loss:  0.18495712240716192 .\n",
      "epoch  185  total number of batches is  20\n",
      "...completed  186  epochs of training. Current loss:  0.1846754885037692 .\n",
      "epoch  186  total number of batches is  20\n",
      "...completed  187  epochs of training. Current loss:  0.18439599591829084 .\n",
      "epoch  187  total number of batches is  20\n",
      "...completed  188  epochs of training. Current loss:  0.18411861227447684 .\n",
      "epoch  188  total number of batches is  20\n",
      "...completed  189  epochs of training. Current loss:  0.18384330591316803 .\n",
      "epoch  189  total number of batches is  20\n",
      "...completed  190  epochs of training. Current loss:  0.1835700458718781 .\n",
      "epoch  190  total number of batches is  20\n",
      "...completed  191  epochs of training. Current loss:  0.183298801865053 .\n",
      "epoch  191  total number of batches is  20\n",
      "...completed  192  epochs of training. Current loss:  0.1830295442649832 .\n",
      "epoch  192  total number of batches is  20\n",
      "...completed  193  epochs of training. Current loss:  0.18276224408334343 .\n",
      "epoch  193  total number of batches is  20\n",
      "...completed  194  epochs of training. Current loss:  0.18249687295333725 .\n",
      "epoch  194  total number of batches is  20\n",
      "...completed  195  epochs of training. Current loss:  0.1822334031124227 .\n",
      "epoch  195  total number of batches is  20\n",
      "...completed  196  epochs of training. Current loss:  0.18197180738559873 .\n",
      "epoch  196  total number of batches is  20\n",
      "...completed  197  epochs of training. Current loss:  0.1817120591692309 .\n",
      "epoch  197  total number of batches is  20\n",
      "...completed  198  epochs of training. Current loss:  0.1814541324153976 .\n",
      "epoch  198  total number of batches is  20\n",
      "...completed  199  epochs of training. Current loss:  0.18119800161673702 .\n",
      "epoch  199  total number of batches is  20\n",
      "...completed  200  epochs of training. Current loss:  0.18094364179177735 .\n",
      "epoch  200  total number of batches is  20\n",
      "...completed  201  epochs of training. Current loss:  0.1806910284707329 .\n",
      "epoch  201  total number of batches is  20\n",
      "...completed  202  epochs of training. Current loss:  0.18044013768174963 .\n",
      "epoch  202  total number of batches is  20\n",
      "...completed  203  epochs of training. Current loss:  0.18019094593758392 .\n",
      "epoch  203  total number of batches is  20\n",
      "...completed  204  epochs of training. Current loss:  0.17994343022269974 .\n",
      "epoch  204  total number of batches is  20\n",
      "...completed  205  epochs of training. Current loss:  0.17969756798076933 .\n",
      "epoch  205  total number of batches is  20\n",
      "...completed  206  epochs of training. Current loss:  0.17945333710256375 .\n",
      "epoch  206  total number of batches is  20\n",
      "...completed  207  epochs of training. Current loss:  0.17921071591421925 .\n",
      "epoch  207  total number of batches is  20\n",
      "...completed  208  epochs of training. Current loss:  0.17896968316586712 .\n",
      "epoch  208  total number of batches is  20\n",
      "...completed  209  epochs of training. Current loss:  0.17873021802061465 .\n",
      "epoch  209  total number of batches is  20\n",
      "...completed  210  epochs of training. Current loss:  0.17849230004386435 .\n",
      "epoch  210  total number of batches is  20\n",
      "...completed  211  epochs of training. Current loss:  0.17825590919296153 .\n",
      "epoch  211  total number of batches is  20\n",
      "...completed  212  epochs of training. Current loss:  0.17802102580715723 .\n",
      "epoch  212  total number of batches is  20\n",
      "...completed  213  epochs of training. Current loss:  0.1777876305978782 .\n",
      "epoch  213  total number of batches is  20\n",
      "...completed  214  epochs of training. Current loss:  0.17755570463929174 .\n",
      "epoch  214  total number of batches is  20\n",
      "...completed  215  epochs of training. Current loss:  0.17732522935915612 .\n",
      "epoch  215  total number of batches is  20\n",
      "...completed  216  epochs of training. Current loss:  0.17709618652994807 .\n",
      "epoch  216  total number of batches is  20\n",
      "...completed  217  epochs of training. Current loss:  0.17686855826025735 .\n",
      "epoch  217  total number of batches is  20\n",
      "...completed  218  epochs of training. Current loss:  0.1766423269864387 .\n",
      "epoch  218  total number of batches is  20\n",
      "...completed  219  epochs of training. Current loss:  0.17641747546451536 .\n",
      "epoch  219  total number of batches is  20\n",
      "...completed  220  epochs of training. Current loss:  0.17619398676232306 .\n",
      "epoch  220  total number of batches is  20\n",
      "...completed  221  epochs of training. Current loss:  0.1759718442518886 .\n",
      "epoch  221  total number of batches is  20\n",
      "...completed  222  epochs of training. Current loss:  0.17575103160203437 .\n",
      "epoch  222  total number of batches is  20\n",
      "...completed  223  epochs of training. Current loss:  0.1755315327712023 .\n",
      "epoch  223  total number of batches is  20\n",
      "...completed  224  epochs of training. Current loss:  0.17531333200048893 .\n",
      "epoch  224  total number of batches is  20\n",
      "...completed  225  epochs of training. Current loss:  0.17509641380688648 .\n",
      "epoch  225  total number of batches is  20\n",
      "...completed  226  epochs of training. Current loss:  0.17488076297672164 .\n",
      "epoch  226  total number of batches is  20\n",
      "...completed  227  epochs of training. Current loss:  0.17466636455928675 .\n",
      "epoch  227  total number of batches is  20\n",
      "...completed  228  epochs of training. Current loss:  0.17445320386065682 .\n",
      "epoch  228  total number of batches is  20\n",
      "...completed  229  epochs of training. Current loss:  0.17424126643768675 .\n",
      "epoch  229  total number of batches is  20\n",
      "...completed  230  epochs of training. Current loss:  0.1740305380921825 .\n",
      "epoch  230  total number of batches is  20\n",
      "...completed  231  epochs of training. Current loss:  0.17382100486524085 .\n",
      "epoch  231  total number of batches is  20\n",
      "...completed  232  epochs of training. Current loss:  0.17361265303175302 .\n",
      "epoch  232  total number of batches is  20\n",
      "...completed  233  epochs of training. Current loss:  0.17340546909506577 .\n",
      "epoch  233  total number of batches is  20\n",
      "...completed  234  epochs of training. Current loss:  0.17319943978179586 .\n",
      "epoch  234  total number of batches is  20\n",
      "...completed  235  epochs of training. Current loss:  0.1729945520367928 .\n",
      "epoch  235  total number of batches is  20\n",
      "...completed  236  epochs of training. Current loss:  0.17279079301824496 .\n",
      "epoch  236  total number of batches is  20\n",
      "...completed  237  epochs of training. Current loss:  0.17258815009292483 .\n",
      "epoch  237  total number of batches is  20\n",
      "...completed  238  epochs of training. Current loss:  0.17238661083156875 .\n",
      "epoch  238  total number of batches is  20\n",
      "...completed  239  epochs of training. Current loss:  0.17218616300438694 .\n",
      "epoch  239  total number of batches is  20\n",
      "...completed  240  epochs of training. Current loss:  0.17198679457670007 .\n",
      "epoch  240  total number of batches is  20\n",
      "...completed  241  epochs of training. Current loss:  0.17178849370469743 .\n",
      "epoch  241  total number of batches is  20\n",
      "...completed  242  epochs of training. Current loss:  0.17159124873131415 .\n",
      "epoch  242  total number of batches is  20\n",
      "...completed  243  epochs of training. Current loss:  0.17139504818222281 .\n",
      "epoch  243  total number of batches is  20\n",
      "...completed  244  epochs of training. Current loss:  0.17119988076193587 .\n",
      "epoch  244  total number of batches is  20\n",
      "...completed  245  epochs of training. Current loss:  0.17100573535001595 .\n",
      "epoch  245  total number of batches is  20\n",
      "...completed  246  epochs of training. Current loss:  0.17081260099739032 .\n",
      "epoch  246  total number of batches is  20\n",
      "...completed  247  epochs of training. Current loss:  0.17062046692276597 .\n",
      "epoch  247  total number of batches is  20\n",
      "...completed  248  epochs of training. Current loss:  0.17042932250914267 .\n",
      "epoch  248  total number of batches is  20\n",
      "...completed  249  epochs of training. Current loss:  0.1702391573004205 .\n",
      "epoch  249  total number of batches is  20\n",
      "...completed  250  epochs of training. Current loss:  0.17004996099809938 .\n",
      "epoch  250  total number of batches is  20\n",
      "...completed  251  epochs of training. Current loss:  0.1698617234580671 .\n",
      "epoch  251  total number of batches is  20\n",
      "...completed  252  epochs of training. Current loss:  0.16967443468747348 .\n",
      "epoch  252  total number of batches is  20\n",
      "...completed  253  epochs of training. Current loss:  0.1694880848416881 .\n",
      "epoch  253  total number of batches is  20\n",
      "...completed  254  epochs of training. Current loss:  0.16930266422133847 .\n",
      "epoch  254  total number of batches is  20\n",
      "...completed  255  epochs of training. Current loss:  0.1691181632694266 .\n",
      "epoch  255  total number of batches is  20\n",
      "...completed  256  epochs of training. Current loss:  0.16893457256852137 .\n",
      "epoch  256  total number of batches is  20\n",
      "...completed  257  epochs of training. Current loss:  0.16875188283802425 .\n",
      "epoch  257  total number of batches is  20\n",
      "...completed  258  epochs of training. Current loss:  0.16857008493150624 .\n",
      "epoch  258  total number of batches is  20\n",
      "...completed  259  epochs of training. Current loss:  0.16838916983411367 .\n",
      "epoch  259  total number of batches is  20\n",
      "...completed  260  epochs of training. Current loss:  0.16820912866004092 .\n",
      "epoch  260  total number of batches is  20\n",
      "...completed  261  epochs of training. Current loss:  0.1680299526500678 .\n",
      "epoch  261  total number of batches is  20\n",
      "...completed  262  epochs of training. Current loss:  0.16785163316915946 .\n",
      "epoch  262  total number of batches is  20\n",
      "...completed  263  epochs of training. Current loss:  0.16767416170412763 .\n",
      "epoch  263  total number of batches is  20\n",
      "...completed  264  epochs of training. Current loss:  0.1674975298613501 .\n",
      "epoch  264  total number of batches is  20\n",
      "...completed  265  epochs of training. Current loss:  0.1673217293645478 .\n",
      "epoch  265  total number of batches is  20\n",
      "...completed  266  epochs of training. Current loss:  0.16714675205261711 .\n",
      "epoch  266  total number of batches is  20\n",
      "...completed  267  epochs of training. Current loss:  0.16697258987751554 .\n",
      "epoch  267  total number of batches is  20\n",
      "...completed  268  epochs of training. Current loss:  0.16679923490219994 .\n",
      "epoch  268  total number of batches is  20\n",
      "...completed  269  epochs of training. Current loss:  0.16662667929861472 .\n",
      "epoch  269  total number of batches is  20\n",
      "...completed  270  epochs of training. Current loss:  0.16645491534572895 .\n",
      "epoch  270  total number of batches is  20\n",
      "...completed  271  epochs of training. Current loss:  0.1662839354276214 .\n",
      "epoch  271  total number of batches is  20\n",
      "...completed  272  epochs of training. Current loss:  0.16611373203161064 .\n",
      "epoch  272  total number of batches is  20\n",
      "...completed  273  epochs of training. Current loss:  0.1659442977464308 .\n",
      "epoch  273  total number of batches is  20\n",
      "...completed  274  epochs of training. Current loss:  0.16577562526044948 .\n",
      "epoch  274  total number of batches is  20\n",
      "...completed  275  epochs of training. Current loss:  0.16560770735992839 .\n",
      "epoch  275  total number of batches is  20\n",
      "...completed  276  epochs of training. Current loss:  0.16544053692732447 .\n",
      "epoch  276  total number of batches is  20\n",
      "...completed  277  epochs of training. Current loss:  0.16527410693963032 .\n",
      "epoch  277  total number of batches is  20\n",
      "...completed  278  epochs of training. Current loss:  0.16510841046675304 .\n",
      "epoch  278  total number of batches is  20\n",
      "...completed  279  epochs of training. Current loss:  0.1649434406699303 .\n",
      "epoch  279  total number of batches is  20\n",
      "...completed  280  epochs of training. Current loss:  0.16477919080018227 .\n",
      "epoch  280  total number of batches is  20\n",
      "...completed  281  epochs of training. Current loss:  0.16461565419679888 .\n",
      "epoch  281  total number of batches is  20\n",
      "...completed  282  epochs of training. Current loss:  0.16445282428586072 .\n",
      "epoch  282  total number of batches is  20\n",
      "...completed  283  epochs of training. Current loss:  0.16429069457879325 .\n",
      "epoch  283  total number of batches is  20\n",
      "...completed  284  epochs of training. Current loss:  0.16412925867095277 .\n",
      "epoch  284  total number of batches is  20\n",
      "...completed  285  epochs of training. Current loss:  0.16396851024024364 .\n",
      "epoch  285  total number of batches is  20\n",
      "...completed  286  epochs of training. Current loss:  0.16380844304576575 .\n",
      "epoch  286  total number of batches is  20\n",
      "...completed  287  epochs of training. Current loss:  0.16364905092649135 .\n",
      "epoch  287  total number of batches is  20\n",
      "...completed  288  epochs of training. Current loss:  0.1634903277999701 .\n",
      "epoch  288  total number of batches is  20\n",
      "...completed  289  epochs of training. Current loss:  0.16333226766106235 .\n",
      "epoch  289  total number of batches is  20\n",
      "...completed  290  epochs of training. Current loss:  0.16317486458069883 .\n",
      "epoch  290  total number of batches is  20\n",
      "...completed  291  epochs of training. Current loss:  0.16301811270466687 .\n",
      "epoch  291  total number of batches is  20\n",
      "...completed  292  epochs of training. Current loss:  0.16286200625242184 .\n",
      "epoch  292  total number of batches is  20\n",
      "...completed  293  epochs of training. Current loss:  0.1627065395159236 .\n",
      "epoch  293  total number of batches is  20\n",
      "...completed  294  epochs of training. Current loss:  0.16255170685849651 .\n",
      "epoch  294  total number of batches is  20\n",
      "...completed  295  epochs of training. Current loss:  0.1623975027137135 .\n",
      "epoch  295  total number of batches is  20\n",
      "...completed  296  epochs of training. Current loss:  0.1622439215843021 .\n",
      "epoch  296  total number of batches is  20\n",
      "...completed  297  epochs of training. Current loss:  0.16209095804107354 .\n",
      "epoch  297  total number of batches is  20\n",
      "...completed  298  epochs of training. Current loss:  0.1619386067218725 .\n",
      "epoch  298  total number of batches is  20\n",
      "...completed  299  epochs of training. Current loss:  0.16178686233054845 .\n",
      "epoch  299  total number of batches is  20\n",
      "...completed  300  epochs of training. Current loss:  0.16163571963594706 .\n",
      "epoch  300  total number of batches is  20\n",
      "...completed  301  epochs of training. Current loss:  0.1614851734709217 .\n",
      "epoch  301  total number of batches is  20\n",
      "...completed  302  epochs of training. Current loss:  0.16133521873136403 .\n",
      "epoch  302  total number of batches is  20\n",
      "...completed  303  epochs of training. Current loss:  0.16118585037525368 .\n",
      "epoch  303  total number of batches is  20\n",
      "...completed  304  epochs of training. Current loss:  0.16103706342172644 .\n",
      "epoch  304  total number of batches is  20\n",
      "...completed  305  epochs of training. Current loss:  0.1608888529501595 .\n",
      "epoch  305  total number of batches is  20\n",
      "...completed  306  epochs of training. Current loss:  0.16074121409927528 .\n",
      "epoch  306  total number of batches is  20\n",
      "...completed  307  epochs of training. Current loss:  0.1605941420662614 .\n",
      "epoch  307  total number of batches is  20\n",
      "...completed  308  epochs of training. Current loss:  0.1604476321059076 .\n",
      "epoch  308  total number of batches is  20\n",
      "...completed  309  epochs of training. Current loss:  0.16030167952975855 .\n",
      "epoch  309  total number of batches is  20\n",
      "...completed  310  epochs of training. Current loss:  0.16015627970528273 .\n",
      "epoch  310  total number of batches is  20\n",
      "...completed  311  epochs of training. Current loss:  0.1600114280550563 .\n",
      "epoch  311  total number of batches is  20\n",
      "...completed  312  epochs of training. Current loss:  0.15986712005596235 .\n",
      "epoch  312  total number of batches is  20\n",
      "...completed  313  epochs of training. Current loss:  0.15972335123840434 .\n",
      "epoch  313  total number of batches is  20\n",
      "...completed  314  epochs of training. Current loss:  0.1595801171855342 .\n",
      "epoch  314  total number of batches is  20\n",
      "...completed  315  epochs of training. Current loss:  0.159437413532494 .\n",
      "epoch  315  total number of batches is  20\n",
      "...completed  316  epochs of training. Current loss:  0.15929523596567163 .\n",
      "epoch  316  total number of batches is  20\n",
      "...completed  317  epochs of training. Current loss:  0.1591535802219693 .\n",
      "epoch  317  total number of batches is  20\n",
      "...completed  318  epochs of training. Current loss:  0.15901244208808538 .\n",
      "epoch  318  total number of batches is  20\n",
      "...completed  319  epochs of training. Current loss:  0.15887181739980857 .\n",
      "epoch  319  total number of batches is  20\n",
      "...completed  320  epochs of training. Current loss:  0.15873170204132495 .\n",
      "epoch  320  total number of batches is  20\n",
      "...completed  321  epochs of training. Current loss:  0.15859209194453638 .\n",
      "epoch  321  total number of batches is  20\n",
      "...completed  322  epochs of training. Current loss:  0.15845298308839145 .\n",
      "epoch  322  total number of batches is  20\n",
      "...completed  323  epochs of training. Current loss:  0.1583143714982275 .\n",
      "epoch  323  total number of batches is  20\n",
      "...completed  324  epochs of training. Current loss:  0.15817625324512416 .\n",
      "epoch  324  total number of batches is  20\n",
      "...completed  325  epochs of training. Current loss:  0.15803862444526792 .\n",
      "epoch  325  total number of batches is  20\n",
      "...completed  326  epochs of training. Current loss:  0.15790148125932732 .\n",
      "epoch  326  total number of batches is  20\n",
      "...completed  327  epochs of training. Current loss:  0.15776481989183894 .\n",
      "epoch  327  total number of batches is  20\n",
      "...completed  328  epochs of training. Current loss:  0.1576286365906035 .\n",
      "epoch  328  total number of batches is  20\n",
      "...completed  329  epochs of training. Current loss:  0.15749292764609243 .\n",
      "epoch  329  total number of batches is  20\n",
      "...completed  330  epochs of training. Current loss:  0.15735768939086375 .\n",
      "epoch  330  total number of batches is  20\n",
      "...completed  331  epochs of training. Current loss:  0.15722291819898837 .\n",
      "epoch  331  total number of batches is  20\n",
      "...completed  332  epochs of training. Current loss:  0.15708861048548522 .\n",
      "epoch  332  total number of batches is  20\n",
      "...completed  333  epochs of training. Current loss:  0.15695476270576605 .\n",
      "epoch  333  total number of batches is  20\n",
      "...completed  334  epochs of training. Current loss:  0.15682137135508914 .\n",
      "epoch  334  total number of batches is  20\n",
      "...completed  335  epochs of training. Current loss:  0.15668843296802223 .\n",
      "epoch  335  total number of batches is  20\n",
      "...completed  336  epochs of training. Current loss:  0.15655594411791354 .\n",
      "epoch  336  total number of batches is  20\n",
      "...completed  337  epochs of training. Current loss:  0.15642390141637208 .\n",
      "epoch  337  total number of batches is  20\n",
      "...completed  338  epochs of training. Current loss:  0.15629230151275586 .\n",
      "epoch  338  total number of batches is  20\n",
      "...completed  339  epochs of training. Current loss:  0.1561611410936684 .\n",
      "epoch  339  total number of batches is  20\n",
      "...completed  340  epochs of training. Current loss:  0.15603041688246347 .\n",
      "epoch  340  total number of batches is  20\n",
      "...completed  341  epochs of training. Current loss:  0.15590012563875724 .\n",
      "epoch  341  total number of batches is  20\n",
      "...completed  342  epochs of training. Current loss:  0.15577026415794895 .\n",
      "epoch  342  total number of batches is  20\n",
      "...completed  343  epochs of training. Current loss:  0.15564082927074843 .\n",
      "epoch  343  total number of batches is  20\n",
      "...completed  344  epochs of training. Current loss:  0.15551181784271123 .\n",
      "epoch  344  total number of batches is  20\n",
      "...completed  345  epochs of training. Current loss:  0.15538322677378144 .\n",
      "epoch  345  total number of batches is  20\n",
      "...completed  346  epochs of training. Current loss:  0.15525505299784115 .\n",
      "epoch  346  total number of batches is  20\n",
      "...completed  347  epochs of training. Current loss:  0.15512729348226714 .\n",
      "epoch  347  total number of batches is  20\n",
      "...completed  348  epochs of training. Current loss:  0.1549999452274945 .\n",
      "epoch  348  total number of batches is  20\n",
      "...completed  349  epochs of training. Current loss:  0.15487300526658695 .\n",
      "epoch  349  total number of batches is  20\n",
      "...completed  350  epochs of training. Current loss:  0.15474647066481378 .\n",
      "epoch  350  total number of batches is  20\n",
      "...completed  351  epochs of training. Current loss:  0.15462033851923335 .\n",
      "epoch  351  total number of batches is  20\n",
      "...completed  352  epochs of training. Current loss:  0.15449460595828277 .\n",
      "epoch  352  total number of batches is  20\n",
      "...completed  353  epochs of training. Current loss:  0.15436927014137428 .\n",
      "epoch  353  total number of batches is  20\n",
      "...completed  354  epochs of training. Current loss:  0.15424432825849713 .\n",
      "epoch  354  total number of batches is  20\n",
      "...completed  355  epochs of training. Current loss:  0.15411977752982636 .\n",
      "epoch  355  total number of batches is  20\n",
      "...completed  356  epochs of training. Current loss:  0.15399561520533664 .\n",
      "epoch  356  total number of batches is  20\n",
      "...completed  357  epochs of training. Current loss:  0.15387183856442252 .\n",
      "epoch  357  total number of batches is  20\n",
      "...completed  358  epochs of training. Current loss:  0.1537484449155241 .\n",
      "epoch  358  total number of batches is  20\n",
      "...completed  359  epochs of training. Current loss:  0.1536254315957586 .\n",
      "epoch  359  total number of batches is  20\n",
      "...completed  360  epochs of training. Current loss:  0.15350279597055708 .\n",
      "epoch  360  total number of batches is  20\n",
      "...completed  361  epochs of training. Current loss:  0.15338053543330704 .\n",
      "epoch  361  total number of batches is  20\n",
      "...completed  362  epochs of training. Current loss:  0.15325864740499984 .\n",
      "epoch  362  total number of batches is  20\n",
      "...completed  363  epochs of training. Current loss:  0.15313712933388368 .\n",
      "epoch  363  total number of batches is  20\n",
      "...completed  364  epochs of training. Current loss:  0.15301597869512196 .\n",
      "epoch  364  total number of batches is  20\n",
      "...completed  365  epochs of training. Current loss:  0.1528951929904558 .\n",
      "epoch  365  total number of batches is  20\n",
      "...completed  366  epochs of training. Current loss:  0.15277476974787277 .\n",
      "epoch  366  total number of batches is  20\n",
      "...completed  367  epochs of training. Current loss:  0.1526547065212795 .\n",
      "epoch  367  total number of batches is  20\n",
      "...completed  368  epochs of training. Current loss:  0.1525350008901792 .\n",
      "epoch  368  total number of batches is  20\n",
      "...completed  369  epochs of training. Current loss:  0.15241565045935493 .\n",
      "epoch  369  total number of batches is  20\n",
      "...completed  370  epochs of training. Current loss:  0.152296652858556 .\n",
      "epoch  370  total number of batches is  20\n",
      "...completed  371  epochs of training. Current loss:  0.15217800574218984 .\n",
      "epoch  371  total number of batches is  20\n",
      "...completed  372  epochs of training. Current loss:  0.15205970678901845 .\n",
      "epoch  372  total number of batches is  20\n",
      "...completed  373  epochs of training. Current loss:  0.15194175370185867 .\n",
      "epoch  373  total number of batches is  20\n",
      "...completed  374  epochs of training. Current loss:  0.15182414420728726 .\n",
      "epoch  374  total number of batches is  20\n",
      "...completed  375  epochs of training. Current loss:  0.15170687605534997 .\n",
      "epoch  375  total number of batches is  20\n",
      "...completed  376  epochs of training. Current loss:  0.1515899470192751 .\n",
      "epoch  376  total number of batches is  20\n",
      "...completed  377  epochs of training. Current loss:  0.15147335489519084 .\n",
      "epoch  377  total number of batches is  20\n",
      "...completed  378  epochs of training. Current loss:  0.15135709750184692 .\n",
      "epoch  378  total number of batches is  20\n",
      "...completed  379  epochs of training. Current loss:  0.15124117268034 .\n",
      "epoch  379  total number of batches is  20\n",
      "...completed  380  epochs of training. Current loss:  0.15112557829384352 .\n",
      "epoch  380  total number of batches is  20\n",
      "...completed  381  epochs of training. Current loss:  0.15101031222734065 .\n",
      "epoch  381  total number of batches is  20\n",
      "...completed  382  epochs of training. Current loss:  0.15089537238736161 .\n",
      "epoch  382  total number of batches is  20\n",
      "...completed  383  epochs of training. Current loss:  0.1507807567017245 .\n",
      "epoch  383  total number of batches is  20\n",
      "...completed  384  epochs of training. Current loss:  0.15066646311927998 .\n",
      "epoch  384  total number of batches is  20\n",
      "...completed  385  epochs of training. Current loss:  0.15055248960965925 .\n",
      "epoch  385  total number of batches is  20\n",
      "...completed  386  epochs of training. Current loss:  0.1504388341630259 .\n",
      "epoch  386  total number of batches is  20\n",
      "...completed  387  epochs of training. Current loss:  0.15032549478983126 .\n",
      "epoch  387  total number of batches is  20\n",
      "...completed  388  epochs of training. Current loss:  0.15021246952057277 .\n",
      "epoch  388  total number of batches is  20\n",
      "...completed  389  epochs of training. Current loss:  0.1500997564055565 .\n",
      "epoch  389  total number of batches is  20\n",
      "...completed  390  epochs of training. Current loss:  0.14998735351466222 .\n",
      "epoch  390  total number of batches is  20\n",
      "...completed  391  epochs of training. Current loss:  0.14987525893711226 .\n",
      "epoch  391  total number of batches is  20\n",
      "...completed  392  epochs of training. Current loss:  0.14976347078124344 .\n",
      "epoch  392  total number of batches is  20\n",
      "...completed  393  epochs of training. Current loss:  0.14965198717428227 .\n",
      "epoch  393  total number of batches is  20\n",
      "...completed  394  epochs of training. Current loss:  0.14954080626212304 .\n",
      "epoch  394  total number of batches is  20\n",
      "...completed  395  epochs of training. Current loss:  0.1494299262091095 .\n",
      "epoch  395  total number of batches is  20\n",
      "...completed  396  epochs of training. Current loss:  0.14931934519781892 .\n",
      "epoch  396  total number of batches is  20\n",
      "...completed  397  epochs of training. Current loss:  0.14920906142884968 .\n",
      "epoch  397  total number of batches is  20\n",
      "...completed  398  epochs of training. Current loss:  0.14909907312061152 .\n",
      "epoch  398  total number of batches is  20\n",
      "...completed  399  epochs of training. Current loss:  0.14898937850911884 .\n",
      "epoch  399  total number of batches is  20\n",
      "...completed  400  epochs of training. Current loss:  0.14887997584778678 .\n",
      "epoch  400  total number of batches is  20\n",
      "...completed  401  epochs of training. Current loss:  0.14877086340723017 .\n",
      "epoch  401  total number of batches is  20\n",
      "...completed  402  epochs of training. Current loss:  0.14866203947506487 .\n",
      "epoch  402  total number of batches is  20\n",
      "...completed  403  epochs of training. Current loss:  0.14855350235571274 .\n",
      "epoch  403  total number of batches is  20\n",
      "...completed  404  epochs of training. Current loss:  0.14844525037020828 .\n",
      "epoch  404  total number of batches is  20\n",
      "...completed  405  epochs of training. Current loss:  0.14833728185600847 .\n",
      "epoch  405  total number of batches is  20\n",
      "...completed  406  epochs of training. Current loss:  0.14822959516680526 .\n",
      "epoch  406  total number of batches is  20\n",
      "...completed  407  epochs of training. Current loss:  0.14812218867234017 .\n",
      "epoch  407  total number of batches is  20\n",
      "...completed  408  epochs of training. Current loss:  0.14801506075822193 .\n",
      "epoch  408  total number of batches is  20\n",
      "...completed  409  epochs of training. Current loss:  0.14790820982574632 .\n",
      "epoch  409  total number of batches is  20\n",
      "...completed  410  epochs of training. Current loss:  0.1478016342917185 .\n",
      "epoch  410  total number of batches is  20\n",
      "...completed  411  epochs of training. Current loss:  0.14769533258827788 .\n",
      "epoch  411  total number of batches is  20\n",
      "...completed  412  epochs of training. Current loss:  0.14758930316272514 .\n",
      "epoch  412  total number of batches is  20\n",
      "...completed  413  epochs of training. Current loss:  0.14748354447735168 .\n",
      "epoch  413  total number of batches is  20\n",
      "...completed  414  epochs of training. Current loss:  0.14737805500927156 .\n",
      "epoch  414  total number of batches is  20\n",
      "...completed  415  epochs of training. Current loss:  0.14727283325025553 .\n",
      "epoch  415  total number of batches is  20\n",
      "...completed  416  epochs of training. Current loss:  0.14716787770656714 .\n",
      "epoch  416  total number of batches is  20\n",
      "...completed  417  epochs of training. Current loss:  0.1470631868988016 .\n",
      "epoch  417  total number of batches is  20\n",
      "...completed  418  epochs of training. Current loss:  0.14695875936172606 .\n",
      "epoch  418  total number of batches is  20\n",
      "...completed  419  epochs of training. Current loss:  0.14685459364412265 .\n",
      "epoch  419  total number of batches is  20\n",
      "...completed  420  epochs of training. Current loss:  0.14675068830863336 .\n",
      "epoch  420  total number of batches is  20\n",
      "...completed  421  epochs of training. Current loss:  0.14664704193160702 .\n",
      "epoch  421  total number of batches is  20\n",
      "...completed  422  epochs of training. Current loss:  0.1465436531029484 .\n",
      "epoch  422  total number of batches is  20\n",
      "...completed  423  epochs of training. Current loss:  0.14644052042596906 .\n",
      "epoch  423  total number of batches is  20\n",
      "...completed  424  epochs of training. Current loss:  0.14633764251724063 .\n",
      "epoch  424  total number of batches is  20\n",
      "...completed  425  epochs of training. Current loss:  0.14623501800644967 .\n",
      "epoch  425  total number of batches is  20\n",
      "...completed  426  epochs of training. Current loss:  0.1461326455362547 .\n",
      "epoch  426  total number of batches is  20\n",
      "...completed  427  epochs of training. Current loss:  0.14603052376214484 .\n",
      "epoch  427  total number of batches is  20\n",
      "...completed  428  epochs of training. Current loss:  0.14592865135230068 .\n",
      "epoch  428  total number of batches is  20\n",
      "...completed  429  epochs of training. Current loss:  0.14582702698745664 .\n",
      "epoch  429  total number of batches is  20\n",
      "...completed  430  epochs of training. Current loss:  0.14572564936076557 .\n",
      "epoch  430  total number of batches is  20\n",
      "...completed  431  epochs of training. Current loss:  0.1456245171776646 .\n",
      "epoch  431  total number of batches is  20\n",
      "...completed  432  epochs of training. Current loss:  0.14552362915574316 .\n",
      "epoch  432  total number of batches is  20\n",
      "...completed  433  epochs of training. Current loss:  0.14542298402461268 .\n",
      "epoch  433  total number of batches is  20\n",
      "...completed  434  epochs of training. Current loss:  0.14532258052577793 .\n",
      "epoch  434  total number of batches is  20\n",
      "...completed  435  epochs of training. Current loss:  0.14522241741250996 .\n",
      "epoch  435  total number of batches is  20\n",
      "...completed  436  epochs of training. Current loss:  0.14512249344972078 .\n",
      "epoch  436  total number of batches is  20\n",
      "...completed  437  epochs of training. Current loss:  0.1450228074138399 .\n",
      "epoch  437  total number of batches is  20\n",
      "...completed  438  epochs of training. Current loss:  0.1449233580926919 .\n",
      "epoch  438  total number of batches is  20\n",
      "...completed  439  epochs of training. Current loss:  0.1448241442853765 .\n",
      "epoch  439  total number of batches is  20\n",
      "...completed  440  epochs of training. Current loss:  0.1447251648021491 .\n",
      "epoch  440  total number of batches is  20\n",
      "...completed  441  epochs of training. Current loss:  0.14462641846430385 .\n",
      "epoch  441  total number of batches is  20\n",
      "...completed  442  epochs of training. Current loss:  0.14452790410405755 .\n",
      "epoch  442  total number of batches is  20\n",
      "...completed  443  epochs of training. Current loss:  0.1444296205644354 .\n",
      "epoch  443  total number of batches is  20\n",
      "...completed  444  epochs of training. Current loss:  0.1443315666991582 .\n",
      "epoch  444  total number of batches is  20\n",
      "...completed  445  epochs of training. Current loss:  0.14423374137253092 .\n",
      "epoch  445  total number of batches is  20\n",
      "...completed  446  epochs of training. Current loss:  0.14413614345933254 .\n",
      "epoch  446  total number of batches is  20\n",
      "...completed  447  epochs of training. Current loss:  0.14403877184470779 .\n",
      "epoch  447  total number of batches is  20\n",
      "...completed  448  epochs of training. Current loss:  0.14394162542405975 .\n",
      "epoch  448  total number of batches is  20\n",
      "...completed  449  epochs of training. Current loss:  0.14384470310294412 .\n",
      "epoch  449  total number of batches is  20\n",
      "...completed  450  epochs of training. Current loss:  0.1437480037969648 .\n",
      "epoch  450  total number of batches is  20\n",
      "...completed  451  epochs of training. Current loss:  0.1436515264316707 .\n",
      "epoch  451  total number of batches is  20\n",
      "...completed  452  epochs of training. Current loss:  0.14355526994245393 .\n",
      "epoch  452  total number of batches is  20\n",
      "...completed  453  epochs of training. Current loss:  0.14345923327444934 .\n",
      "epoch  453  total number of batches is  20\n",
      "...completed  454  epochs of training. Current loss:  0.1433634153824352 .\n",
      "epoch  454  total number of batches is  20\n",
      "...completed  455  epochs of training. Current loss:  0.14326781523073526 .\n",
      "epoch  455  total number of batches is  20\n",
      "...completed  456  epochs of training. Current loss:  0.1431724317931219 .\n",
      "epoch  456  total number of batches is  20\n",
      "...completed  457  epochs of training. Current loss:  0.1430772640527206 .\n",
      "epoch  457  total number of batches is  20\n",
      "...completed  458  epochs of training. Current loss:  0.14298231100191566 .\n",
      "epoch  458  total number of batches is  20\n",
      "...completed  459  epochs of training. Current loss:  0.142887571642257 .\n",
      "epoch  459  total number of batches is  20\n",
      "...completed  460  epochs of training. Current loss:  0.14279304498436793 .\n",
      "epoch  460  total number of batches is  20\n",
      "...completed  461  epochs of training. Current loss:  0.14269873004785458 .\n",
      "epoch  461  total number of batches is  20\n",
      "...completed  462  epochs of training. Current loss:  0.142604625861216 .\n",
      "epoch  462  total number of batches is  20\n",
      "...completed  463  epochs of training. Current loss:  0.14251073146175547 .\n",
      "epoch  463  total number of batches is  20\n",
      "...completed  464  epochs of training. Current loss:  0.1424170458954931 .\n",
      "epoch  464  total number of batches is  20\n",
      "...completed  465  epochs of training. Current loss:  0.1423235682170791 .\n",
      "epoch  465  total number of batches is  20\n",
      "...completed  466  epochs of training. Current loss:  0.1422302974897086 .\n",
      "epoch  466  total number of batches is  20\n",
      "...completed  467  epochs of training. Current loss:  0.14213723278503723 .\n",
      "epoch  467  total number of batches is  20\n",
      "...completed  468  epochs of training. Current loss:  0.14204437318309754 .\n",
      "epoch  468  total number of batches is  20\n",
      "...completed  469  epochs of training. Current loss:  0.14195171777221705 .\n",
      "epoch  469  total number of batches is  20\n",
      "...completed  470  epochs of training. Current loss:  0.14185926564893644 .\n",
      "epoch  470  total number of batches is  20\n",
      "...completed  471  epochs of training. Current loss:  0.14176701591792956 .\n",
      "epoch  471  total number of batches is  20\n",
      "...completed  472  epochs of training. Current loss:  0.1416749676919238 .\n",
      "epoch  472  total number of batches is  20\n",
      "...completed  473  epochs of training. Current loss:  0.14158312009162158 .\n",
      "epoch  473  total number of batches is  20\n",
      "...completed  474  epochs of training. Current loss:  0.14149147224562295 .\n",
      "epoch  474  total number of batches is  20\n",
      "...completed  475  epochs of training. Current loss:  0.141400023290349 .\n",
      "epoch  475  total number of batches is  20\n",
      "...completed  476  epochs of training. Current loss:  0.141308772369966 .\n",
      "epoch  476  total number of batches is  20\n",
      "...completed  477  epochs of training. Current loss:  0.14121771863631044 .\n",
      "epoch  477  total number of batches is  20\n",
      "...completed  478  epochs of training. Current loss:  0.14112686124881552 .\n",
      "epoch  478  total number of batches is  20\n",
      "...completed  479  epochs of training. Current loss:  0.14103619937443748 .\n",
      "epoch  479  total number of batches is  20\n",
      "...completed  480  epochs of training. Current loss:  0.14094573218758374 .\n",
      "epoch  480  total number of batches is  20\n",
      "...completed  481  epochs of training. Current loss:  0.1408554588700414 .\n",
      "epoch  481  total number of batches is  20\n",
      "...completed  482  epochs of training. Current loss:  0.14076537861090635 .\n",
      "epoch  482  total number of batches is  20\n",
      "...completed  483  epochs of training. Current loss:  0.1406754906065138 .\n",
      "epoch  483  total number of batches is  20\n",
      "...completed  484  epochs of training. Current loss:  0.14058579406036897 .\n",
      "epoch  484  total number of batches is  20\n",
      "...completed  485  epochs of training. Current loss:  0.1404962881830791 .\n",
      "epoch  485  total number of batches is  20\n",
      "...completed  486  epochs of training. Current loss:  0.14040697219228568 .\n",
      "epoch  486  total number of batches is  20\n",
      "...completed  487  epochs of training. Current loss:  0.14031784531259775 .\n",
      "epoch  487  total number of batches is  20\n",
      "...completed  488  epochs of training. Current loss:  0.14022890677552613 .\n",
      "epoch  488  total number of batches is  20\n",
      "...completed  489  epochs of training. Current loss:  0.1401401558194178 .\n",
      "epoch  489  total number of batches is  20\n",
      "...completed  490  epochs of training. Current loss:  0.14005159168939146 .\n",
      "epoch  490  total number of batches is  20\n",
      "...completed  491  epochs of training. Current loss:  0.13996321363727368 .\n",
      "epoch  491  total number of batches is  20\n",
      "...completed  492  epochs of training. Current loss:  0.13987502092153556 .\n",
      "epoch  492  total number of batches is  20\n",
      "...completed  493  epochs of training. Current loss:  0.13978701280723033 .\n",
      "epoch  493  total number of batches is  20\n",
      "...completed  494  epochs of training. Current loss:  0.13969918856593133 .\n",
      "epoch  494  total number of batches is  20\n",
      "...completed  495  epochs of training. Current loss:  0.13961154747567087 .\n",
      "epoch  495  total number of batches is  20\n",
      "...completed  496  epochs of training. Current loss:  0.13952408882087963 .\n",
      "epoch  496  total number of batches is  20\n",
      "...completed  497  epochs of training. Current loss:  0.1394368118923265 .\n",
      "epoch  497  total number of batches is  20\n",
      "...completed  498  epochs of training. Current loss:  0.13934971598705956 .\n",
      "epoch  498  total number of batches is  20\n",
      "...completed  499  epochs of training. Current loss:  0.139262800408347 .\n",
      "epoch  499  total number of batches is  20\n",
      "...completed  500  epochs of training. Current loss:  0.13917606446561895 .\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    " # The part for perturbation method is commented out since this method is not very relevant for the research objective. It works, just need to uncomment\n",
    "\n",
    "# numupdates = numepochs * int(np.floor(x_train_corpus.shape[1]/batchsize))\n",
    "numupdates = numepochs * int(cnp.floor(train_images.shape[1]/batchsize))\n",
    "\n",
    "print(\"total iteration to be run per training: \", numupdates)\n",
    "\n",
    "# initialize the loss and accuracy holders\n",
    "# losses_perturb_img  = cnp.zeros((numupdates,1))\n",
    "losses_backprop_img = cnp.zeros((numupdates,1))\n",
    "losses_feedback_img = cnp.zeros((numupdates,1))\n",
    "\n",
    "accuracy_perturb_img  = cnp.zeros((numepochs,1))\n",
    "accuracy_backprop_img = cnp.zeros((numepochs,1))\n",
    "accuracy_feedback_img = cnp.zeros((numepochs,1))\n",
    "\n",
    "\n",
    "# # create a network and train it using weight perturbation\n",
    "# netperturb = MLP(train_images.shape[0], train_labels.shape[0], numhidden,batchsize,learnrate,alpha=initweight,algorithm='perturb',sigma=10)\n",
    "# (losses_perturb_img[:,0],accuracy_perturb_img[:,0]) = \\\n",
    "# netperturb.train(train_images,train_labels,numepochs,test_images[:,:],test_labels[:,:],report=True, report_rate=1)\n",
    "\n",
    "# create a network and train it using backprop\n",
    "netbackprop = MLP(train_images.shape[0], train_labels.shape[0], numhidden,batchsize,learnrate,alpha=initweight,algorithm='backprop')\n",
    "(losses_backprop_img[:,0],accuracy_backprop_img[:,0]) = \\\n",
    "netbackprop.train(train_images,train_labels,numepochs,test_images[:,:],test_labels[:,:],report=True, report_rate=1)\n",
    "\n",
    "# # create a network and train it using feedback alignment\n",
    "netfeedback = MLP(train_images.shape[0], train_labels.shape[0], numhidden,batchsize,learnrate,alpha=initweight,algorithm='feedback')\n",
    "(losses_feedback_img[:,0],accuracy_feedback_img[:,0]) = \\\n",
    "netfeedback.train(train_images,train_labels,numepochs,test_images[:,:],test_labels[:,:],report=True, report_rate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHHCAYAAABZbpmkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAB5BElEQVR4nO3dd1hT59sH8G8IIYQV9lJBxL03orXu4qxaa9XaOltr3aP1VzsctdW2dlg7rFpXraPVV60dWhW3RVQUt9aB4kJUZG/yvH/EnBICCJIB8fu5rlzAc55zcudwQm6edWRCCAEiIiIiK2Vj6QCIiIiITInJDhEREVk1JjtERERk1ZjsEBERkVVjskNERERWjckOERERWTUmO0RERGTVmOwQERGRVWOyQ0RERFaNyQ6RCezduxcymQx79+61dChmt3LlSshkMly7ds3SoejRaDSoX78+Pv74Y0uHYnVmzZoFmUyG+/fvP7Zu1apVMWzYsMfWK811VNJjVgRP89+Ogs6dOwdbW1ucOXOmzMdislMImUxWoocxLsb09HTMmjWLFzYRgLVr12LBggUmOfa6detw48YNjBs3ziTHL0pWVhb+97//wd/fHyqVCiEhIdi5c2eJ97916xZeeukluLq6wsXFBb1798bVq1cLrbts2TLUqVMH9vb2qFGjBr755huDOhcvXsTkyZPRunVr2Nvbl8vElPSZ8n3xOEeOHMGYMWPQrFkzKBQKyGSyYuuX5BoESnZd161bFz169MCMGTPK/Dpsy3wEK7R69Wq9n3/66Sfs3LnToLxOnTplfq709HTMnj0bANC+ffsyH4/Kh2effRYZGRmws7OzdCgVytq1a3HmzBlMmjTJ6MeeP38+Bg4cCLVabfRjF2fYsGHYuHEjJk2ahBo1amDlypXo3r079uzZg2eeeabYfVNTU9GhQwckJSXh3XffhUKhwFdffYV27dohOjoaHh4eUt3Fixdj9OjR6NevH6ZMmYIDBw5gwoQJSE9Px//+9z+pXkREBBYuXIi6deuiTp06iI6ONtVLL9TFixdhY8P/s4tS2N8OU74vHuevv/7Cjz/+iIYNG6JatWr4999/i6xb0muwNNf16NGj0b17d1y5cgXBwcFP/kIEPdbYsWOFqU7VvXv3BAAxc+ZMkxzf0lJTUy0dApnZihUrBAARExNT6n179OghAgMDjR7T8ePHBQCxa9cuox+7OJGRkQKAmD9/vlSWkZEhgoODRWho6GP3//TTTwUAceTIEans/PnzQi6Xi+nTp0tl6enpwsPDQ/To0UNv/8GDBwtHR0eRkJAglT148EAkJycLIYSYP3/+E/+u8ps5c6YAIO7du1em4+RXmusoMDBQDB061GjPXd6Y6n1REnFxcSI9PV0IUfxnYWmuwZJe10IIkZ2dLdzc3MQHH3xQptfB9PoJaTQaLFiwAPXq1YO9vT18fHzwxhtv4OHDh3r1jh07hrCwMHh6ekKlUiEoKAgjRowAAFy7dg1eXl4AgNmzZ0vdY7NmzSryeRMSEvDWW2+hQYMGcHJygouLC7p164aTJ08a1M3MzMSsWbNQs2ZN2Nvbw8/PDy+88AKuXLmi9zq+/vprNGjQAPb29vDy8kLXrl1x7NgxKUaZTIaVK1caHL9grLp++3PnzuHll1+Gm5ub9J/rqVOnMGzYMFSrVg329vbw9fXFiBEj8ODBA4Pj3rp1CyNHjoS/vz+USiWCgoLw5ptvIjs7G1evXoVMJsNXX31lsN8///wDmUyGdevWFXn+ACA+Ph4jR46Ej48P7O3t0ahRI6xatUqvju51f/7551iyZAmCg4OhVCrRokULHD16tNjjA4X3u7dv3x7169fHqVOn0K5dOzg4OKB69erYuHEjAGDfvn0ICQmBSqVCrVq1sGvXLr1jXr9+HWPGjEGtWrWgUqng4eGB/v37F9oFoXsOlUqFypUr46OPPsKKFSsK7bLYtm0b2rZtC0dHRzg7O6NHjx44e/bsY18jAJw9exYdO3bUex6NRmNQ77fffkOPHj2k32lwcDDmzJmDvLw8vfPz559/4vr169J7oWrVqgCA7OxszJgxA82aNYNarYajoyPatm2LPXv2lCjOLVu2wM7ODs8++6xeeWnO6ZPYuHEj5HI5Ro0aJZXZ29tj5MiRiIiIwI0bNx67f4sWLdCiRQuprHbt2ujUqRN+/fVXqWzPnj148OABxowZo7f/2LFjkZaWhj///FMqc3d3h7Ozc1lfWqESExMxbNgwuLq6Qq1WY/jw4UhPT9erU9j4mpJeR0IIfPTRR6hcuTIcHBzQoUOHIq/VxMRETJo0CVWqVIFSqUT16tXx6aef6h23rO9z3d+8ggobb1S1alX07NkTBw8eRMuWLWFvb49q1arhp59+0tu34N+O4t4X5uDj4wOVSvXYeqW5Bkt6XQOAQqFA+/bt8dtvv5XpdbAb6wm98cYbWLlyJYYPH44JEyYgJiYG3377LU6cOIFDhw5BoVAgPj4ezz33HLy8vPDOO+/A1dUV165dw6ZNmwAAXl5eWLRoEd5880307dsXL7zwAgCgYcOGRT7v1atXsWXLFvTv3x9BQUG4e/cuFi9ejHbt2uHcuXPw9/cHAOTl5aFnz54IDw/HwIEDMXHiRKSkpGDnzp04c+aM1Bw4cuRIrFy5Et26dcNrr72G3NxcHDhwAIcPH0bz5s2f6Nz0798fNWrUwNy5cyGEAADs3LkTV69exfDhw+Hr64uzZ89iyZIlOHv2LA4fPiz9wbh9+zZatmyJxMREjBo1CrVr18atW7ewceNGpKeno1q1amjTpg3WrFmDyZMn6z3vmjVr4OzsjN69excZW0ZGBtq3b4/Lly9j3LhxCAoKwoYNGzBs2DAkJiZi4sSJevXXrl2LlJQUvPHGG5DJZPjss8/wwgsv4OrVq1AoFKU+Nw8fPkTPnj0xcOBA9O/fH4sWLcLAgQOxZs0aTJo0CaNHj8bLL7+M+fPn48UXX8SNGzekD6ajR4/in3/+wcCBA1G5cmVcu3YNixYtQvv27XHu3Dk4ODgA0CaLHTp0gEwmw/Tp0+Ho6Igff/wRSqXSIJ7Vq1dj6NChCAsLw6effor09HQsWrQIzzzzDE6cOFHsH9W4uDh06NABubm5eOedd+Do6IglS5YU+odx5cqVcHJywpQpU+Dk5ITdu3djxowZSE5Oxvz58wEA7733HpKSknDz5k0pmXVycgIAJCcn48cff8SgQYPw+uuvIyUlBcuWLUNYWBiOHDmCxo0bF3ve//nnH9SvX9/gd1bSc6rRaJCQkFDsc+io1WrpeU6cOIGaNWvCxcVFr07Lli0BANHR0ahSpUqhx9FoNDh16pT0z1HB/Xfs2IGUlBQ4OzvjxIkTAGDwnm3WrBlsbGxw4sQJvPLKKyWKvyxeeuklBAUFYd68eTh+/Dh+/PFHeHt749NPPy1yn9JcRzNmzMBHH32E7t27o3v37jh+/Diee+45ZGdn69VLT09Hu3btcOvWLbzxxhsICAjAP//8g+nTp+POnTsG41+M/T4vyuXLl/Hiiy9i5MiRGDp0KJYvX45hw4ahWbNmqFevXqH7FPe+KEpSUhJycnIeG4+9vf1jj1VSJb0GS3Nd5z/Gb7/9huTkZIP3UomVqV3oKVGw6e7AgQMCgFizZo1eve3bt+uVb968WQAQR48eLfLYpe3GyszMFHl5eXplMTExQqlUig8//FAqW758uQAgvvzyS4NjaDQaIYQQu3fvFgDEhAkTiqwTExMjAIgVK1YY1CkYt64pe9CgQQZ1dc2g+a1bt04AEPv375fKhgwZImxsbAo9Z7qYFi9eLACI8+fPS9uys7OFp6fnY5uyFyxYIACIn3/+WW/f0NBQ4eTkJDXv6163h4eHXvPrb7/9JgCI33//vdjn2bNnjwAg9uzZI5W1a9dOABBr166Vyi5cuCAACBsbG3H48GGp/O+//zY474Wdw4iICAFA/PTTT1LZ+PHjhUwmEydOnJDKHjx4INzd3fW6BVJSUoSrq6t4/fXX9Y4ZFxcn1Gq1QXlBkyZNEgBEZGSkVBYfHy/UarVB90Nhsb/xxhvCwcFBZGZmSmVFNdfn5uaKrKwsvbKHDx8KHx8fMWLEiGLjFEKIypUri379+hmUl/Sc6q6Hkjzy/87r1asnOnbsaPAcZ8+eFQDEDz/8UGTMur8N+d/XOt99950AIC5cuCCE0P6NksvlhR7Hy8tLDBw4sNBtxu7GKvi76Nu3r/Dw8NArK9jlVNLrKD4+XtjZ2YkePXpIfwuEEOLdd98VAPSOOWfOHOHo6Cj+/fdfved+5513hFwuF7GxsUKIsr/Pda+7oMK64AIDAw3+3sXHxwulUimmTp0qlRX2t6O03Vi6vzWPe5S266+4bqySXoOlua511q5da3CNlBa7sZ7Ahg0boFar0aVLF9y/f196NGvWDE5OTlLTuqurKwDgjz/+KFGWXRJKpVIa3JeXl4cHDx7AyckJtWrVwvHjx6V6//d//wdPT0+MHz/e4Bi6VpT/+7//g0wmw8yZM4us8yRGjx5tUJb/v7TMzEzcv38frVq1AgApbo1Ggy1btqBXr16FtirpYnrppZdgb2+PNWvWSNv+/vtv3L9//7H/vf7111/w9fXFoEGDpDKFQoEJEyYgNTUV+/bt06s/YMAAuLm5ST+3bdsWAIqcDfM4Tk5OGDhwoPRzrVq14Orqijp16iAkJEQq132f/3nyn8OcnBw8ePAA1atXh6urq97vfvv27QgNDdVr7XB3d8fgwYP1Ytm5cycSExMxaNAgvetYLpcjJCTksV1Ef/31F1q1aiW1UgDa1sqCz1Mw9pSUFNy/fx9t27ZFeno6Lly4UOzzAIBcLpcGbOpaWXJzc9G8eXO9116UBw8e6P0eC4uruHPq6+uLnTt3lujRqFEjab+MjIxCW9Ts7e2l7UXRbSvJ/sUNhre3ty/2eYyp4Hu/bdu2ePDgAZKTk4vcp6TX0a5du5CdnY3x48fr/X0qbNDuhg0b0LZtW7i5ueld2507d0ZeXh7279+vV9/Y7/Oi1K1bVzo2oH2dtWrVMvrzfPHFFyW6VqdNm2a05yzpNVia61pH97spydIGRWE31hO4dOkSkpKS4O3tXej2+Ph4AEC7du3Qr18/zJ49G1999RXat2+PPn364OWXXy70F10SujE233//PWJiYvTGPOQfwX7lyhXUqlULtrZF/4qvXLkCf39/uLu7P1EsRQkKCjIoS0hIwOzZs7F+/Xrp/OgkJSUBAO7du4fk5GTUr1+/2OO7urqiV69eWLt2LebMmQNA24VVqVIldOzYsdh9r1+/jho1ahjMBtHNrLt+/bpeeUBAgN7PujddwbFZJVW5cmWDRFKtVht0ZehmDOV/noyMDMybNw8rVqzArVu3pC5C4L9zqHsNoaGhBs9dvXp1vZ8vXboEAEWes8c1F1+/fl0vQdOpVauWQdnZs2fx/vvvY/fu3QYffPljL86qVavwxRdf4MKFC3r/PBR2vRUm//nSKek5tbe3R+fOnUv0PPmpVCpkZWUZlGdmZkrbi9sXQIn2V6lUBl05+euWZMyFMRT3finqeirpdaR7b9aoUUOv3MvLyyCRvXTpEk6dOiWNiSyo4N8gY7/Pi1LweXTPZeznadasmVGPVxIlvQZLc13r6N6XZfknnMnOE9BoNPD29tZrWchP9waTyWTYuHEjDh8+jN9//x1///03RowYgS+++AKHDx9+or7SuXPn4oMPPsCIESMwZ84cuLu7w8bGBpMmTSp0QF9ZFXVx5U+yCirsD+tLL72Ef/75B2+//TYaN24MJycnaDQadO3a9YniHjJkCDZs2IB//vkHDRo0wNatWzFmzBijT2mVy+WFlhf2wVmW45XkecaPH48VK1Zg0qRJCA0NhVqthkwmw8CBA5/oHOr2Wb16NXx9fQ22F5col0ZiYiLatWsHFxcXfPjhhwgODoa9vT2OHz+O//3vfyWK/eeff8awYcPQp08fvP322/D29oZcLse8efP0BtwXxcPDo9APlJKe07y8PNy7d69Er9fd3V36D9fPzw+3bt0yqHPnzh0AkMbYFXUcpVIp1S1ufz8/P+Tl5SE+Pl7vn7Ds7Gw8ePCg2OcxJmO/X56URqNBly5dimy5qFmzpt7PTxp3af8+muv8JCQkFJl45KdSqYy2FENJr8HSXNc6uveup6fnE8fHZOcJBAcHY9euXWjTpk2J/mNq1aoVWrVqhY8//hhr167F4MGDsX79erz22mulzlQ3btyIDh06YNmyZXrliYmJehdCcHAwIiMjkZOTU+QAu+DgYPz9999ISEgosnVH9x9OYmKiXnnBFpDiPHz4EOHh4Zg9e7be4lC6lgUdLy8vuLi4lGi1zK5du8LLywtr1qxBSEgI0tPT8eqrrz52v8DAQJw6dQoajUYvMdJ1pQQGBpb0ZZndxo0bMXToUHzxxRdSWWZmpsHvJjAwEJcvXzbYv2CZbpC6t7f3E7VaBAYGGvwOAe06Kvnt3bsXDx48wKZNm/RmQ8XExBjsW9T7YePGjahWrRo2bdqkV6ewLtjC1K5du9DnK+k5vXHjRolbkPbs2SOtmdW4cWPs2bPHYGBlZGSktL0oNjY2aNCggTQzMr/IyEhUq1ZNGsSpO86xY8fQvXt3qd6xY8eg0WgeO4Dbkkp6Henem5cuXUK1atWk8nv37hkkssHBwUhNTX2i67o08v991A1bAEr397EkSvs58cILLxh0yRdm6NChhc60fRIlvQZLc13rxMTEwMbGxiBJLQ2O2XkCL730EvLy8qQulPxyc3OlP5QPHz40yNh1v3BdE55utkfBP65FkcvlBsfcsGGDwX+P/fr1w/379/Htt98aHEO3f79+/SCEkBY1LKyOi4sLPD09Dfq4v//++xLFq4s5/zF1Cs6IsLGxQZ8+ffD7778X+kbIv7+trS0GDRqEX3/9FStXrkSDBg2KncWm0717d8TFxeGXX36RynJzc/HNN9/AyckJ7dq1K/HrMrfCfvfffPONwX+RYWFhiIiI0FssLiEhwaAlMiwsDC4uLpg7d26hY8oe15LRvXt3HD58GEeOHNHbp+DzFPb7z87OLvQacnR0LLRbq7BjREZGIiIiotgYdUJDQ3HmzBmDpvOSntMnHbPz4osvIi8vD0uWLJHKsrKysGLFCoSEhOh1X8bGxhqMX3rxxRdx9OhRvffDxYsXsXv3bvTv318q69ixI9zd3bFo0SK9/RctWgQHBwf06NGjROfJEkp6HXXu3BkKhQLffPON3u+ssJWFX3rpJURERODvv/822JaYmIjc3FyjxK77hyH/38e0tDSDpSzKqqj3RVEsMWanNNdgSa9rnaioKNSrV69MrVBs2XkC7dq1wxtvvIF58+YhOjoazz33HBQKBS5duoQNGzbg66+/xosvvohVq1bh+++/R9++fREcHIyUlBQsXboULi4uUuarUqlQt25d/PLLL6hZsybc3d1Rv379Iset9OzZEx9++CGGDx+O1q1b4/Tp01izZo3efzqAtpvnp59+wpQpU3DkyBG0bdsWaWlp2LVrF8aMGYPevXujQ4cOePXVV7Fw4UJcunRJ6lI6cOAAOnToIC2r/9prr+GTTz7Ba6+9hubNm2P//v3FrqJZkIuLC5599ll89tlnyMnJQaVKlbBjx45C/9OeO3cuduzYgXbt2mHUqFGoU6cO7ty5gw0bNuDgwYN6/z0NGTIECxcuxJ49e4qd2prfqFGjsHjxYgwbNgxRUVGoWrUqNm7ciEOHDmHBggUmW3/EGHr27InVq1dDrVajbt26iIiIwK5du/TGagHAtGnT8PPPP6NLly4YP368NPU8ICAACQkJ0n+JLi4uWLRoEV599VU0bdoUAwcOhJeXF2JjY/Hnn3+iTZs2hSbL+Z9n9erV6Nq1KyZOnChNGda1num0bt0abm5uGDp0KCZMmACZTIbVq1cX2nTfrFkz/PLLL5gyZQpatGgBJycn9OrVCz179sSmTZvQt29f9OjRAzExMfjhhx9Qt25dpKamPvbc9e7dG3PmzMG+ffvw3HPPlfqcPumYnZCQEPTv3x/Tp09HfHw8qlevjlWrVuHatWsGrbNDhgzBvn379M7LmDFjsHTpUvTo0QNvvfUWFAoFvvzyS/j4+GDq1KlSPZVKhTlz5mDs2LHo378/wsLCcODAAfz888/4+OOP9Vpuk5KSpCX8Dx06BAD49ttv4erqCldXV73baQwbNgyrVq1CTEyMydZ2Kel15OXlhbfeegvz5s1Dz5490b17d5w4cQLbtm0z6N54++23sXXrVvTs2VOa2p2WlobTp09j48aNuHbtWpm6RHSee+45BAQEYOTIkXj77bchl8uxfPly6X1kLEW9L4qrbyzXr1+X7h6gS04++ugjANrWNl2LemmuwZJe14B24sC+ffsM1u8ptSeex/UUKWq63ZIlS0SzZs2ESqUSzs7OokGDBmLatGni9u3bQgjtqq2DBg0SAQEBQqlUCm9vb9GzZ09x7NgxveP8888/olmzZsLOzu6x09AzMzPF1KlThZ+fn1CpVKJNmzYiIiJCtGvXTrRr106vbnp6unjvvfdEUFCQUCgUwtfXV7z44oviypUrUp3c3Fwxf/58Ubt2bWFnZye8vLxEt27dRFRUlN5xRo4cKdRqtXB2dhYvvfSSiI+PL3LqeWGrqN68eVP07dtXuLq6CrVaLfr37y9u375d6Ou9fv26GDJkiPDy8hJKpVJUq1ZNjB071mDqsRDaqb02Njbi5s2bRZ6zgu7evSuGDx8uPD09hZ2dnWjQoIHB1HrdlNT8K9/qPO53JETRU8/r1atnUDcwMNBg1VHd84wdO1b6+eHDh1LcTk5OIiwsTFy4cKHQ1WNPnDgh2rZtK5RKpahcubKYN2+eWLhwoQAg4uLiDGINCwsTarVa2Nvbi+DgYDFs2DCD67Qwp06dEu3atRP29vaiUqVKYs6cOWLZsmUG024PHTokWrVqJVQqlfD39xfTpk2TptfnP0epqani5ZdfFq6urgKANN1Wo9GIuXPnisDAQKFUKkWTJk3EH3/8IYYOHVriKbkNGzYUI0eO1CsrzTl9UhkZGeKtt94Svr6+QqlUihYtWojt27cb1NNNFy7oxo0b4sUXXxQuLi7CyclJ9OzZU1y6dKnQ51qyZImoVauWsLOzE8HBweKrr77Sm6YtRPHT6Auey379+gmVSiUePnxY7Gss6r1f1BTsgue2pNdRXl6emD17tvT3r3379uLMmTOFHjMlJUVMnz5dVK9eXdjZ2QlPT0/RunVr8fnnn4vs7Gy9c/Gk73MhhIiKihIhISHCzs5OBAQEiC+//LLI113Y+7zg3+7C/nYU9b4wB108hT0KfuYIUbJrUIiSX9fbtm0TAIq85ktKJoSZR44RGVGTJk3g7u6O8PBwS4dS7k2aNAmLFy9GampqkQMlrdnq1asxduxYxMbG6rUQUtF8fHwwZMgQaeFHInPr06cPZDIZNm/eXKbjcMwOVVjHjh1DdHQ0hgwZYulQyp2C61Q8ePAAq1evxjPPPPNUJjoAMHjwYAQEBOC7776zdCgVwtmzZ5GRkaF3A0ciczp//jz++OOPQsfHlhZbdqjCOXPmDKKiovDFF1/g/v37uHr1qrQYFWk1btwY7du3R506dXD37l0sW7YMt2/fRnh4uMH9oYiIrB0HKFOFs3HjRnz44YeoVasW1q1bx0SnEN27d8fGjRuxZMkSyGQyNG3aFMuWLWOiQ0RPJYu27Ozfvx/z589HVFQU7ty5g82bN6NPnz7SdiEEZs6ciaVLlyIxMRFt2rTBokWL9FbQTEhIwPjx4/H777/DxsYG/fr1w9dff220m5sRERFRxWbRMTtpaWlo1KhRkX3on332GRYuXIgffvgBkZGRcHR0RFhYmLSkNKDthz979ix27tyJP/74A/v378eoUaPM9RKIiIionCs3Y3Z0o611LTtCCPj7+2Pq1Kl46623AGjXh/Dx8cHKlSsxcOBAnD9/HnXr1sXRo0elG0du374d3bt3x82bN822RDoRERGVX+V2zE5MTAzi4uL0FvJSq9UICQlBREQEBg4ciIiICLi6uurdIbtz586wsbFBZGQk+vbtW+ixs7Ky9FZS1d1F2cPDo0w3GiMiIiLzEUIgJSUF/v7+xd4bsdwmO3FxcQC06zzk5+PjI22Li4szuPO4ra0t3N3dpTqFmTdvXqG3SCAiIqKK58aNG6hcuXKR28ttsmNK06dPx5QpU6Sfk5KSEBAQgBs3bujdrI+IiIjKr+TkZFSpUuWxt/opt8mOr68vAODu3bvw8/OTyu/evSvdTNPX1xfx8fF6++Xm5iIhIUHavzBKpRJKpdKg3MXFhckOERFRBfO4ISjldgXloKAg+Pr66t0GIDk5GZGRkQgNDQWgvZNxYmIioqKipDq7d++GRqNBSEiI2WMmIiKi8seiLTupqam4fPmy9HNMTAyio6Ph7u6OgIAATJo0CR999BFq1KiBoKAgfPDBB/D395dmbNWpUwddu3bF66+/jh9++AE5OTkYN24cBg4cyJlYREREBMDCyc6xY8fQoUMH6WfdOJqhQ4di5cqVmDZtGtLS0jBq1CgkJibimWeewfbt2/VWzF2zZg3GjRuHTp06SYsKLly40OyvhYiIiMqncrPOjiUlJydDrVYjKSmJY3aIiIgqiJJ+fpfbMTtERERExsBkh4iIiKwakx0iIiKyakx2iIiIyKox2SEiIiKrxmSHiIiIrBqTHSIiIrJqTHaIiIjIqjHZISIiIqvGZIeIiIisGpMdIiIismpMdoiIiMiqMdkhIiIiq8Zkh4iIiKwakx0iIiKyakx2iIiIyKox2SEiIiKrxmSHiIiIrBqTHSIiIrJqTHaIiIjIqtlaOgAiIiKyIklJQGamYbmHB2BrmbSDyQ4REZE1EAI4eRLIygJatgRksic7zqVLwJIlwKlTpd83MRE4cqTwbRcuALVqPVlMZcRkh4iI6EncuAHExBiWq1RA06aAXK5frtEA0dFAamrJn+PyZSAiQrtvQffvA+Hh2uRGd3xdPZnM8PlLKjf3yfZ7nMuXmewQEREZnRDapKS4D/A7d4A9e4CcnKLr3L2rraNLLPLygJs3i67v7Ay4u+uXpaVpExRzEKJsSUuNGkDt2qVPmGQyoHFj4KWX9PdVqYDKlZ88njJiskNERFpCFP+Bbwp5ecC+fdqEpKzH+ecf4OpV/fJbt4Br18p27OK4uRkmBCkp/z0KUigAF5eSdzEpFEBQEGBvb7jNxgYICACee077PaA9dpMm2tag7OzSvRYde3ugTRvDZK0CY7JDRFSeJCUBkZHAiRPah7loNMDx48CVK+Z7TnORybRJQ1FsbICqVbWtMcXVCQzUtnboEhVPT2DAAMDLS79uejqwapW2NahgHA0aAL16FR+PMfTubdrjVzBMdojIusTGahOGknJ21n7QlVV2tnZgZ1FjK3bt0g7QPHhQ2wpRlKQk042ZKK8cHLRdHE86oFbH2VmbkCiV/5XZ2gJdugAdOhR/fHf3wltPnoSDA/Dmm8Y5FhkFkx0iKn9yc4G//9YmDwCQnAzs3Fn8wM769bVjIjZvLv3zBQZquyPKIjYWSEgo2zF01Grth2+1aqZvAcjPwQHo0cM4yV9pqNXarhcbLv1GpsFkh4gsJycH2LtX25px86Z27EZurnbGSnGDPwsTHf3f946OJd8vPR24fl37KCuFArCzMyzXja3w8NAmMI0aFX+MDh2AunXLHg8RAWCyQ0SmpNFo1+rIv8BYRgYwcyZw7Ji266eoLh2VSpsY6P7br1RJ29VR2KJkubna58nKAho2BObNA5ycShbjnTvAypWl6/oqjJ2ddpxEs2aFb3dwMG8rDRFJmOwQUeGEAOLitOMY8nfx3L0LbNyoXTNj167/puIWJjERuHev+OdxcNAO9LS11XafODlpE51XXgG6dzd914ZaDXzyiWmfg8iKZeRk4H7646fU+zr5QiG3TMLPZIfIUoSw7PNfugQcPqxtFTl0yHBxtJs3tXV062a4umq7nSIjSzc9WaEwbGXx9AQ6dQJatdKu5xEaWvbBqUTllCjmvX4m/gyO3zkOAAh2D0brKq0hg/a9cDvlNvZe24tcjWUGrOdqcnEg9gBik2KLrJMn8hB1OwppOWmPPd6FsRdQy5OLChJZv5wc4KuvtMup79pV9q4TcxDCcAp0pUqAr692HEr16kW3vshk2mXru3fXX4ukLKu70lPjQfoDnLx7sthk4XEUcgVaVmoJe9v/Zlr9++Bf3Ei6AZlMhnpe9XAp4RKycotpoSxCjiYH+6/vx83koseX5WpycTD2IG4kl3EdoXJOBhlsZMW3wl5JuMJkh6jM8vK0yYO7u3ZcSF6etkXh7l3g4cPC9/Hy0g4azU+j0S5MVtT034QE7RLtGRmFb09L084cSkw03JaZWXQsllClinaarouLdkZS/gRELtcuVlalCrB27X8zoapUASZO1NYnq5SQkYD4tPgyHycnLwf7ru/D7ZTbBtuEEDgedxxn7p6BgGEyIyBwP/2+UVo17OX2cFNpu2JzNbm4l/6YrlUzspHZoIpLFchlctxIvoEcjX6rqZ+THxztSjHg3shc7V2l+Iri7uCO15q8Bi9HryLrqGxV8HHyMUWIJcJkhyoWIbTJBKBNNnbt0iYP8fHaQabXr2sHtcbFaceS1KgBXLxYdJeRjY12ynL+9TXu3Cn7aq7FsbMDWrTQxtmrl/HW9igtpRJo3rxkq6R26WL6eEiPEOKxXQNCCJyIO4Ez8WcMtt1JuYM91/YYfHjq2MhsEFo5FNXdq+sdb9/1fdh8YbPFuk4KcrV3hZ28kBluJZSanYr0nHTcSb0jlckgg6eDJ7LzspGUlQSVrQrOymIWFCyGm70bqrhUgayYblhXe1d0r9EdHg4eBttUchVCKofAWemMlKwUHIw9iFyhPfcKGwUa+TSCn7PfE8VG/5GJsrQPWonk5GSo1WokJSXBxcXF0uE8fe7eBZYv187OKYxCAXTsqF2Ybdkyw+XgS6KwhEKIogfX2tgUPoUY0HbBBARox7AUtd3XVzvOpbBjVK0KvPgiZ+Y8pW6n3Max28ekrpmM3AyEXw1HQuZ/a/QIIXAm/gwuJVyyVJh63T5l4a5yR2WXytI4lPwcFA6o41kHldWF3zPJXm6PXjV7obpH9UK3l0ROXg5+PfsrYpP/G3fi6+iLfnX7wdnOGRE3IpAn8vBMwDPFJixUPpX085vJDpjsmERMDPD229ob5z1uNdi0tOJXlC2Ouzvg46PtcqleXdtScvas9niNG2sH4FauDLz6qnbWT0FRUcCWLfoDbuVybXLVsWPRg2adnS3XIkMW8zDjIa4+/C/ZTsxMxK6ru/Bvwr/Yd21fka0oOkIIpGQXcr+kMlDYKBDkGgRbG/2GehsbG1R1rQovB69CE42MnAycvX8WuXn6709npTMa+zbG1NCpcLV3LXN8dnI7qO3VZT4OUWGY7JQCkx0jEAL46y/g3Dntz0uX/rf6bUlUrqxdRK2w1o7kZG33lK2ttsvpxReB4GDtNm9vbZJDFd7DjIfIznvCGxcWI0/kIeJGhF6Skp+Ativo9N3TxR5HIzS48vCKUWL0cvCCUv7fLQ28nbzh6+Srl5QobZXoXr076njVKfZYSrkSDXwalKmrh6iiKunnN8fsUNkIoe1++vprYM0a/W0uLkC/fkBISPHHUCq1rSgBAaaLk4wiT5OHPdf24HLCZey8uhNp2YWPKanhXgP1vOshPCYcKVmPb8m4l35Pmn5b3jkoHKRWFBlkqOxSGV4OXghyC0LLSi0LbUXJT2mrRJdqXVDJpZI5wiUiMNmhwiQna8fRVK+uHah77Zr+9tu3tWNn9u/XdlHlv/FhzZraBeHs7LRrqHz9NddPKceEEPj51M/Yd31fsfXyRB4O3zyMC/cvlOi4f1/52xjhGY2znTOqulYtcmqsylaFam7V4O3oXexx1PZqDG4wWC9RUcqVkNtwGj1RecZkh/4jBPDFF8CsWdpxNK6uhU+fLsjWVrvuSq1awDffaBMeMovkrGRp5dJ7afewO2Y3svKy4GTnhC7VukAmk+H93e/jdPzpQtcqydPk4WZK6e5BpbJVwdfJF5VdKsND5YGCDRl5mjxcT7yO1OxU+Dn7wcvBy6BOQTawQUjlEDxf6/nHrtXxJGxtbBHkGsQBqERPKSY7pJWZCbz2mn5XlC7RcXPTXzROLtcmNK1ba+8D5OoKPPssB+w+oVxNLjRCg4v3LyLyVqRBUvIw8yF2Xd2FjFz9dX1y8nJwIu5EmceQyCBDy0otH7uWh7OdMzoGdUQzv2YIrRJqkqSEiMgUmOyQtitqyBBgwwZtl1PXrsD//qcdi6NUAgMHapf3p1JLyUrB4ZuHcfLuSRy/c9xg8bQH6Q+w59qeMq1pYmtjCxuZDWSQIVAdCBd7F9xPvy8t5Obj6IMOVTsgyC2o0P29HbzxSqNX4KLk4Hwisk5Mdp52hw8DL7+snSpuY6P9/vvvtVOr27WzdHTlQlJmEmKTYpGRm4FdV3chISOhyLpZuVkIjwmXVmhNzkouccuLLllR2ioNyiurK8PTwRM20G9N8XXyxYgmI6Qpwm4qNzgoHKTnTslKgb2tfaGLmRERPS2Y7DzNzp8HOnTQdmGpVEC3bsCiRYY3bbRieZo8JGclA9AmBzuu7MClhEvSzfc0QoML9y8gK6/0983RcVG6wN3eHdXcqhlMD5bJZAh2C0bv2r2hlCvRyLeR0VpYXJQubK0hIgKTnadXZibwxhvar1WrAtOna8fsFHVDxwouIycDR28fRV2vusjOy0bEjQhE3orEyuiVJbpPjr2tPeQyOXwcfeDj5FPs9GIvRy+09G8JBzvtFOV2ge3QwKeBMV8OERGVApOdp8WaNdqEpmVL4NYtbfcVoJ1J1a8fMGqUZeMrQAiBC/cvICmr6LuCZ+Vm4f/O/x9+OftLsWu5+Dr5IiEjodhj6fg5+cHb0RtVXatKi7y52rvi1Yavwt/FH0q5EiqF6oleExERWQaTHWuWnQ2sXq29seX8+dr1c/Lf4FKl0q5G/PHHlouxgMzcTETdjsLkvyfj6O2jRjlmTGKMQZm3ozfcVe5o5NMIwxoPg5+T9kZ7Pk4+8HXyNcrzEhFR+cBkx9pcvgzs26d97NkD3Cywhkr79top4i1bAs88o125WG75BdHOxJ/BoqOLsPrUauneQTYyG7goXYrtMvJx8kEzv2boULWDwb2BAO307AOxB5An8jCuxTgAwLXEa2gb2JYr2BIRPSV4byxY0b2xNm8GBg3Sv5O3UgnUqaO951RYGDBnjuXiK8SJOycwftt4HLpxSCqTQYZqbtUwsP5ATH9musHspPxsZDZc74WI6CnFe2M9bTQaYMwYbaLj4QEEBWnvNdWiBfDWW9qxORaWlJmEuNQ4pGanYseVHUjOSsayE8twL/0eZJChtmdtNPdvjrEtxqK6e3VOlyYiIqOw/CcgGcfRo0BcnPaeVIsXawcdW9DDjIfYcmEL0nPSpQX1TsefLnTNGV8nX7zW5DW83eZtTpUmIiKjY7JjLTZs0H6tUQN4/nmzPrUQAkduHcH6M+vh4eABf2d/zNk/B9cSrxnUtZPbQS6Tw8/ZD54OnlDZqtCrZi9MbT3VrDETEdHTg8lORafRALNna2/gCfw3PsdEriVeQyXnSlDItc9xJ+UOBv3foELvmq1WqlHJuRKUtkoEuwUjyDUIw5sMh6eDJxztHKWVfomIiEyJyU5FN2UK8PXX2u9btQLee89oh87Oy8bvF3/H1YdXAQAn757EmtNr4OngiWGNhsHDwQPfHPkGt1NuQy6To7p7deRqcpGdl42qrlXxcv2XMbrFaKPFQ0RE9CSY7FRUmZna9XF0iU6vXsB33wFVqhjl8CfjTmLAxgG4+OCiwbb76ffxecTn0s+eDp4YVH8QPu38KRfcIyKicofJTkW0d692inlcnPbnjh2BZcsAL68yHfbcvXP44dgPOBF3AgdjDwIAHBWOqOZWTZriXderLqqoq+Dvy39DIzRwUbpgWONhGNFkRBlfFBERkWkw2aloDhwAnnsOyMnR3pm8Uydg4cIyJzqHbx5Gl9VdkJqdCgDSVPBXGryC6W2nQybTX9hvXqd5ZXo+IiIic2GyU5F89hnw/vvaRKdWLWDWLGDAAEBW9ArDJZGdl42RW0ciNTsVfk5+aOjTEK0rt8aEVhPgau9qlNCJiIgshclORXHjBvC//2m/DwzUzsAaMMAoh14atRTn7p2Dg8IB05+ZjvEh441yXCIiovKAyU5F8csv2q9+fsDWrUDDhmU6XFp2Gq4+vAovRy98cugTAED7wPYY23JsWSMlIiIqV5jsVBRr12q/Nm1a5kQnJy8HjRc3xuWEy1KZs50z3mj+Bu8zRUREVofJTkVw8SJw4gRgYwN07/5Eh0jLTsPS40txNv4sXO1d9RIdla0K/er0Q6+avYwVMRERUblRrv+Nz8vLwwcffICgoCCoVCoEBwdjzpw5yH+jdiEEZsyYAT8/P6hUKnTu3BmXLl2yYNQmsG6d9mtwMPDqq6Xe/WbyTYQuC8XkvyfjxxM/SmvktA1oi98G/oYfev6AZb2XGcy4IiIisgblumXn008/xaJFi7Bq1SrUq1cPx44dw/Dhw6FWqzFhwgQAwGeffYaFCxdi1apVCAoKwgcffICwsDCcO3cO9vb2Fn4FRvLHH9qv9eppp5uXkEZo8OPxHzFz70zEpcbBUeGIILcgaUXkztU64/la5r2PFhERkbmV62Tnn3/+Qe/evdGjRw8AQNWqVbFu3TocOXIEgLZVZ8GCBXj//ffRu3dvAMBPP/0EHx8fbNmyBQMHDrRY7EaTkwOcPq39vlWrUu3686mf8cYfbwAAHBQOeK3pa/gq7CvkaHJw/M5xtPBvYexoiYiIyp1y3Y3VunVrhIeH499//wUAnDx5EgcPHkS3bt0AADExMYiLi0Pnzp2lfdRqNUJCQhAREVHkcbOyspCcnKz3KLcuXACyswGlEujSpVS7Ljq2CIA20RnWaBi+DPsSMpkMdnI7tKrcCnIbuSkiJiIiKlfKdcvOO++8g+TkZNSuXRtyuRx5eXn4+OOPMXjwYABA3KPbJfj4+Ojt5+PjI20rzLx58zB79mzTBW5M0dHarz4+QP36Jd5t3oF5OHzzMGxkNpjTYQ6mhE4xTXxERETlXLlu2fn111+xZs0arF27FsePH8eqVavw+eefY9WqVWU67vTp05GUlCQ9bty4YaSITUCX7Pj6AnZ2Jdrlz3//xLu73wUAPBPwDEY1G2Wi4IiIiMq/ct2y8/bbb+Odd96Rxt40aNAA169fx7x58zB06FD4+voCAO7evQs/Pz9pv7t376Jx48ZFHlepVEKpVJo0dqPZuVP71d+/RNXzNHmYsF07eDukUgh+6vMTnOycTBUdERFRuVeuW3bS09NhY6Mfolwuh0ajAQAEBQXB19cX4eHh0vbk5GRERkYiNDTUrLGaxJkz2sHJNjbAo0Haj7P32l5cfXgV9rb2GNNiDAJdA00cJBERUflWrlt2evXqhY8//hgBAQGoV68eTpw4gS+//BIjRowAAMhkMkyaNAkfffQRatSoIU099/f3R58+fSwbvDHobhFRowYwaFCJdll7WrvScl2vuhjcYLCpIiMiIqowynWy88033+CDDz7AmDFjEB8fD39/f7zxxhuYMWOGVGfatGlIS0vDqFGjkJiYiGeeeQbbt2+3jjV2tm3Tfq1dG3B0fGz1JVFLsPLkSgBAA+8GnG1FREQEQCbyL0f8lEpOToZarUZSUhJcXFwsHY7W/fuAtzcgBLBgATBxYrHVL96/iIY/NER2Xjaa+jXFz31/Rh2vOuaJlYiIyAJK+vldrsfsPNXCw7WJjrc30L9/sVWFEBi3bRyy87JR3a06VvZeyUSHiIjoESY75dX//Z/2a/Xqj52JteHcBuy6ugtymRzdanRDA58GZgiQiIioYmCyUx6lpAC//679vl69YqsKITD3wFwA2jV1PuvymamjIyIiqlCY7JRHmzYBmZmAuzsweXKxVSNvReLk3ZOwtbHFgHoDYG9rBQOziYiIjIjJTnm0eLH2a6NGQJ3ix978fOpnAEA9r3oY3mS4qSMjIiKqcJjslDenTwMREdqFBB/d8LQ4O67sAADU8azDVh0iIqJCMNkpb37WttSgZk1g9Ohiq+64sgOXEi5BBhl61eplhuCIiIgqHiY75YlGA6xbp/2+QQPA2bnIqpvOb0LYz2EAgCouVdCjRsluJ0FERPS0YbJTnuzbB9y4ob27+eCib/WQnJWM8dvGSz+3rtIaanu1OSIkIiKqcMr17SKeOj/8oP3aoAHQvXuR1WbtnYXbKbfhZu+Gpb2W4oU6L5gpQCIiooqHyU55kZSknXIOAM2bAwpFodVuJt/EwsiFAIDuNbqjX91+5oqQiIioQmI3Vnlx4QKQm6sdpzN1apHVtl/ejjyRh8rOlTG301wzBkhERFQxMdkpLy5d0n51dweCg4uspptqHuwejAB1gDkiIyIiqtCY7JQXly9rv7q7a9fYKUSuJhfhMeEAgGC3ohMiIiIi+g+TnfJC17Lj4VFklTWn1iAhIwGOCkcMaTTETIERERFVbEx2yov83ViFEELgk0OfAABCK4fi2cBnzRUZERFRhcZkpzwQ4r9kp3btQqtcSriEC/cvQC6TY3CDwZDJZGYMkIiIqOJislMe3LgBJCZqx+q0aVNoFd3A5AB1APrX62/G4IiIiCo2JjvlQWSk9quPD9CqVaFVdMlONbdqcLRzNFdkREREFR6TnfLgyBHt10qVABcXg80P0h/o3d2ciIiISo7JTnmga9mpVKnQzSujVyIrLwu+Tr6YEDLBjIERERFVfEx2LC03F4iK0n7fvLnBZo3QYHHUYu1m/+ao4VHDnNERERFVeEx2LO3sWSA9XXun8969DTbvjtmNSwmXYCe3w6B6gywQIBERUcXGZMfS8o/XqVvXYPNPJ38CADT0aYiX6r9kzsiIiIisApMdS9ON1/H3B+Ryg81Hbx8FANTyqAVbG96knoiIqLSY7FiabrxOIYOT07LTcPH+RQBAu8B25oyKiIjIajDZsSSNBvj3X+33TZoYbD4dfxoCAk52TugQ1MHMwREREVkHJjuWdPu2dnCyTAaEhhpsjo6LBgD4Ovqimls1MwdHRERkHZjsWJKuVcfNDahjuFjgsdvHAAC+Tr6wkfFXRURE9CT4CWpJumTHwwPw8jLYvP/6fgBAFXUVc0ZFRERkVZjsWJLuTuceHtqurHzupNzBpQTt9hfqvGDuyIiIiKwGkx1Lyt+yU4CuVcfXyRfda3Q3Z1RERERWhcmOJZUg2QlUB8JB4WDOqIiIiKwKkx1LyckBrl7Vft+0qcHmfdf3AQACXQPNGRUREZHVYbJjKdeuaW8CamsLtGqlt+l++n2cvXcWANC7luH9soiIiKjkmOxYSv4urALTzg9cPwAA8HLwQlhwmLkjIyIisipMdiwlf7Lj4qK3KX8XloeD4XgeIiIiKjkmO5aim3bu7m6wKf/gZCIiIiobJjuWcu2a9qubm15xYmaidJuI9lXbmzUkIiIia8Rkx1Ju3NB+LdCysztmNwQE3FXueL7W8xYIjIiIyLow2bGU2Fjt19q19YrXn1kPAKjlUQsB6gBzR0VERGR1mOxYQlISkJys/b5RI6k4JSsFv//7OwCggXcDS0RGRERkdZjsWIKuVUelAmrVkopPxJ1AZm4mXJQumNRqkmViIyIisjJMdixBN15HrQYqVZKKrz7Urqjs6eCJ2p61C9uTiIiISonJjiXoWnbUakCplIpjHsYAAFztXSErcBd0IiIiejJMdixB17JTYDHBq4nalh03e7eCexAREdETYrJjCfHx2q9OTnrFum4sJjtERETGw2THEnTJjqOjXrGuG4t3OiciIjIeJjuWcO+e9quDg1SUkZOBO6l3AABNfJtYIioiIiKrxGTHEnQtO/7+UtG1xGsAAKVciYY+DS0QFBERkXVismMJupadgP9WSJbG66jcEOQWZImoiIiIrBKTHXPLyvpv9eTgYKk4JvG/aedOdk6F7UlERERPgMmOueladWxsgKD/WnA4E4uIiMg0mOyYW/6ZWN7eUnH+lh0iIiIyHiY75qZLdhwcAA8PqZgtO0RERKbBZMfcdN1Yjo6ArS0AQAghrbFT06OmpSIjIiKySkx2zC0hQfs13xo7DzIeICU7BQDQzL+ZJaIiIiKyWkx2zC01VftVoZCKdF1YznbOvNs5ERGRkTHZMTddsmNnJxXpurDc7N0QqOatIoiIiIyJyY65FZLs6Fp2XFWuUMgVhe1FRERET8i2NJU1Gg327duHAwcO4Pr160hPT4eXlxeaNGmCzp07o0qVKqaK03oU1rKT+F/LDhERERlXiVp2MjIy8NFHH6FKlSro3r07tm3bhsTERMjlcly+fBkzZ85EUFAQunfvjsOHD5s65oqtmJYdJjtERETGV6KWnZo1ayI0NBRLly5Fly5doFAYdrVcv34da9euxcCBA/Hee+/h9ddfN3qwViEtTfu1kJYdXydfS0RERERk1UrUsrNjxw78+uuv6N69e6GJDgAEBgZi+vTpuHTpEjp27Gi0AG/duoVXXnkFHh4eUKlUaNCgAY4dOyZtF0JgxowZ8PPzg0qlQufOnXHp0iWjPb/RFWjZEULgVvItAEB97/qWioqIiMhqlSjZqVOnTokPqFAoEJzvBpdl8fDhQ7Rp0wYKhQLbtm3DuXPn8MUXX8DN7b/uns8++wwLFy7EDz/8gMjISDg6OiIsLAyZmZlGicHodMmOiwsAIDkrGVl5WQCAWp61LBUVERGR1SrVAOX8cnNzsXjxYuzduxd5eXlo06YNxo4dC3t7e6MF9+mnn6JKlSpYsWKFVBaU7+aZQggsWLAA77//Pnr37g0A+Omnn+Dj44MtW7Zg4MCBRovFaHTJjloNALibdhcAYCe347RzIiIiE3jiqecTJkzA5s2b0aFDB7Rr1w5r167F8OHDjRkbtm7diubNm6N///7w9vZGkyZNsHTpUml7TEwM4uLi0LlzZ6lMrVYjJCQEERERRR43KysLycnJeg+z0SU7j1qn7qZqkx0nOyf4OPmYLw4iIqKnRIlbdjZv3oy+fftKP+/YsQMXL16EXC4HAISFhaFVq1ZGDe7q1atYtGgRpkyZgnfffRdHjx7FhAkTYGdnh6FDhyIuLg4A4OOjnyT4+PhI2wozb948zJ4926ixlliK9rYQUrLzqGXHUeEIZztny8RERERkxUrcsrN8+XL06dMHt2/fBgA0bdoUo0ePxvbt2/H7779j2rRpaNGihVGD02g0aNq0KebOnYsmTZpg1KhReP311/HDDz+U6bjTp09HUlKS9Lhx44aRIn4MIYD0dO33j+54nr9lRyaTmScOIiKip0iJk53ff/8dgwYNQvv27fHNN99gyZIlcHFxwXvvvYcPPvgAVapUwdq1a40anJ+fH+rWratXVqdOHcTGxgIAfH21U7Xv3r2rV+fu3bvStsIolUq4uLjoPcwiMxPQaLTfP0p24lK1LVCOdo7miYGIiOgpU6oxOwMGDMCRI0dw+vRphIWF4ZVXXkFUVBSio6Px3XffwcvLy6jBtWnTBhcvXtQr+/fffxEYqB3IGxQUBF9fX4SHh0vbk5OTERkZidDQUKPGYhS68ToA4OkJQL8bi4iIiIyv1AOUXV1dsWTJEsyfPx9DhgzB22+/bbJp3pMnT8bhw4cxd+5cXL58GWvXrsWSJUswduxYAIBMJsOkSZPw0UcfYevWrTh9+jSGDBkCf39/9OnTxyQxlUn+O54XmI3lZOdkqaiIiIisWomTndjYWLz00kto0KABBg8ejBo1aiAqKgoODg5o1KgRtm3bZvTgWrRogc2bN2PdunWoX78+5syZgwULFmDw4MFSnWnTpmH8+PEYNWoUWrRogdTUVGzfvt2oU+CNJv+Cgk7a5EY3ZofdWERERKYhE0KIklRs3749fH19MWzYMPz999+4cuUKtm7dCgA4f/483njjDfj6+uLXX381acCmkJycDLVajaSkJNOO34mIAFq3BlxdgVu3AAcHBC8MxtWHVzG51WR8Gfal6Z6biIjIypT087vEU8+PHTuGkydPIjg4GGFhYXqL+9WpUwf79+/HkiVLyha1tct/XyyVCgDwIP0BAMDf2d9SUREREVm1Eic7zZo1w4wZMzB06FDs2rULDRo0MKgzatQoowZndfJ3Y8lkyMnLQVJWEgCgsktlCwZGRERkvUo8Zuenn35CVlYWJk+ejFu3bmHx4sWmjMs6FbjjeUJGgrQpQB1giYiIiIisXolbdgIDA7Fx40ZTxmL9dAsKPrpz/IMMbReWva09vB29LRUVERGRVStRy06arkWihEpb/6lRINm5n34fAOCgcIC7yt1SUREREVm1EiU71atXxyeffII7d+4UWUcIgZ07d6Jbt25YuHCh0QK0Krok0FbboKYbnOxg6wC1Um2pqIiIiKxaibqx9u7di3fffRezZs1Co0aN0Lx5c/j7+8Pe3h4PHz7EuXPnEBERAVtbW0yfPh1vvPGGqeOumHQtO4/G7Oi6sVQKFeQ2cktFRUREZNVKlOzUqlUL//d//4fY2Fhs2LABBw4cwD///IOMjAx4enqiSZMmWLp0Kbp16ybdBZ0KUUw3FhEREZlGiQcoA0BAQACmTp2KqVOnmioe61ZwgHL6fy07REREZBqlvjcWlYFuzI6uZSeDLTtERESmxmTHnIpo2XGwZbJDRERkKkx2zKmIdXbYskNERGQ6THbMqYgByhyzQ0REZDpMdsxJN2bHyQnAf91YPo4+loqIiIjI6pU62alatSo+/PBDxMbGmiIe66Zr2XFxQZ4mT7o3VhV1FQsGRUREZN1KnexMmjQJmzZtQrVq1dClSxesX78eWVlZpojN+uiSHWdnJGYmQkAAAALVgRYMioiIyLo9UbITHR2NI0eOoE6dOhg/fjz8/Pwwbtw4HD9+3BQxWg9dN5ZaLQ1OVsqV8HXytWBQRERE1u2Jx+w0bdoUCxcuxO3btzFz5kz8+OOPaNGiBRo3bozly5dDCGHMOK2DLtlxddUbnOzp4GnBoIiIiKxbqVZQzi8nJwebN2/GihUrsHPnTrRq1QojR47EzZs38e6772LXrl1Yu3atMWOt2IQAMjK037u5/bfGjsIBHg4eFgyMiIjIupU62Tl+/DhWrFiBdevWwcbGBkOGDMFXX32F2rVrS3X69u2LFi1aGDXQCi8z87/vXV1xP/0GAO2Cgs52zhYKioiIyPqVOtlp0aIFunTpgkWLFqFPnz5QPFozJr+goCAMHDjQKAFaDd3gZABwd8eDjGgA2m4smUxmmZiIiIieAqVOdq5evYrAwOJnDzk6OmLFihVPHJRV0o3XkcsBFxc8SODqyUREROZQ6gHK8fHxiIyMNCiPjIzEsWPHjBKUVcq/erKDA1dPJiIiMpNSJztjx47FjRs3DMpv3bqFsWPHGiUoq1Qg2ZHui8WbgBIREZlUqZOdc+fOoWnTpgblTZo0wblz54wSlFXKn+yoVLwJKBERkZmUOtlRKpW4e/euQfmdO3dga/vEM9mtn242lq0tYGfHbiwiIiIzKXWy89xzz2H69OlISkqSyhITE/Huu++iS5cuRg3OquhuqWFrC8hk0jo7vo5cPZmIiMiUSt0U8/nnn+PZZ59FYGAgmjRpAgCIjo6Gj48PVq9ebfQArUa+lh0hhNSNxZuAEhERmVapk51KlSrh1KlTWLNmDU6ePAmVSoXhw4dj0KBBha65Q4/kS3aSs5KRq8kFAAS5BlkwKCIiIuv3RINsHB0dMWrUKGPHYt3yJTu6Vh2FjQKV1ZUtGBQREZH1e+IRxefOnUNsbCyys7P1yp9//vkyB2WV8o3ZyT842cvBy4JBERERWb8nWkG5b9++OH36NGQymXR3c90tD/Ly8owbobXQtezI5Xo3AeUdz4mIiEyr1LOxJk6ciKCgIMTHx8PBwQFnz57F/v370bx5c+zdu9cEIVqJfN1YupYdB1sHKG2VFgyKiIjI+pW6ZSciIgK7d++Gp6cnbGxsYGNjg2eeeQbz5s3DhAkTcOLECVPEWfEVMmaHa+wQERGZXqlbdvLy8uDs7AwA8PT0xO3btwEAgYGBuHjxonGjsyb5xuzk78YiIiIi0yp1y079+vVx8uRJBAUFISQkBJ999hns7OywZMkSVKtWzRQxWod8Y3a4ejIREZH5lDrZef/995GWlgYA+PDDD9GzZ0+0bdsWHh4e+OWXX4weoNUopBuLLTtERESmV+pkJywsTPq+evXquHDhAhISEuDm5ibNyKJCFDFAmYiIiEyrVGN2cnJyYGtrizNnzuiVu7u7M9F5nPxjdh617Kjt1RYMiIiI6OlQqmRHoVAgICCAa+k8ifzdWI8GKFdyrmTBgIiIiJ4OpZ6N9d577+Hdd99FQkKCKeKxXo+SHWH73wDlyi68VQQREZGplXrMzrfffovLly/D398fgYGBcHR01Nt+/PhxowVnVR4lO+kKGbLytF1aVV2rWjAgIiKip0Opk50+ffqYIIynwKMxO/edtGOb5DI5W3aIiIjMoNTJzsyZM00Rh/V71LLz4NHSOiqFCl6OvAkoERGRqZV6zA49IV2y82i2uYPCAe4qdwsGRERE9HQodcuOjY1NsdPMOVOrCI+Snfv22rvEO9g6wNam1KefiIiISqnUn7abN2/W+zknJwcnTpzAqlWrMHv2bKMFZnV0LTvKPCCPt4ogIiIyl1InO7179zYoe/HFF1GvXj388ssvGDlypFECszqPkp2HCiY7RERE5mS0MTutWrVCeHi4sQ5nfR4lO4m2uQAAe7m9JaMhIiJ6ahgl2cnIyMDChQtRqRJXBC6UENLU84e22QAAewWTHSIiInModTdWwRt+CiGQkpICBwcH/Pzzz0YNzmrk5moTHgCJskfJji2THSIiInModbLz1Vdf6SU7NjY28PLyQkhICNzc3IwanNXQ3RcLwEORAQBQ2XLMDhERkTmUOtkZNmyYCcKwcnrJTjoAwNXe1ULBEBERPV1KPWZnxYoV2LBhg0H5hg0bsGrVKqMEZXV0yY5cjsTcFACAt6O3BQMiIiJ6epQ62Zk3bx48PT0Nyr29vTF37lyjBGV18iU7CdnJAAA/Jz8LBkRERPT0KHWyExsbi6CgIIPywMBAxMbGGiUoq5OhHaeTZ2eLlJxUAIC/s78lIyIiInpqlDrZ8fb2xqlTpwzKT548CQ8PD6MEZXXSteN0khz/GyJVxaWKpaIhIiJ6qpQ62Rk0aBAmTJiAPXv2IC8vD3l5edi9ezcmTpyIgQMHmiLGiu9Ry85DJ22yo7BRwNuJY3aIiIjModSzsebMmYNr166hU6dOsLXV7q7RaDBkyBCO2SnKo5adRAdtbmlvaw83e07TJyIiModSJzt2dnb45Zdf8NFHHyE6OhoqlQoNGjRAYGCgKeKzDrqWHUdtsqNSqOCsdLZkRERERE+NUic7OjVq1ECNGjWMGYv1epTsJKq0izHay+1hIzPabcmIiIioGKX+xO3Xrx8+/fRTg/LPPvsM/fv3N0pQVudRN9ZDXbLD+2IRERGZTamTnf3796N79+4G5d26dcP+/fuNEpTV0bXs2Gvvj8X7YhEREZlPqZOd1NRU2NnZGZQrFAokJycbJaiifPLJJ5DJZJg0aZJUlpmZibFjx8LDwwNOTk7o168f7t69a9I4Sk3XsvMox2GyQ0REZD6lTnYaNGiAX375xaB8/fr1qFu3rlGCKszRo0exePFiNGzYUK988uTJ+P3337Fhwwbs27cPt2/fxgsvvGCyOJ6IboCyUgOANwElIiIyp1IPUP7ggw/wwgsv4MqVK+jYsSMAIDw8HOvWrSv0nlnGkJqaisGDB2Pp0qX46KOPpPKkpCQsW7YMa9eulWJZsWIF6tSpg8OHD6NVq1YmiafUdFPP7diNRUREZG6lbtnp1asXtmzZgsuXL2PMmDGYOnUqbt68iV27dqFPnz4mCBEYO3YsevTogc6dO+uVR0VFIScnR6+8du3aCAgIQERERJHHy8rKQnJyst7DpHQtO3Z5ANiyQ0REZE5PNPW8R48e6NGjh0H5mTNnUL9+/TIHld/69etx/PhxHD161GBbXFwc7Ozs4Orqqlfu4+ODuLi4Io85b948zJ4926hxFkvXsqPQJjueDoY3UiUiIiLTKPNiLykpKViyZAlatmyJRo0aGSMmyY0bNzBx4kSsWbMG9vbG6/qZPn06kpKSpMeNGzeMduxCPWrZSVDkAgB8nX1N+3xEREQkeeJkZ//+/RgyZAj8/Pzw+eefo2PHjjh8+LAxY0NUVBTi4+PRtGlT2NrawtbWFvv27cPChQtha2sLHx8fZGdnIzExUW+/u3fvwte36IRCqVTCxcVF72FSum4s2xwAgL8T73hORERkLqXqxoqLi8PKlSuxbNkyJCcn46WXXkJWVha2bNlikplYnTp1wunTp/XKhg8fjtq1a+N///sfqlSpAoVCgfDwcPTr1w8AcPHiRcTGxiI0NNTo8Tyx9HQIAElybcsO73hORERkPiVOdnr16oX9+/ejR48eWLBgAbp27Qq5XI4ffvjBZME5OzsbjAFydHSEh4eHVD5y5EhMmTIF7u7ucHFxwfjx4xEaGlp+ZmIBQEYG0hVAjo12NlaAOsDCARERET09SpzsbNu2DRMmTMCbb75Zru6J9dVXX8HGxgb9+vVDVlYWwsLC8P3331s6LH3p6Uh8NORIBhmqqNmyQ0REZC4lTnYOHjyIZcuWoVmzZqhTpw5effVVDBw40JSxFWrv3r16P9vb2+O7777Dd999Z/ZYSiwjAw8fzTZXKVRQ26stGw8REdFTpMQDlFu1aoWlS5fizp07eOONN7B+/Xr4+/tDo9Fg586dSElJMWWcFVu+lh17W97xnIiIyJxK/anr6OiIESNG4ODBgzh9+jSmTp2KTz75BN7e3nj++edNEWPFl5HB+2IRERFZSJmaGGrVqoXPPvsMN2/exLp164wVk/XJ343F1ZOJiIjMyij9KXK5HH369MHWrVuNcTjrIoRBNxYRERGZDwePmFp2NiAEu7GIiIgshMmOqenui8Vkh4iIyCKY7Jia7lYRj4bqMNkhIiIyLyY7pvaoZeehgwwAkx0iIiJzY7Jjao9adhJV2mTH2c7ZktEQERE9dZjsmJquZedRg46ng6cFgyEiInr6MNkxNV3LzqNkx8vRy4LBEBERPX2Y7JiaboCyUnvHc18nX0tGQ0RE9NRhsmNq6enItQFS7bTJjp+Tn4UDIiIierow2TG1jAypCwsAKrlUslwsRERETyEmO6aWni4NTraT28FD5WHZeIiIiJ4yTHZMLV/Ljr2tPVztXS0aDhER0dOGyY6ppafr3fHcWcl1doiIiMyJyY6pFWjZsZHxlBMREZkTP3lNLd+YHd4qgoiIyPyY7JhaRgZvAkpERGRBTHZMrUA3FhEREZkXkx1Ty9eNpbJVWTYWIiKipxCTHVNjyw4REZFFMdkxtXxTz+0VTHaIiIjMjcmOqeVr2XFSOFk2FiIioqcQkx1Tyzdmx8vBy7KxEBERPYWY7Jhavqnnvs6+lo2FiIjoKcRkx8RERrrUjeXrxGSHiIjI3JjsmFhGdjryHp1lf2d/ywZDRET0FGKyY2KZORnS9+4qdwtGQkRE9HRismNKQiArOx0AIAPgqHC0bDxERERPISY7ppSTgywbAQCwhRxKW6WFAyIiInr6MNkxpfR0ZMm138plcijlTHaIiIjMjcmOKWVkIMtW+63cRs7bRRAREVkAkx1TyteyY2vDbiwiIiJLYLJjSnotO7bsxiIiIrIAJjumlJGRr2XHFnIbuWXjISIiegox2TGl9HS9lh0iIiIyPyY7ppS/ZUfOZIeIiMgSmOyYElt2iIiILI7Jjinlb9mRMdkhIiKyBCY7pqTXssPByURERJbAZMeUCszGIiIiIvNjsmNKBVZQJiIiIvNjsmNKBe6NRURERObHZMeU8rXssBuLiIjIMpjsmFL+lh12YxEREVkEkx1Tyt+yw6nnREREFsFkx5TYskNERGRxTHZMiWN2iIiILI7JjimxZYeIiMjimOyYEsfsEBERWRyTHVPKt4IyW3aIiIgsg8mOKeW7NxbH7BAREVkGkx1TcnNDlp32FNvb2ls4GCIioqcTkx1TOnQIWa1aAABUtioLB0NERPR0YrJjYll5WQAAlYLJDhERkSUw2TGxrNxHyQ5bdoiIiCyCyY6JZeZmAgAcFA4WjoSIiOjpxGTHxKRkx47JDhERkSUw2TEx3ZgdR1tHC0dCRET0dGKyY2LZedkAAEc7JjtERESWwGTHxHQDlDlmh4iIyDLKdbIzb948tGjRAs7OzvD29kafPn1w8eJFvTqZmZkYO3YsPDw84OTkhH79+uHu3bsWilifEAI5mhwAgJOdk4WjISIiejqV62Rn3759GDt2LA4fPoydO3ciJycHzz33HNLS0qQ6kydPxu+//44NGzZg3759uH37Nl544QULRv0fXRcWwGSHiIjIUsr1DZu2b9+u9/PKlSvh7e2NqKgoPPvss0hKSsKyZcuwdu1adOzYEQCwYsUK1KlTB4cPH0arVq0sEbZENzgZYLJDRERkKeW6ZaegpKQkAIC7uzsAICoqCjk5OejcubNUp3bt2ggICEBERESRx8nKykJycrLewxR043UAJjtERESWUmGSHY1Gg0mTJqFNmzaoX78+ACAuLg52dnZwdXXVq+vj44O4uLgijzVv3jyo1WrpUaVKFZPErGvZsZHZ8HYRREREFlJhkp2xY8fizJkzWL9+fZmPNX36dCQlJUmPGzduGCFCQ7qWHblMDqVcaZLnICIiouKV6zE7OuPGjcMff/yB/fv3o3LlylK5r68vsrOzkZiYqNe6c/fuXfj6+hZ5PKVSCaXS9MmHrmXH1sYWSlsmO0RERJZQrlt2hBAYN24cNm/ejN27dyMoKEhve7NmzaBQKBAeHi6VXbx4EbGxsQgNDTV3uAaklh0bOezkdhaOhoiI6OlUrlt2xo4di7Vr1+K3336Ds7OzNA5HrVZDpVJBrVZj5MiRmDJlCtzd3eHi4oLx48cjNDTU4jOxgAItO+zGIiIisohynewsWrQIANC+fXu98hUrVmDYsGEAgK+++go2Njbo168fsrKyEBYWhu+//97MkRYu/5gduY3cwtEQERE9ncp1siOEeGwde3t7fPfdd/juu+/MEFHp5G/ZISIiIsso12N2Krr8Y3aIiIjIMpjsmJDUsiNjyw4REZGlMNkxIbbsEBERWR6THRPimB0iIiLLY7JjQmzZISIisjwmOybEMTtERESWx2THhNiyQ0REZHlMdkxI17LDZIeIiMhymOyYkK5lhwOUiYiILIfJjglJLTsytuwQERFZCpMdE2LLDhERkeUx2TEhjtkhIiKyPCY7JsSp50RERJbHT2ET4tRzIjK2vLw85OTkWDoMIrNQKBSQy8v+Gcpkx4R4uwgiMhYhBOLi4pCYmGjpUIjMytXVFb6+vpDJZE98DH4KmxBbdojIWHSJjre3NxwcHMr0h5+oIhBCID09HfHx8QAAPz+/Jz4Wkx0T4pgdIjKGvLw8KdHx8PCwdDhEZqNSqQAA8fHx8Pb2fuIuLQ5QNiFdy46d3M7CkRBRRaYbo+Pg4GDhSIjMT3fdl2WsGpMdE9K17Njb2ls4EiKyBuy6oqeRMa57JjsmpGvZUSlUFo6EiOjpMWzYMPTp00f6uX379pg0aZLF4iHLY7JjQrqWHZUtkx0iejrFxcVh4sSJqF69Ouzt7eHj44M2bdpg0aJFSE9PN0sMmzZtwpw5c4x6zIIJVXH1ZDIZZDIZFAoFfHx80KVLFyxfvhwajcaoMZnarFmz0LhxY0uH8UQ4ctaEhjYaii0XtqBHzR6WDoWIyOyuXr2KNm3awNXVFXPnzkWDBg2gVCpx+vRpLFmyBJUqVcLzzz9f6L45OTlQKBRGicPd3d0ox3lSXbt2xYoVK5CXl4e7d+9i+/btmDhxIjZu3IitW7fC1pYfxSYnSCQlJQkAIikpydKhEBEZyMjIEOfOnRMZGRmWDqVUwsLCROXKlUVqamqh2zUajfQ9APH999+LXr16CQcHBzFz5kyRm5srRowYIapWrSrs7e1FzZo1xYIFC/SOkZubKyZPnizUarVwd3cXb7/9thgyZIjo3bu3VKddu3Zi4sSJ0s+ZmZli6tSpwt/fXzg4OIiWLVuKPXv2SNtXrFgh1Gq12L59u6hdu7ZwdHQUYWFh4vbt20IIIWbOnCkA6D3y75/f0KFD9WLRCQ8PFwDE0qVLpbKHDx+KkSNHCk9PT+Hs7Cw6dOggoqOjpe3R0dGiffv2wsnJSTg7O4umTZuKo0ePStsPHjwo2rVrJ1QqlXB1dRXPPfecSEhIEEIIkZeXJ+bOnSudy4YNG4oNGzZI++7Zs0cAELt27RLNmjUTKpVKhIaGigsXLkjnpOBrXrFiRaGv2diKu/5L+vnNbiwioopGCCAtzTIPIUoU4oMHD7Bjxw6MHTsWjo6OhdYpOPB01qxZ6Nu3L06fPo0RI0ZAo9GgcuXK2LBhA86dO4cZM2bg3Xffxa+//irt88UXX2DlypVYvnw5Dh48iISEBGzevLnY2MaNG4eIiAisX78ep06dQv/+/dG1a1dcunRJqpOeno7PP/8cq1evxv79+xEbG4u33noLAPDWW2/hpZdeQteuXXHnzh3cuXMHrVu3LtF50enYsSMaNWqETZs2SWX9+/dHfHw8tm3bhqioKDRt2hSdOnVCQkICAGDw4MGoXLkyjh49iqioKLzzzjtS61d0dDQ6deqEunXrIiIiAgcPHkSvXr2Ql5cHAJg3bx5++ukn/PDDDzh79iwmT56MV155Bfv27dOL67333sMXX3yBY8eOwdbWFiNGjAAADBgwAFOnTkW9evWk1zxgwIBSvWaLMlUmVpGwZYeIyjOD/2xTU4XQph3mfxTRSlPQ4cOHBQCxadMmvXIPDw/h6OgoHB0dxbRp06RyAGLSpEmPPe7YsWNFv379pJ/9/PzEZ599Jv2ck5MjKleuXGTLzvXr14VcLhe3bt3SO26nTp3E9OnThRD/tWJcvnxZ2v7dd98JHx8f6eeiWmwKKq7egAEDRJ06dYQQQhw4cEC4uLiIzMxMvTrBwcFi8eLFQgghnJ2dxcqVKws91qBBg0SbNm0K3ZaZmSkcHBzEP//8o1c+cuRIMWjQICGEfsuOzp9//ikASNfdzJkzRaNGjYp/wSZgjJYddhQSEZHZHDlyBBqNBoMHD0ZWVpbetubNmxvU/+6777B8+XLExsYiIyMD2dnZ0iDZpKQk3LlzByEhIVJ9W1tbNG/eHKKIFqjTp08jLy8PNWvW1CvPysrSW7DRwcEBwcHB0s9+fn7SSr7GIoSQWrdOnjyJ1NRUg0UjMzIycOXKFQDAlClT8Nprr2H16tXo3Lkz+vfvL8UYHR2N/v37F/o8ly9fRnp6Orp06aJXnp2djSZNmuiVNWzYUPpet2JxfHw8AgICyvBKLY/JDhFRRePgAKSmWu65S6B69eqQyWS4ePGiXnm1atUA/Lcybn4Fu7vWr1+Pt956C1988QVCQ0Ph7OyM+fPnIzIy8gmDB1JTUyGXyxEVFWWwGq+Tk5P0fcHB0TKZrMgE6kmdP38eQUFBUlx+fn7Yu3evQT1XV1cA2m6+l19+GX/++Se2bduGmTNnYv369ejbt2+h51Mn9dG18ueff6JSpUp625RKpd7P+V+3LhGraLPGCsNkh4ioopHJgCLGwZQXHh4e6NKlC7799luMHz++yHE7xTl06BBat26NMWPGSGW6Vg4AUKvV8PPzQ2RkJJ599lkAQG5urjTepTBNmjRBXl4e4uPj0bZt21LHpGNnZyeNh3kSu3fvxunTpzF58mQAQNOmTREXFwdbW1tUrVq1yP1q1qyJmjVrYvLkyRg0aBBWrFiBvn37omHDhggPD8fs2bMN9qlbty6USiViY2PRrl27J465rK/ZkjhAmYiITOL7779Hbm4umjdvjl9++QXnz5/HxYsX8fPPP+PChQuPvc9RjRo1cOzYMfz999/4999/8cEHH+Do0aN6dSZOnIhPPvkEW7ZswYULFzBmzJhi7wxfs2ZNDB48GEOGDMGmTZsQExODI0eOYN68efjzzz9L/NqqVq2KU6dO4eLFi7h//36xtzLIyspCXFwcbt26hePHj2Pu3Lno3bs3evbsiSFDhgAAOnfujNDQUPTp0wc7duzAtWvX8M8//+C9997DsWPHkJGRgXHjxmHv3r24fv06Dh06hKNHj6JOnToAgOnTp+Po0aMYM2YMTp06hQsXLmDRokW4f/8+nJ2d8dZbb2Hy5MlYtWoVrly5guPHj+Obb77BqlWrSvWaY2JiEB0djfv37xt0Q5ZrJhlNVMFwgDIRlWcVdeq5EELcvn1bjBs3TgQFBQmFQiGcnJxEy5Ytxfz580VaWppUD4DYvHmz3r6ZmZli2LBhQq1WC1dXV/Hmm2+Kd955R2+QbE5Ojpg4caJwcXERrq6uYsqUKY+dep6dnS1mzJghqlatKhQKhfDz8xN9+/YVp06dEkL8N/U8v82bN4v8H5nx8fGiS5cuwsnJ6bFTz/Foqratra3w8vISnTt3FsuXLxd5eXl6dZOTk8X48eOFv7+/UCgUokqVKmLw4MEiNjZWZGVliYEDB4oqVaoIOzs74e/vL8aNG6d3Tezdu1e0bt1aKJVK4erqKsLCwsTDhw+FENpp/gsWLBC1atUSCoVCeHl5ibCwMLFv3z4hxH8DlHX1hRDixIkTAoCIiYmRfh/9+vUTrq6uFW7quUwII3dCVkDJyclQq9VISkqCi4uLpcMhItKTmZmJmJgYBAUFwd6e99qjp0tx139JP7/ZjUVERERWjckOERERWTUmO0RERGTVmOwQERGRVWOyQ0RERFaNyQ4RERFZNSY7REREZNWY7BAREZFVY7JDREREVo3JDhERWY1Zs2ahcePG0s/Dhg1Dnz59zPqcpjquOV6LtWKyQ0REJjFs2DDIZDKDx+XLly0dWrkxb948yOVyzJ8//7F1v/76a6xcudL0QZnJypUr4erqapbnYrJDREQm07VrV9y5c0fvERQUZOmwyo3ly5dj2rRpWL58+WPrqtVqsyUH1obJDhERmYxSqYSvr6/eQy6XAwB+++03NG3aFPb29qhWrRpmz56N3Nxcad/ExES89tpr8PLygouLCzp27IiTJ0/qHf+TTz6Bj48PnJ2dMXLkSGRmZhYax+zZs6XjjB49GtnZ2dK27du345lnnoGrqys8PDzQs2dPXLlyRW//mzdvYtCgQXB3d4ejoyOaN2+OyMjIQp/rypUrqFatGsaNG4fi7rW9b98+ZGRk4MMPP0RycjL++eefYs9lwW6slJQUDB48GI6OjvDz88NXX32F9u3bY9KkSVKdqlWrYu7cuRgxYgScnZ0REBCAJUuWSNuvXbsGmUyGX3/9FW3btoVKpUKLFi3w77//4ujRo2jevDmcnJzQrVs33Lt3Ty+eH3/8EXXq1IG9vT1q166N77//3uC4mzZtQocOHeDg4IBGjRohIiICALB3714MHz4cSUlJUovfrFmzin39ZcFkh4ioghFCIC07zSKP4j68S+PAgQMYMmQIJk6ciHPnzmHx4sVYuXIlPv74Y6lO//79ER8fj23btiEqKgpNmzZFp06dkJCQAAD49ddfMWvWLMydOxfHjh2Dn5+f3geuTnh4OM6fP4+9e/di3bp12LRpE2bPni1tT0tLw5QpU3Ds2DGEh4fDxsYGffv2hUajAQCkpqaiXbt2uHXrFrZu3YqTJ09i2rRp0vb8Tp06hWeeeQYvv/wyvv32W8hksiLPwbJlyzBo0CAoFAoMGjQIy5YtK9U5nDJlCg4dOoStW7di586dOHDgAI4fP25Q74svvkDz5s1x4sQJjBkzBm+++SYuXryoV2fmzJl4//33cfz4cdja2uLll1/GtGnT8PXXX+PAgQO4fPkyZsyYIdVfs2YNZsyYgY8//hjnz5/H3Llz8cEHH2DVqlV6x33vvffw1ltvITo6GjVr1sSgQYOQm5uL1q1bY8GCBXBxcZFa/N56661Svf5SESSSkpIEAJGUlGTpUIiIDGRkZIhz586JjIwMIYQQqVmpArNgkUdqVmqJ4x46dKiQy+XC0dFRerz44otCCCE6deok5s6dq1d/9erVws/PTwghxIEDB4SLi4vIzMzUqxMcHCwWL14shBAiNDRUjBkzRm97SEiIaNSokV4M7u7uIi0tTSpbtGiRcHJyEnl5eYXGfe/ePQFAnD59WgghxOLFi4Wzs7N48OBBofVnzpwpGjVqJA4dOiTc3NzE559//rhTI5KSkoRKpRLR0dFCCCFOnDghnJycREpKisFx87+W3r17CyGESE5OFgqFQmzYsEHanpiYKBwcHMTEiROlssDAQPHKK69IP2s0GuHt7S0WLVokhBAiJiZGABA//vijVGfdunUCgAgPD5fK5s2bJ2rVqiX9HBwcLNauXav3mubMmSNCQ0OLPO7Zs2cFAHH+/HkhhBArVqwQarX6seeq4PWfX0k/v21Nl0YREdHTrkOHDli0aJH0s6OjIwDg5MmTOHTokF5LTl5eHjIzM5Geno6TJ08iNTUVHh4eesfLyMiQupjOnz+P0aNH620PDQ3Fnj179MoaNWoEBwcHvTqpqam4ceMGAgMDcenSJcyYMQORkZG4f/++1GITGxuL+vXrIzo6Gk2aNIG7u3uRrzM2NhZdunTBxx9/rNeNVJR169YhODgYjRo1AgA0btwYgYGB+OWXXzBy5MjH7n/16lXk5OSgZcuWUplarUatWrUM6jZs2FD6XiaTwdfXF/Hx8UXW8fHxAQA0aNBAr0y3T1paGq5cuYKRI0fi9ddfl+rk5uZCrVYXeVw/Pz8AQHx8PGrXrv3Y12hMTHaIiCoYB4UDUqenWuy5S8PR0RHVq1c3KE9NTcXs2bPxwgsvGGyzt7dHamoq/Pz8sHfvXoPtxh6k26tXLwQGBmLp0qXw9/eHRqNB/fr1pXE9KpXqscfw8vKCv78/1q1bhxEjRsDFxaXY+suWLcPZs2dha/vfx7BGo8Hy5ctLlOyUhkKh0PtZJpMZdMHlr6PreitYlr9bDwCWLl2KkJAQvePoxmMVd9zCuv9MjckOEVEFI5PJ4GjnaOkwyqRp06a4ePFioYmQbntcXBxsbW1RtWrVQuvUqVMHkZGRGDJkiFR2+PBhg3onT55ERkaGlLQcPnwYTk5OqFKlCh48eICLFy9i6dKlaNu2LQDg4MGDevs3bNgQP/74IxISEops3VGpVPjjjz/QvXt3hIWFYceOHXB2di607unTp3Hs2DHs3btX73gJCQlo3749Lly48NiWj2rVqkGhUODo0aMICAgAACQlJeHff//Fs88+W+y+ZeXj4wN/f39cvXoVgwcPfuLj2NnZIS8vz4iRFY3JDhERmd2MGTPQs2dPBAQE4MUXX4SNjQ1OnjyJM2fO4KOPPkLnzp0RGhqKPn364LPPPkPNmjVx+/Zt/Pnnn+jbty+aN2+OiRMnYtiwYWjevDnatGmDNWvW4OzZs6hWrZrec2VnZ2PkyJF4//33ce3aNcycORPjxo2DjY0N3Nzc4OHhgSVLlsDPzw+xsbF455139PYfNGgQ5s6diz59+mDevHnw8/PDiRMn4O/vj9DQUKmeo6Mj/vzzT3Tr1g3dunXD9u3b4eTkZPDaly1bhpYtWxaalLRo0QLLli177Lo7zs7OGDp0KN5++224u7vD29sbM2fOhI2NTbGDoo1l9uzZmDBhAtRqNbp27YqsrCwcO3YMDx8+xJQpU0p0jKpVqyI1NRXh4eFSV2P+7kZj4mwsIiIyu7CwMPzxxx/YsWMHWrRogVatWuGrr75CYGAgAG3r1V9//YVnn30Ww4cPR82aNTFw4EBcv35dGlMyYMAAfPDBB5g2bRqaNWuG69ev48033zR4rk6dOqFGjRp49tlnMWDAADz//PPSNGcbGxusX78eUVFRqF+/PiZPnmyQaNjZ2WHHjh3w9vZG9+7d0aBBA3zyyScGXTYA4OTkhG3btkEIgR49eiAtLU1ve3Z2Nn7++Wf069ev0PPSr18//PTTT8jJyXnsOfzyyy8RGhqKnj17onPnzmjTpo00FdzUXnvtNfz4449YsWIFGjRogHbt2mHlypWlWkOpdevWGD16NAYMGAAvLy989tlnJotXJoSR5hFWYMnJyVCr1UhKSnpsPysRkbllZmYiJiYGQUFBZvkgo4opLS0NlSpVwhdffGH0cT+WVNz1X9LPb3ZjERERVUAnTpzAhQsX0LJlSyQlJeHDDz8EAPTu3dvCkZU/THaIiIgqqM8//xwXL16EnZ0dmjVrhgMHDsDT09PSYZU7THaIiIgqoCZNmiAqKsrSYVQIHKBMREREVo3JDhEREVk1JjtERBUEJ8/S08gY1z2THSKick635H56erqFIyEyP911X/C2F6XBAcpEROWcXC6Hq6urdCNGBwcHs6ySS2RJQgikp6cjPj4erq6uhS7iWFJMdoiIKgBfX18AMLhbNZG1c3V1la7/J8Vkh4ioApDJZPDz84O3t3eJbiVAZA0UCkWZWnR0rCbZ+e677zB//nzExcWhUaNG+Oabb9CyZUtLh0VEZFRyudwof/yJniZWMUD5l19+wZQpUzBz5kwcP34cjRo1QlhYGJt7iYiIyDqSnS+//BKvv/46hg8fjrp16+KHH36Ag4MDli9fbunQiIiIyMIqfLKTnZ2NqKgodO7cWSqzsbFB586dERERYcHIiIiIqDyo8GN27t+/j7y8PPj4+OiV+/j44MKFC4Xuk5WVhaysLOnnpKQkANpbxRMREVHFoPvcftzCgxU+2XkS8+bNw+zZsw3Kq1SpYoFoiIiIqCxSUlKgVquL3F7hkx1PT0/I5XLcvXtXr/zu3btFzsufPn06pkyZIv2s0WiQkJAADw8Poy7UlZycjCpVquDGjRtwcXEx2nHJEM+1efA8mwfPs/nwXJuHqc6zEAIpKSnw9/cvtl6FT3bs7OzQrFkzhIeHo0+fPgC0yUt4eDjGjRtX6D5KpRJKpVKvzNXV1WQxuri48E1kJjzX5sHzbB48z+bDc20epjjPxbXo6FT4ZAcApkyZgqFDh6J58+Zo2bIlFixYgLS0NAwfPtzSoREREZGFWUWyM2DAANy7dw8zZsxAXFwcGjdujO3btxsMWiYiIqKnj1UkOwAwbty4IrutLEWpVGLmzJkGXWZkfDzX5sHzbB48z+bDc20elj7PMvG4+VpEREREFViFX1SQiIiIqDhMdoiIiMiqMdkhIiIiq8Zkh4iIiKwakx0T+u6771C1alXY29sjJCQER44csXRIFcr+/fvRq1cv+Pv7QyaTYcuWLXrbhRCYMWMG/Pz8oFKp0LlzZ1y6dEmvTkJCAgYPHgwXFxe4urpi5MiRSE1NNeOrKP/mzZuHFi1awNnZGd7e3ujTpw8uXryoVyczMxNjx46Fh4cHnJyc0K9fP4NVy2NjY9GjRw84ODjA29sbb7/9NnJzc835Usq1RYsWoWHDhtKiaqGhodi2bZu0nefYND755BPIZDJMmjRJKuO5No5Zs2ZBJpPpPWrXri1tL1fnWZBJrF+/XtjZ2Ynly5eLs2fPitdff124urqKu3fvWjq0CuOvv/4S7733nti0aZMAIDZv3qy3/ZNPPhFqtVps2bJFnDx5Ujz//PMiKChIZGRkSHW6du0qGjVqJA4fPiwOHDggqlevLgYNGmTmV1K+hYWFiRUrVogzZ86I6Oho0b17dxEQECBSU1OlOqNHjxZVqlQR4eHh4tixY6JVq1aidevW0vbc3FxRv3590blzZ3HixAnx119/CU9PTzF9+nRLvKRyaevWreLPP/8U//77r7h48aJ49913hUKhEGfOnBFC8BybwpEjR0TVqlVFw4YNxcSJE6VynmvjmDlzpqhXr564c+eO9Lh37560vTydZyY7JtKyZUsxduxY6ee8vDzh7+8v5s2bZ8GoKq6CyY5GoxG+vr5i/vz5UlliYqJQKpVi3bp1Qgghzp07JwCIo0ePSnW2bdsmZDKZuHXrltlir2ji4+MFALFv3z4hhPa8KhQKsWHDBqnO+fPnBQAREREhhNAmpjY2NiIuLk6qs2jRIuHi4iKysrLM+wIqEDc3N/Hjjz/yHJtASkqKqFGjhti5c6do166dlOzwXBvPzJkzRaNGjQrdVt7OM7uxTCA7OxtRUVHo3LmzVGZjY4POnTsjIiLCgpFZj5iYGMTFxemdY7VajZCQEOkcR0REwNXVFc2bN5fqdO7cGTY2NoiMjDR7zBVFUlISAMDd3R0AEBUVhZycHL1zXbt2bQQEBOid6wYNGuitWh4WFobk5GScPXvWjNFXDHl5eVi/fj3S0tIQGhrKc2wCY8eORY8ePfTOKcDr2dguXboEf39/VKtWDYMHD0ZsbCyA8neerWYF5fLk/v37yMvLM7hdhY+PDy5cuGChqKxLXFwcABR6jnXb4uLi4O3trbfd1tYW7u7uUh3Sp9FoMGnSJLRp0wb169cHoD2PdnZ2BjfLLXiuC/td6LaR1unTpxEaGorMzEw4OTlh8+bNqFu3LqKjo3mOjWj9+vU4fvw4jh49arCN17PxhISEYOXKlahVqxbu3LmD2bNno23btjhz5ky5O89MdohIMnbsWJw5cwYHDx60dChWqVatWoiOjkZSUhI2btyIoUOHYt++fZYOy6rcuHEDEydOxM6dO2Fvb2/pcKxat27dpO8bNmyIkJAQBAYG4tdff4VKpbJgZIbYjWUCnp6ekMvlBqPO7969C19fXwtFZV1057G4c+zr64v4+Hi97bm5uUhISODvoRDjxo3DH3/8gT179qBy5cpSua+vL7Kzs5GYmKhXv+C5Lux3odtGWnZ2dqhevTqaNWuGefPmoVGjRvj66695jo0oKioK8fHxaNq0KWxtbWFra4t9+/Zh4cKFsLW1hY+PD8+1ibi6uqJmzZq4fPlyubummeyYgJ2dHZo1a4bw8HCpTKPRIDw8HKGhoRaMzHoEBQXB19dX7xwnJycjMjJSOsehoaFITExEVFSUVGf37t3QaDQICQkxe8zllRAC48aNw+bNm7F7924EBQXpbW/WrBkUCoXeub548SJiY2P1zvXp06f1ksudO3fCxcUFdevWNc8LqYA0Gg2ysrJ4jo2oU6dOOH36NKKjo6VH8+bNMXjwYOl7nmvTSE1NxZUrV+Dn51f+rmmjDncmyfr164VSqRQrV64U586dE6NGjRKurq56o86peCkpKeLEiRPixIkTAoD48ssvxYkTJ8T169eFENqp566uruK3334Tp06dEr179y506nmTJk1EZGSkOHjwoKhRowannhfw5ptvCrVaLfbu3as3hTQ9PV2qM3r0aBEQECB2794tjh07JkJDQ0VoaKi0XTeF9LnnnhPR0dFi+/btwsvLi1N183nnnXfEvn37RExMjDh16pR45513hEwmEzt27BBC8BybUv7ZWELwXBvL1KlTxd69e0VMTIw4dOiQ6Ny5s/D09BTx8fFCiPJ1npnsmNA333wjAgIChJ2dnWjZsqU4fPiwpUOqUPbs2SMAGDyGDh0qhNBOP//ggw+Ej4+PUCqVolOnTuLixYt6x3jw4IEYNGiQcHJyEi4uLmL48OEiJSXFAq+m/CrsHAMQK1askOpkZGSIMWPGCDc3N+Hg4CD69u0r7ty5o3eca9euiW7dugmVSiU8PT3F1KlTRU5OjplfTfk1YsQIERgYKOzs7ISXl5fo1KmTlOgIwXNsSgWTHZ5r4xgwYIDw8/MTdnZ2olKlSmLAgAHi8uXL0vbydJ5lQghh3LYiIiIiovKDY3aIiIjIqjHZISIiIqvGZIeIiIisGpMdIiIismpMdoiIiMiqMdkhIiIiq8Zkh4iIiKwakx0iIgAymQxbtmyxdBhEZAJMdojI4oYNGwaZTGbw6Nq1q6VDIyIrYGvpAIiIAKBr165YsWKFXplSqbRQNERkTdiyQ0TlglKphK+vr97Dzc0NgLaLadGiRejWrRtUKhWqVauGjRs36u1/+vRpdOzYESqVCh4eHhg1ahRSU1P16ixfvhz16tWDUqmEn58fxo0bp7f9/v376Nu3LxwcHFCjRg1s3bpV2vbw4UMMHjwYXl5eUKlUqFGjhkFyRkTlE5MdIqoQPvjgA/Tr1w8nT57E4MGDMXDgQJw/fx4AkJaWhrCwMLi5ueHo0aPYsGEDdu3apZfMLFq0CGPHjsWoUaNw+vRpbN26FdWrV9d7jtmzZ+Oll17CqVOn0L17dwwePBgJCQnS8587dw7btm3D+fPnsWjRInh6eprvBBDRkzP6rUWJiEpp6NChQi6XC0dHR73Hxx9/LITQ3pl99OjRevuEhISIN998UwghxJIlS4Sbm5tITU2Vtv/555/CxsZGxMXFCSGE8Pf3F++9916RMQAQ77//vvRzamqqACC2bdsmhBCiV69eYvjw4cZ5wURkVhyzQ0TlQocOHbBo0SK9Mnd3d+n70NBQvW2hoaGIjo4GAJw/fx6NGjWCo6OjtL1NmzbQaDS4ePEiZDIZbt++jU6dOhUbQ8OGDaXvHR0d4eLigvj4eADAm2++iX79+uH48eN47rnn0KdPH7Ru3fqJXisRmReTHSIqFxwdHQ26lYxFpVKVqJ5CodD7WSaTQaPRAAC6deuG69ev46+//sLOnTvRqVMnjB07Fp9//rnR4yUi4+KYHSKqEA4fPmzwc506dQAAderUwcmTJ5GWliZtP3ToEGxsbFCrVi04OzujatWqCA8PL1MMXl5eGDp0KH7++WcsWLAAS5YsKdPxiMg82LJDROVCVlYW4uLi9MpsbW2lQcAbNmxA8+bN8cwzz2DNmjU4cuQIli1bBgAYPHgwZs6ciaFDh2LWrFm4d+8exo8fj1dffRU+Pj4AgFmzZmH06NHw9vZGt27dkJKSgkOHDmH8+PElim/GjBlo1qwZ6tWrh6ysLPzxxx9SskVE5RuTHSIqF7Zv3w4/Pz+9slq1auHChQsAtDOl1q9fjzFjxsDPzw/r1q1D3bp1AQAODg74+++/MXHiRLRo0QIODg7o168fvvzyS+lYQ4cORWZmJr766iu89dZb8PT0xIsvvlji+Ozs7DB9+nRcu3YNKpUKbdu2xfr1643wyonI1GRCCGHpIIiIiiOTybB582b06dPH0qEQUQXEMTtERERk1ZjsEBERkVXjmB0iKvfY205EZcGWHSIiIrJqTHaIiIjIqjHZISIiIqvGZIeIiIisGpMdIiIismpMdoiIiMiqMdkhIiIiq8Zkh4iIiKwakx0iIiKyav8PL8oC8+8dczoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_results([], [], losses_backprop_img, accuracy_backprop_img, losses_feedback_img, accuracy_feedback_img, numupdates, numepochs, \"image\",\\\n",
    "            learnrate,numhidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a LogisticRegression linear model in Scikit-learn model for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The Accuracy on AG News Corpus with Logistic Regresion is  0.814 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for txt data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_classifier = LogisticRegression(C=1)\n",
    "\n",
    "lr_classifier.fit(x_train_tf.toarray(),tmp_train_l[0:10000])\n",
    "predicted = lr_classifier.predict(x_test_tf.toarray())\n",
    "print(\"\\n The Accuracy on AG News Corpus with Logistic Regresion is \", str(np.mean(predicted == tmp_test_l[0:1000])), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The Accuracy on MNIST with Logistic Regresion is  0.882 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felis/Programming/work/university/comp550/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# for image data\n",
    "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "\n",
    "train_images = train_set[0][0:10000]\n",
    "tran_labels = train_set[1][0:10000]\n",
    "\n",
    "test_images = test_set[0][0:1000]\n",
    "test_labels = test_set[1][0:1000]\n",
    "\n",
    "\n",
    "lr_classifier = LogisticRegression(C=1)\n",
    "lr_classifier.fit(train_images,tran_labels)\n",
    "predicted = lr_classifier.predict(test_images)\n",
    "print(\"\\n The Accuracy on MNIST with Logistic Regresion is \", np.mean(predicted == test_labels), \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
